{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization - Worked Examples\n",
    "\n",
    "Author: Yamini Manral (manral.y@northeastern.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore diverse inference methodologies, including zero-shot, one-shot, and few-shot techniques, evaluating their distinct impacts on output quality. A key focus will be on prompt engineering, examining its influence on model responses. Leveraging various Google models as base architectures, we will immerse ourselves in the intricacies of in-context learning. There will also be an implementation of the Instruct model, an instruction fine-tuned variant, providing an understanding of customizing models for specific tasks. Comparative analyses, both manual and quantitative (ROUGE scores) will offer insights into the model's performance. The integration of Prompt-based Extractive Fine-tuning (PEFT) models will uncover variances in summarization outputs, emphasizing the significance of prompt diversity. Further exploration will delve into fine-tuning models for detoxifying summaries, employing reinforcement learning techniques such as feedback and rewards and qualitative assessments to measure the discernible differences in model behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 - Generative AI Use Case: Summarize Dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the practical side of this course. In this lab we will do the dialogue summarization task using generative AI. We will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task we need. By comparing zero shot, one shot, and few shot inferences, we will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --disable-pip-version-check \\\n",
    "#     torch==1.13.1 \\\n",
    "#     torchdata==0.5.1 --quiet\n",
    "\n",
    "# %pip install \\\n",
    "#     transformers==4.27.2 \\\n",
    "#     datasets==2.11.0  --quiet\n",
    "\n",
    "# %pip install \\\n",
    "#     evaluate==0.4.0 \\\n",
    "#     rouge_score==0.1.2 \\\n",
    "#     loralib==0.1.1 \\\n",
    "#     peft==0.3.0 --quiet\n",
    "\n",
    "# # Installing the Reinforcement Learning library directly from github.\n",
    "# %pip install git+https://github.com/lvwerra/trl.git@25fa1bd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems encountered here:**\n",
    "\n",
    "datasets was not upgraded ran the following code to fix it\n",
    "`pip install -U datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index). \n",
    "\n",
    "Let's upload some simple dialogues from the [Samsum](https://huggingface.co/datasets/samsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** Changed the dialog dataset to [samsum](https://huggingface.co/datasets/samsum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"samsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a couple of dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [50, 400]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models. \n",
    "\n",
    "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([1521,   39,    3, 3770,  376,  147, 8988,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "Are your bringing him over tonight\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Are your bringing him over tonight\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Susan's date was a sweet man. Susan and her date went to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instruct the model to perform a task - summarize a dialogue - we can take the dialogue and convert it into an instruction prompt. This is often called **zero shot inference**. Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Even though the model is able to understand and summarize parts of the conversation, it still does not pick up on the nuance of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Exercise:***\n",
    "\n",
    "- Experiment with the `prompt` text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. `Summary: `?\n",
    "- Try to rephrase the beginning of the `prompt` text from `Summarize the following conversation.` to something different - and see how it will influence the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** Upon changing the prompt from `Summary` to nothing, there is no change in the output generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Write a short summary for the given conversation:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Write a short summary for the given conversation:\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Write a short summary for the given conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** There doesn't seem to be any changes upon changing prompt text. The output for zero-shot inference remains the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, we will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** No noticible change from introducting flan t5 prompt template. This is what you will try to solve with the few shot inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Summarize Dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.1 - One Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which we want the model to complete (`example_index_to_summarize`).  We will use the FLAN-T5 prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the prompt to perform one shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Ryan: I have a bad feeling about this\n",
      "Ryan: <file_other>\n",
      "Sebastian: Ukraine...\n",
      "Sebastian: This russian circus will never end...\n",
      "Ryan: I hope the leaders of of nations will react somehow to this shit.\n",
      "Sebastian: I hope so too :(\n",
      "\n",
      "What was going on?\n",
      "Ryan and Sebastian are worried about the political situation in Ukraine.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Shaldona: WE ARE GONNA GET MARRIED ❤️❤️\n",
      "Shaldona: <file_others>\n",
      "Shaldona: This is our mobile inviation for our wedding.\n",
      "Shaldona: Invitation*\n",
      "Piper: Hey. You haven’t sent me any messages for a few years.\n",
      "Piper: And now you are sending me your wedding invitation \n",
      "Piper: THROUGH MESSENGER?\n",
      "Shaldona: .....\n",
      "Shaldona: Well..\n",
      "Shaldona: I had no enough time to meet everybody and give this in person.\n",
      "Shaldona: Hope you understand.\n",
      "Piper: If you don't have time to give the invitation card in person but expect people go to your wedding\n",
      "Piper: Shaldona, if so, you are too greedy.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [80]\n",
    "example_index_to_summarize = 250\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass this prompt to perform the one shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Shaldona sends mobile invitations to her wedding, as she has no time to give them in person.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "Shaldona and Piper are getting married. Shaldona hasn't sent Piper messages for a few years. Piper is worried about Shaldona's wedding invitation.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** One-shot inference seems to work great now that it is able to summarize better and in greater detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's explore few shot inference by adding two more full dialogue-summary pairs to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Ali: I think I left my wallet at your place yesterday. Could you check? \n",
      "Mohammad: Give me a sec, I'll have a look around my room.\n",
      "Ali: OK.\n",
      "Mohammad: Found it!\n",
      "Ali: Phew, I don't know what I'd do if it wasn't there. Can you bring it to uni tomorrow?\n",
      "Mohammad: Sure thing.\n",
      "\n",
      "What was going on?\n",
      "Ali left his wallet at Mohammad's place. Mohammad'll bring it to uni tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Chris: Hi there! Where are you? Any chance of skyping?\n",
      "Rick: Hi! Our last two days in Cancun before flying to Havana. Yeah, skyping is an idea. When would it suit you?\n",
      "Rick: We don't have the best of connections in the room but I can get you pretty well in the lobby.\n",
      "Chris: What's the time in your place now?\n",
      "Rick: 6:45 pm\n",
      "Chris: It's a quarter to one in the morning here. Am still in front of the box.\n",
      "Rick: Gracious me! Sorry mate. You needn't have answered.\n",
      "Chris: 8-D\n",
      "Rick: Just tell me when we could skype.\n",
      "Chris: Preferably in the evening. Just a few hours earlier than now. And not tomorrow.\n",
      "Rick: Shute! Only tomorrow makes sense as there's no workable internet in Cuba.\n",
      "Chris: Could you make it like 3 pm your time?\n",
      "Rick: Sure.\n",
      "Chris: Perfect. So talk to you tomorrow.\n",
      "Chris: Give my love to Helen please.\n",
      "Rick: I will. Thx.\n",
      "\n",
      "What was going on?\n",
      "Rick and Helen are in Cancun. They're flying to Havana in two days. Chris and Rick will talk on Skype at 3 PM in Mexico.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Abdellilah: Where are you?\n",
      "Sam: work\n",
      "Abdellilah: What time you finish?\n",
      "Sam: Not til 5\n",
      "Abdellilah: Are your bringing him over tonight:\n",
      "Sam: No in the morning:\n",
      "Abdellilah: ok, what time?\n",
      "Sam: About 9. Is that ok?\n",
      "Abdellilah: ok - see you then\n",
      "\n",
      "What was going on?\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Debbie: Help, I don't know which dress to buy! <file_photo> or <file_photo>?\n",
      "Kelly: The red one! It's beautiful.\n",
      "Denise: It is, but the green one will suit you better.\n",
      "Kelly: Why? Debbie looks good in red.\n",
      "Denise: She does, but in my opinion that dress would look better on someone taller. Deb needs a shorter one.\n",
      "Kelly: Right, I haven't thought about it.\n",
      "Debbie: So the green one?\n",
      "Denise: Definitely!\n",
      "Kelly: Yeah. But can you send me the link to the store? I'm considering buying the red one for myself :D\n",
      "Debbie: LOL, okay. Here's the link: <file_other>\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [70, 100, 200]\n",
    "example_index_to_summarize = 260\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass this prompt to perform a few shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Debbie can't decide between buying a red dress and a green one. On Kelly and Denise's advice she will buy the green one. Kelly is considering buying the red one for herself.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Debbie is looking for a red dress. Kelly recommends the green dress. Kelly is considering buying the red one for herself.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, few shot did not provide much of an improvement over one shot inference.  And, anything above 5 or 6 shot will typically not help much, either.\n",
    "\n",
    "However, we can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Exercise:***\n",
    "\n",
    "Experiment with the few shot inferencing.\n",
    "- Choose different dialogues - change the indices in the `example_indices_full` list and `example_index_to_summarize` value.\n",
    "- Change the number of shots. Be sure to stay within the model's 512 context length, however.\n",
    "\n",
    "How well does few shot inferencing work with other examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing various other dialogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Deirdre: Hi Beth, how are you love?\n",
      "Beth: Hi Auntie Deirdre, I'm been meaning to message you, had a favour to ask.\n",
      "Deirdre: Wondered if you had any thought about your Mum's 40th, we've got to do something special!\n",
      "Beth: How about a girls weekend, just mum, me, you and the girls, Kira will have to come back from Uni, of course.\n",
      "Deirdre: Sounds fab! Get your thinking cap on, it's only in 6 weeks! Bet she's dreading it, I remember doing that!\n",
      "Beth: Oh yeah, we had a surprise party for you, you nearly had a heart attack! \n",
      "Deirdre: Well, it was a lovely surprise! Gosh, thats nearly 4 years ago now, time flies! What was the favour, darling?\n",
      "Beth: Oh, it was just that I fancied trying a bit of work experience in the salon, auntie.\n",
      "Deirdre: Well, I am looking for Saturday girls, are you sure about it? you could do well in the exams and go on to college or 6th form.\n",
      "Beth: I know, but it's not for me, auntie, I am doing all foundation papers and I'm struggling with those.\n",
      "Deirdre: What about a tutor? Kira could help you in the hols.\n",
      "Beth: Maybe, but I'd like to try working. I'm 16 soon, I'm old enough.\n",
      "Deirdre: I know. Look, pop in tomorrow after school and we'll have a cuppa and a chat.\n",
      "Beth: Yes, thanks auntie. I'd really like to try the beauty therapy side.\n",
      "Deirdre: Its not for the squeamish, mind. Massage, pedicures, not to mention waxing!\n",
      "Beth: Oh yes, I was chatting to a friend about it yesterday!\n",
      "Deirdre: Maxine manages the beauty side, you can meet her tomorrow and we'll see how it goes.\n",
      "Beth: Yes, I'd really like that. \n",
      "Deirdre: We can try a few hours on a Saturday for a couple of weeks as work experience. I'll give you a tenner or so per session to start off for your lunch, coffee and bus fare etc. If you like, we'll take it from there.\n",
      "Beth: OK, I like the sound of it! See you tomorrow Auntie! Love you!\n",
      "Deirdre: Bye, lovely girl! Xx\n",
      "\n",
      "What was going on?\n",
      "Beth wants to organize a girls weekend to celebrate her mother's 40th birthday. She also wants to work at Deidre's beauty salon. Deidre offers her a few hours on Saturdays as work experience. They set up for a meeting tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "What was going on?\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Ali: I think I left my wallet at your place yesterday. Could you check? \n",
      "Mohammad: Give me a sec, I'll have a look around my room.\n",
      "Ali: OK.\n",
      "Mohammad: Found it!\n",
      "Ali: Phew, I don't know what I'd do if it wasn't there. Can you bring it to uni tomorrow?\n",
      "Mohammad: Sure thing.\n",
      "\n",
      "What was going on?\n",
      "Ali left his wallet at Mohammad's place. Mohammad'll bring it to uni tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Teacher: Rashi, why are you so low? \n",
      "Rashi: Ma’am I’m a bit confused about my career. \n",
      "Teacher: What is your confusion?\n",
      "Rashi: I was discussing with my friends about the career options. \n",
      "Teacher: Hmm.\n",
      "Rashi: There are too many to choose from.\n",
      "Teacher: Choose a career based on what truly interests you. \n",
      "Rashi: I have many that interests me. How does it determine the career?\n",
      "Teacher: The passion you have for what you do drives you to success. \n",
      "Rashi: But what about earnings?\n",
      "Teacher: Remember at some point of time one should learn to balance  between duties and success.\n",
      "Rashi: How do I do that?\n",
      "Teacher: Choose a career which interests you, get experienced and try to progress and widen the scope after a while.\n",
      "Rashi: Hmm, ok.\n",
      "Teacher: Something like earn and learn sort of..\n",
      "Rashi: You are so right. I will remember this.\n",
      "Teacher: So hope I managed to answer your questions.\n",
      "Rashi: Yes mam! Thank you very much! \n",
      "Teacher : You are most welcome, Rashi.\n",
      "\n",
      "What was going on?\n",
      "Rashi is confused by too many career choices. Teacher advises him to choose something he has passion for and what interests him.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Alexander: Personal request to send me message when you will be in taxi\n",
      "Alexander: If any problem, call me\n",
      "Tom: ;)\n",
      "Tom: Thank You, I appreciate it\n",
      "Alexander: Taxi confirmation below\n",
      "Alexander: <file_photo>\n",
      "Tom: Thank you for the transport, we arrived safely, although without luggages :/\n",
      "Alexander: Good but bad\n",
      "Tom: Yeeees\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [20, 50, 70, 110]\n",
    "example_index_to_summarize = 160\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom a message when he will be in taxi. Tom arrived safely without luggages.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Few shot inference with different dialogs, does a good job of summarization.\n",
    "\n",
    "Changing the number of shots from 3 to 4 does not seem to make a lot of changes to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. \n",
    "\n",
    "A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Exercise:***\n",
    "\n",
    "Change the configuration parameters to investigate their influence on the output. \n",
    "\n",
    "Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). \n",
    "\n",
    "Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom a message when he will be in taxi. Tom arrived safely without luggages.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander asks Tom to inform Alexander when Tom is in taxi. Tom and Alexander are travelling together. Tom arrived safely without luggage.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments related to the choice of the parameters in the code cell above:\n",
    "- Choosing `max_new_tokens=10` will make the output text too short, so the dialogue summary will be cut.\n",
    "- Putting `do_sample = True` and changing the temperature value you get more flexibility in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, prompt engineering can take you a long way for this use case, but there are some limitations. Next, you will start to explore how you can use fine-tuning to help your LLM to understand a particular use case in better depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='Lab2'></a>\n",
    "## Lab 2: Fine-Tune a Generative AI Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** using smaller version of flan-t5 [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name='google/flan-t5-small'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2 - Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Linda: Hi Dad, I want to buy flowers for mum! But I don't remember which one she likes :(\n",
      "Michael: Well, she likes all the flowers I believe\n",
      "Linda: That doesn't help! I'm on a flower market right now!\n",
      "Michael: Send me some pics then\n",
      "Linda: <file_photo> \n",
      "Michael: Tulips are nice, roses too\n",
      "Linda:  What about carnations?\n",
      "Michael: No, carnations are boring :D\n",
      "Linda: Thanks Dad, srsly…\n",
      "Michael:  What about freesias? She likes them a lot, are there any there?\n",
      "Linda: <file_photo> \n",
      "Michael: Take those!\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Linda wants to buy flowers for her mother and asks Michael which flowers does she like. Michael suggests Linda to buy freesias.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Linda wants to buy flowers for mum. She's on a flower market. Michael sends her some pictures.\n"
     ]
    }
   ],
   "source": [
    "index = 800\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 - Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 - Preprocess the Dialog-Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d8a1dd95004d4a941d50894ccd04f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25aaa1e0fa3947278b1f65efc03aebba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fde28c953445758bf0299b7fa16a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save some time in the lab, you will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8f05b36d174dd6bd9eec7d4af18fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6c09f9ce0448efb7ad6a9a02ede180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b97772332684fe5b9e6d56b8c381c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of all three parts of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (148, 2)\n",
      "Validation: (9, 2)\n",
      "Test: (9, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 148\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
    "\n",
    "Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** Training a fully fine-tuned version of the model would take a few hours on a GPU. Instead we download a pre-fine-tuned model [mrm8488/flan-t5-small-finetuned-samsum](https://huggingface.co/mrm8488/flan-t5-small-finetuned-samsum?text=Sid%3A+Wanna+catch+a+movie%3F%0AAnnie%3A+sure+what+do+you+have+in+mind%3F%0ASid%3B+the+Aquaman%3F+%3AD%0AAnnie%3A+haha+isn%27t+it+a+bit+childish%0ASid%3A+noooooo+I+mean+yes+but+it%27s+the+highest+grossing+movie+this+week%0AAnnie%3A+seriously%3F%0ASid%3A+yeah%3F%0AAnnie%3A+okay+let%27s+see+what+the+fuss+is+all+about) to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_name=\"mrm8488/flan-t5-small-finetuned-samsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Nick and Jane are going to meet up for a drink.\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "      <td>Betty called Larry last time they were at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "      <td>Eric and Rob are watching a show on YouTube.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "      <td>Bob will send Lenny photos of the trousers. Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "      <td>Emma will pick Will up at the moment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "      <td>Jane is in Warsaw. Ollie will bring some sun w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hilary has the keys to the apartment. Benjamin...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "      <td>Benjamin, Hilary and Daniel are meeting for dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Payton provides Max with websites selling clot...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "      <td>Payton is looking for clothes to buy. Max will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rita and Tina are bored at work and have still...</td>\n",
       "      <td>Rita is tired and is not happy at work.</td>\n",
       "      <td>Rita is tired and is tired. Tina is tired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beatrice wants to buy Leo a scarf, but he does...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "      <td>Beatrice is in town. She doesn't have a scarf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric doesn't know if his parents let him go to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "      <td>Eric is coming to the wedding. He has a lot to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Hannah needs Betty's number but Amanda doesn't...   \n",
       "1  Eric and Rob are going to watch a stand-up on ...   \n",
       "2  Lenny can't decide which trousers to buy. Bob ...   \n",
       "3  Emma will be home soon and she will let Will k...   \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....   \n",
       "5  Hilary has the keys to the apartment. Benjamin...   \n",
       "6  Payton provides Max with websites selling clot...   \n",
       "7  Rita and Tina are bored at work and have still...   \n",
       "8  Beatrice wants to buy Leo a scarf, but he does...   \n",
       "9  Eric doesn't know if his parents let him go to...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Amanda can't find Betty's number. Amanda will ...   \n",
       "1  Eric and Rob are watching a stand-up. Eric and...   \n",
       "2  Lenny wants to buy two pairs of purple trouser...   \n",
       "3     Emma will be home soon. Will will pick her up.   \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...   \n",
       "5  Hilary and Elliot are meeting at the conferenc...   \n",
       "6  Payton likes shopping but he doesn't always bu...   \n",
       "7            Rita is tired and is not happy at work.   \n",
       "8  Beatrice is in town, shopping. She has a scarf...   \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  Betty called Larry last time they were at the ...  \n",
       "1       Eric and Rob are watching a show on YouTube.  \n",
       "2  Bob will send Lenny photos of the trousers. Le...  \n",
       "3              Emma will pick Will up at the moment.  \n",
       "4  Jane is in Warsaw. Ollie will bring some sun w...  \n",
       "5  Benjamin, Hilary and Daniel are meeting for dr...  \n",
       "6  Payton is looking for clothes to buy. Max will...  \n",
       "7         Rita is tired and is tired. Tina is tired.  \n",
       "8  Beatrice is in town. She doesn't have a scarf....  \n",
       "9  Eric is coming to the wedding. He has a lot to...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.46985436352130705, 'rouge2': 0.22581970994728395, 'rougeL': 0.3816583797939229, 'rougeLsum': 0.3852519615421176}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.3994572016278605, 'rouge2': 0.14111831257416574, 'rougeL': 0.3108604710116508, 'rougeLsum': 0.3118647194203424}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge scores of this model are bad, even worse than our regular model. Let's move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23337391746914432, 'rouge2': 0.07620718933525607, 'rougeL': 0.2017702446072403, 'rougeLsum': 0.20152238762082608}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4214309303597156, 'rouge2': 0.18040230847807043, 'rougeL': 0.33809319137790006, 'rougeLsum': 0.3381026931436848}\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 18.81%\n",
      "rouge2: 10.42%\n",
      "rougeL: 13.63%\n",
      "rougeLsum: 13.66%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Perform Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n",
    "\n",
    "PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes:** \n",
    "Since trainig a model from scratch is time consuming and needs compute resources, the models chosen here are [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) and [flan-t5-base-peft-samsum](https://huggingface.co/RohitKeswani/flan-t5-base-peft-samsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       \"RohitKeswani/flan-t5-base-peft-samsum\",\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "Make inferences with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Sam is working at 9. Sam will bring him over tonight.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "# baseline_human_summary = dataset['test'][index]['summary']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT model result looks very good, almost as good and detailed as human baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "      <td>Betty called Larry last time they were at the ...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "      <td>Eric and Rob are watching a show on YouTube.</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "      <td>Bob will send Lenny photos of the trousers. Le...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "      <td>Emma will pick Will up at the moment.</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "      <td>Jane is in Warsaw. Ollie will bring some sun w...</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hilary has the keys to the apartment. Benjamin...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "      <td>Benjamin, Hilary and Daniel are meeting for dr...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Payton provides Max with websites selling clot...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "      <td>Payton is looking for clothes to buy. Max will...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rita and Tina are bored at work and have still...</td>\n",
       "      <td>Rita is tired and is not happy at work.</td>\n",
       "      <td>Rita is tired and is tired. Tina is tired.</td>\n",
       "      <td>Rita is tired and is not able to concentrate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beatrice wants to buy Leo a scarf, but he does...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "      <td>Beatrice is in town. She doesn't have a scarf....</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric doesn't know if his parents let him go to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "      <td>Eric is coming to the wedding. He has a lot to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Hannah needs Betty's number but Amanda doesn't...   \n",
       "1  Eric and Rob are going to watch a stand-up on ...   \n",
       "2  Lenny can't decide which trousers to buy. Bob ...   \n",
       "3  Emma will be home soon and she will let Will k...   \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....   \n",
       "5  Hilary has the keys to the apartment. Benjamin...   \n",
       "6  Payton provides Max with websites selling clot...   \n",
       "7  Rita and Tina are bored at work and have still...   \n",
       "8  Beatrice wants to buy Leo a scarf, but he does...   \n",
       "9  Eric doesn't know if his parents let him go to...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Amanda can't find Betty's number. Amanda will ...   \n",
       "1  Eric and Rob are watching a stand-up. Eric and...   \n",
       "2  Lenny wants to buy two pairs of purple trouser...   \n",
       "3     Emma will be home soon. Will will pick her up.   \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...   \n",
       "5  Hilary and Elliot are meeting at the conferenc...   \n",
       "6  Payton likes shopping but he doesn't always bu...   \n",
       "7            Rita is tired and is not happy at work.   \n",
       "8  Beatrice is in town, shopping. She has a scarf...   \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  Betty called Larry last time they were at the ...   \n",
       "1       Eric and Rob are watching a show on YouTube.   \n",
       "2  Bob will send Lenny photos of the trousers. Le...   \n",
       "3              Emma will pick Will up at the moment.   \n",
       "4  Jane is in Warsaw. Ollie will bring some sun w...   \n",
       "5  Benjamin, Hilary and Daniel are meeting for dr...   \n",
       "6  Payton is looking for clothes to buy. Max will...   \n",
       "7         Rita is tired and is tired. Tina is tired.   \n",
       "8  Beatrice is in town. She doesn't have a scarf....   \n",
       "9  Eric is coming to the wedding. He has a lot to...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  Amanda can't find Betty's number. Amanda will ...  \n",
       "1  Eric and Rob are watching a stand-up. Eric and...  \n",
       "2  Lenny wants to buy two pairs of purple trouser...  \n",
       "3     Emma will be home soon. Will will pick her up.  \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...  \n",
       "5  Hilary and Elliot are meeting at the conferenc...  \n",
       "6  Payton likes shopping but he doesn't always bu...  \n",
       "7  Rita is tired and is not able to concentrate a...  \n",
       "8  Beatrice is in town, shopping. She has a scarf...  \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.46985436352130705, 'rouge2': 0.22581970994728395, 'rougeL': 0.3816583797939229, 'rougeLsum': 0.3852519615421176}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.3994572016278605, 'rouge2': 0.14111831257416574, 'rougeL': 0.3108604710116508, 'rougeLsum': 0.3118647194203424}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.47451100961572235, 'rouge2': 0.22690539599006532, 'rougeL': 0.37721480092992143, 'rougeLsum': 0.38095574175069424}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that PEFT model performed a little bit better than flan-t5-base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23337391746914432, 'rouge2': 0.07620718933525607, 'rougeL': 0.2017702446072403, 'rougeLsum': 0.20152238762082608}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4214309303597156, 'rouge2': 0.18040230847807043, 'rougeL': 0.33809319137790006, 'rougeLsum': 0.3381026931436848}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.40810554423302325, 'rouge2': 0.16353829312593815, 'rougeL': 0.3250376063319481, 'rougeLsum': 0.32473416304982294}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n",
    "\n",
    "Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 17.47%\n",
      "rouge2: 8.73%\n",
      "rougeL: 12.33%\n",
      "rougeLsum: 12.32%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the improvement of PEFT over a full fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.33%\n",
      "rouge2: -1.69%\n",
      "rougeL: -1.31%\n",
      "rougeLsum: -1.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 - Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will keep working with the same Hugging Face dataset [samsum](https://huggingface.co/datasets/samsum) and the pre-trained model [FLAN-T5-BASE](https://huggingface.co/google/flan-t5-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"samsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "\n",
    "dataset_original = load_dataset(\"samsum\")\n",
    "\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to preprocess the dataset. We will take only a part of it, then filter the dialogues of a particular length (just to make those examples long enough and, at the same time, easy to read). Then wrap each dialogue with the instruction and tokenize the prompts. Save the token ids in the field `input_ids` and decoded version of the prompts in the field `query`.\n",
    "\n",
    "We could do that all step by step in the cell below, but it is a good habit to organize that all in a function `build_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'query'],\n",
      "        num_rows: 7851\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'query'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length,\n",
    "                  input_max_text_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Tokenizer model name.\n",
    "    - dataset_name (str): Name of the dataset to load.\n",
    "    - input_min_text_length (int): Minimum length of the dialogues.\n",
    "    - input_max_text_length (int): Maximum length of the dialogues.\n",
    "\n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # load dataset (only \"train\" part will be enough for this lab).\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    def tokenize(sample):\n",
    "\n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "\n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "\n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200,\n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a function to pull out the number of model parameters (it is the same as in the previous lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the adapter to the original FLAN-T5 model. In the previous lab you were adding the fully trained adapter only for inferences, so there was no need to pass LoRA configurations doing that. Now you need to pass them to the constructed PEFT model, also putting `is_trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       'RohitKeswani/flan-t5-base-peft-samsum',\n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       device_map=\"auto\",\n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you are preparing to fine-tune the LLM using Reinforcement Learning (RL). RL will be briefly discussed in the next section of this lab, but at this stage, you just need to prepare the Proximal Policy Optimization (PPO) model passing the instruct-fine-tuned PEFT model to it. PPO will be used to optimize the RL policy against the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During PPO, only a few parameters will be updated. Specifically, the parameters of the `ValueHead`. More information about this class of models can be found in the [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters can be computed as $(n+1)*m$, where $n$ is the number of input units (here $n=768$) and $m$ is the number of output units ($m=1$). The $+1$ term in the equation takes into account the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a frozen copy of the PPO which will not be fine-tuned - a reference model. The reference model will represent the LLM before detoxification. None of the parameters of the reference model will be updated during PPO training. This is on purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is set. It is time to prepare the reward model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Prepare Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning (RL)** is one type of machine learning where agents take actions in an environment aimed at maximizing their cumulative rewards. The agent's behavior is defined by the **policy**. And the goal of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the **reward function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the previous section, the original policy is based on the instruct PEFT model - this is the LLM before detoxification. Then we could ask human labelers to give feedback on the outputs' toxicity. However, it can be expensive to use them for the entire fine-tuning process. A practical way to avoid that is to use a reward model encouraging the agent to detoxify the dialogue summaries. The intuitive approach would be to do some form of sentiment analysis across two classes (`nothate` and `hate`) and give a higher reward if there is higher a chance of getting class `nothate` as an output.\n",
    "\n",
    "You will use [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) for the reward model. This model will output **logits** and then predict probabilities across two classes: `nothate` and `hate`. The logits of the output `nothate` will be taken as a positive reward. Then, the model will be fine-tuned with PPO using those reward values.\n",
    "\n",
    "We create the instance of the required model class for the RoBERTa model. We also need to load a tokenizer to test the model. Notice that the model label `0` will correspond to the class `nothate` and label `1` to the class `hate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the same device as input_ids\n",
    "toxicity_model = toxicity_model.to(device)\n",
    "\n",
    "# Ensure input_ids is on the same device\n",
    "toxicity_input_ids = toxicity_input_ids.to(device)\n",
    "\n",
    "# Move logits to the device\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly set the device for input_ids\n",
    "toxicity_input_ids = toxicity_input_ids.to(device)\n",
    "\n",
    "# Move logits to the device\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU if GPU is not available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Move the model to the device\n",
    "toxicity_model = toxicity_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.114100694656372, -2.4896175861358643]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.0036706167738884687]\n",
      "reward (high): [3.114100694656372]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a toxic comment.  This will have a low reward because it is more toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities [not hate, hate]: [0.2564719617366791, 0.7435280084609985]\n",
      "reward (low): [-0.6921164393424988]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "# print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (low): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output:\n",
      "For non-toxic text\n",
      "[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n",
      "For toxic text\n",
      "[{'label': 'hate', 'score': 0.37227070331573486}, {'label': 'nothate', 'score': -0.6921164393424988}]\n",
      "[{'label': 'hate', 'score': 0.7435280084609985}, {'label': 'nothate', 'score': 0.25647199153900146}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                          model=toxicity_model_name,\n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output:\")\n",
    "print(\"For non-toxic text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"For toxic text\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are the logits for both `nothate` (positive) and `hate` (negative) classes. But PPO will be using logits only of the `nothate` class as the positive reward signal used to help detoxify the LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 0.37227070331573486}, {'label': 'nothate', 'score': -0.6921164393424988}]\n",
      "[{'label': 'hate', 'score': 0.7435280084609985}, {'label': 'nothate', 'score': 0.25647199153900146}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Evaluate Toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To evaluate the model before and after fine-tuning/detoxification we need to set up the [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity). The **toxicity score** is a decimal value between 0 and 1 where 1 is the highest toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate toxicity for the same sentences as seen previosly. It's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.003670595120638609]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.7435280084609985]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluator can be used to compute the toxicity of the dialogues prepared in earlier sections. We will need to pass the test dataset (`dataset[\"test\"]`), the same tokenizer which was used  earlier, the frozen PEFT model, and the toxicity evaluator. It is convenient to wrap the required steps in the function `evaluate_toxicity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model,\n",
    "                      toxicity_evaluator,\n",
    "                      tokenizer,\n",
    "                      dataset,\n",
    "                      num_samples):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model (trl model): Model to be evaluated.\n",
    "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
    "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
    "    - dataset (dataset): Input dataset for the evaluation.\n",
    "    - num_samples (int): Maximum number of samples for the evaluation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two numpy.float64 values:\n",
    "    - mean (numpy.float64): Mean of the samples toxicity.\n",
    "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "\n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now perform the calculation of the model toxicity before fine-tuning/detoxification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:39,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.012077189002990384, 0.020708743850324163]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
    "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                          tokenizer=tokenizer,\n",
    "                                                                          dataset=dataset[\"test\"],\n",
    "                                                                          num_samples=10)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Perform Fine-Tuning to Detoxify the Summaries\n",
    "Optimize a RL policy against the reward model using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Initialize `PPOTrainer`\n",
    "\n",
    "For the `PPOTrainer` initialization, we will need a collator. Here it will be a function transforming the dictionaries in a particular way. We can define and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration parameters. Load the `ppo_model` and the tokenizer. We will also load a frozen version of the model `ref_model`. The first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This works as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config,\n",
    "                         model=ppo_model,\n",
    "                         ref_model=ref_model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset[\"train\"],\n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning loop consists of the following main steps:\n",
    "1. Get the query responses from the policy LLM (PEFT model).\n",
    "2. Get sentiments for query/responses from hate speech RoBERTa model.\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet.\n",
    "\n",
    "The operation is running if you see the following metrics appearing:\n",
    "* `objective/kl`: minimize kl divergence,\n",
    "* `ppo/returns/mean`: maximize mean returns,\n",
    "* `ppo/policy/advantages_mean`: maximize advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/model.py:357: UserWarning: Careful, disabling adapter layers with bias configured to be 'lora_only' does not produce the same output as the the base model would without adaption.\n",
      "  warnings.warn(msg)\n",
      "1it [00:18, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0027043893933296204\n",
      "ppo/returns/mean: 1.1912437677383423\n",
      "ppo/policy/advantages_mean: 0.04806836321949959\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "2it [00:35, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.04662386700510979\n",
      "ppo/returns/mean: 1.1792616844177246\n",
      "ppo/policy/advantages_mean: 0.041750453412532806\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r3it [00:54, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0608404204249382\n",
      "ppo/returns/mean: 1.3092687129974365\n",
      "ppo/policy/advantages_mean: 0.1101483702659607\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r4it [01:13, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.015154408290982246\n",
      "ppo/returns/mean: 1.4760247468948364\n",
      "ppo/policy/advantages_mean: 0.028458036482334137\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r5it [01:43, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.00789235346019268\n",
      "ppo/returns/mean: 1.0292794704437256\n",
      "ppo/policy/advantages_mean: 0.0061109475791454315\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r6it [01:59, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0338313989341259\n",
      "ppo/returns/mean: 1.3382985591888428\n",
      "ppo/policy/advantages_mean: 0.017700575292110443\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r7it [02:18, 20.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.006845513824373484\n",
      "ppo/returns/mean: 1.3699721097946167\n",
      "ppo/policy/advantages_mean: 0.07090044766664505\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r8it [02:37, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.04499111324548721\n",
      "ppo/returns/mean: 1.2051234245300293\n",
      "ppo/policy/advantages_mean: 0.04936248064041138\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r9it [02:53, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.007876656018197536\n",
      "ppo/returns/mean: 1.4433653354644775\n",
      "ppo/policy/advantages_mean: 0.046412259340286255\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [03:10, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.054574836045503616\n",
      "ppo/returns/mean: 1.3778477907180786\n",
      "ppo/policy/advantages_mean: 0.0006798207759857178\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "\n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Evaluate the Model Qualitatively\n",
    "\n",
    "Let's inspect some examples from the test dataset. We can compare the original `ref_model` to the fine-tuned/detoxified `ppo_model` using the toxicity evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.63s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "\n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store and review the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-785b36ec-456b-4644-b5fc-7f40c37b03b8\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the following conversation. Missy: hey Missy: i was just thinking Clara: haha really? Missy: about that song that we heard in the radio Missy: hey not funny! Clara: sorry, what about this song? Missy: what was the name of it? Clara: it was Feel It Still Missy: found it! thanks Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Missy's watch was funny. Clara is right.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; It was the song called Feel It Still.&lt;/s&gt;</td>\n",
       "      <td>1.998531</td>\n",
       "      <td>2.820851</td>\n",
       "      <td>0.822320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the following conversation. Gloria: Sean I need to take your car, I cannot start mine! Sean: Did you remember to buy petrol Honey? XD Gloria: Of course I did! There is something wrong! I Sean: Ok, drive safe then and be careful Gloria: I'll do my best, I wouldn't dare to hurt your baby :P Sean: Thanks a lot Honey! :* Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Gloria needs Sean's carve in.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Gloria sends Sean to the baby concerned for his baby.&lt;/s&gt;</td>\n",
       "      <td>1.349928</td>\n",
       "      <td>2.080774</td>\n",
       "      <td>0.730845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the following conversation. Kate: good news Kate: &lt;unk&gt;file_photo&gt; Emma: $1000!!! Kate: yes Melanie: how much did you expect to get from them? Kate: I applied for 1500 but I knew they never give the maximum amount Kate: so 1000 is kinda fine Melanie: sure, you're pay the student house at least Kate: exactly Kate: and a chocolate bar hahaha Melanie: not much left after that? Kate: the college is almost 1000 per trimester Melanie: Bastards Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Kate has won a full scholarship to St. Lawrence University. Kate applied for 1500 and was told the tuition was not handed out to her. The university earned $1,000 per trimester.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Kate won a professorship scholarship at a university for $1000 grant. Melanie has written a delicious note of her students' night.&lt;/s&gt;</td>\n",
       "      <td>1.810878</td>\n",
       "      <td>2.365676</td>\n",
       "      <td>0.554798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the following conversation. Pauline: Can I reserve a table for 6 for tonight? Restaurant: Of course, what time? Pauline: 7 PM, if possible. Restaurant: No problem. It's done. Can I help with anything else? Pauline: No, thank you. Restaurant: We're looking forward to seeing you tonight. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Restaurant wants 6 people for tonight.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The restaurant's invitation is'Top Shine' and seats 6 of 7 for special occasions tonight at 7 PM.&lt;/s&gt;</td>\n",
       "      <td>3.485358</td>\n",
       "      <td>3.784537</td>\n",
       "      <td>0.299179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize the following conversation. Isaac: it's white outside &lt;unk&gt;3 Ivy: maybe where you are :&lt;unk&gt; Isaac: yeah it is, but i think it's just hoarfrost and not snow Ivy: where's my snow :&lt;unk&gt; Isaac: i checked the forecast and it no longer says that it's gonna be snowing ;&lt;unk&gt; Isaac: buuut my friend from lublin just messaged me that it's actually snowing there right now Isaac: so maaaybe it will come! Ivy: XD Ivy: not really sure I'd like that XD Isaac: i hope it will! but it's so early t...</td>\n",
       "      <td>&lt;pad&gt; Ivy's snow pile in the basement is going to be white. Isaac thinks it will snow. Ivy's just not sure about this.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; It is snowing there now.&lt;/s&gt;</td>\n",
       "      <td>2.024485</td>\n",
       "      <td>2.260841</td>\n",
       "      <td>0.236357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summarize the following conversation. Lilly: you're not returning my calls again Lilly: please let me know if everything is okay Michelle: everything is fine, don't worry Michelle: I told you that I will be on a training course this week Michelle: can't really talk on the phone while in here, I'll call you after it ends Lilly: oh, completely forgot about that, sorry Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Michelle is not returning Lilly's phone calls. Michelle's going to be on a training course this week.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Michelle doesn't return Lilly's number, as she planned.&lt;/s&gt;</td>\n",
       "      <td>2.789807</td>\n",
       "      <td>2.873711</td>\n",
       "      <td>0.083904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summarize the following conversation. Robert: I'm so sorry for your loss Karen Karen: Thank you Robert Tom: I'm sorry Karen, Harry was an amazing man :( Michael: Are you planning on a memorial service? Karen: Yes, it's not easy at this stage to get my head around everything, but yes Karen: We reserved an Italian place, the one Harry loved - La Tomatina Tom: That's a lovely idea Robert: I've heard from Janet that the service will be in St. Thomas's Church Karen: Yes, at 11 am. I hope you will...</td>\n",
       "      <td>&lt;pad&gt; Karen says that Harry Salvatore died suddenly. They have to celebrate the life of Harry, and have to plan a memorial service. They say they will attend an Italian place at 11 am.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Harry was a great man. He used to prefer something Italian than a can ’t by many people. Karen normally reserved an apotheestage at the church on their waiting.&lt;/s&gt;</td>\n",
       "      <td>3.929939</td>\n",
       "      <td>3.933781</td>\n",
       "      <td>0.003842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summarize the following conversation. Luke: Hey, you coming to the movies with us tonight? Grace: Hey sorry for replying so late it was a busy day for me. Sure why not Luke: we're planning on meeting at my place at 7-ish. Carla told you we're dressing up right? Grace: yup, she told me all about it. I don't think I have anything to wear, though. Luke: I can lend you something, even just a long black cape would be alright, it's just for fun Grace: cool, thx. I don't know, I heard Jason's going...</td>\n",
       "      <td>&lt;pad&gt; Luke and Grace are going to The Avengers.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Grace is going to show up with Luke after ending a long day.&lt;/s&gt;</td>\n",
       "      <td>2.366690</td>\n",
       "      <td>2.366966</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summarize the following conversation. Dean: I feel sick Scott: hungover? Dean: no, like I ate something bad Scott: what did you eat yesterday? Dean: breakfast at Coffee Lovers' Scott: this is a rather safe place Dean: and Chinese from TaoTao for dinner Scott: now we have a suspect Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Dean taken his sick partner, a suspect.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Dean eats Chinese food, as did Scott.&lt;/s&gt;</td>\n",
       "      <td>1.702894</td>\n",
       "      <td>1.696505</td>\n",
       "      <td>-0.006389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summarize the following conversation. Jane: When is the report due? Miriam: 2nd of August Augusto: And then there will be a control visit? Miriam: As usual Jane: I can take care of that Miriam: Ok Miriam: Show them all the projects and choose the right people they will talk to Jane: I'm on it Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Jane will take care of the report due on 2nd of August, then one control visit and then the control visit.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The report consists of a control visit--Sanlin, the speaker--September 23.&lt;/s&gt;</td>\n",
       "      <td>3.913431</td>\n",
       "      <td>3.893194</td>\n",
       "      <td>-0.020237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the following conversation. Chris: Hello Anne, there's no water in the flat Anne: Hi, have you called the administrator? Or a plumber? Chris: No...? I've thought you may help with this issue. Anne: Ok, I think we need to establish something first. I'm renting you the flat, yes, and thank you for choosing it, but I'm not your housekeeper or anything. Chris: I get it, but you rented me a fully functional flat and now I'm paying for a flat without water now? Anne: Sorry, but... what? ...</td>\n",
       "      <td>&lt;pad&gt; Chris needs to buy a new house for his £25 owing to his lack of water, leaving her with as little money as possible. He is now responsible for bills which he is taking care of.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Chris has problems with water in the flat. Anne is unavailable. Anne is renting. Anne's providing the flat for Chris.&lt;/s&gt;</td>\n",
       "      <td>1.732846</td>\n",
       "      <td>1.692643</td>\n",
       "      <td>-0.040203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summarize the following conversation. Kenny: what time do you wanna meet the guys tm? Peter: hmm not sure Kenny: im free after 6 Peter: where we going Kenny: I just thought going downtown Peter: it may cost a lot Kenny: did you have a different idea? Peter: we could always go to my place Kenny: yeah we could do that Peter: may be boring but if we get a lot of people to come Kenny: yeah no could be fun lets do that a then Peter: ok ill send a text to everyone and see if they wanna come Kenny:...</td>\n",
       "      <td>&lt;pad&gt; Peter and Kenny are going out tomorrow at Kenny's place. Peter wants the guys to come. He will let everyone bring drinks and some pizza.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Kenny will go with Peter during the funeral party at Peter's place after 6or he may meet a lot of people at Peter's too. Peter will have all bring some drinks.&lt;/s&gt;</td>\n",
       "      <td>2.860176</td>\n",
       "      <td>2.793499</td>\n",
       "      <td>-0.066677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summarize the following conversation. Fiona: &lt;unk&gt;file_photo&gt; Eliza: who's that hottie? &lt;unk&gt;3 Fiona: &lt;unk&gt;file_photo&gt; Eliza: sweet! new hair? Fiona: yeah, got it done today! Eliza: going blonde was the best decision you ever made hun! Fiona: i know :D but i miss being a brunette sometimes Eliza: don't even say that, never go back!! Fiona: but I seemed smarter as a brunette :D hahahah Eliza: well... only seemed hahah :D Fiona: you witch!!! I will get you!!! Eliza: just kidding sweetheart :* ...</td>\n",
       "      <td>&lt;pad&gt; Fiona got expensive new brunettes' hair for a while. She misses being a brunette especially when she's brunette. Eliza is glad they are not going blonde to celebrate Fiona's anniversary.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Fiona gotten her blonder hair. Eliza approves Fiona'll try to kiss her.&lt;/s&gt;</td>\n",
       "      <td>1.218263</td>\n",
       "      <td>1.093494</td>\n",
       "      <td>-0.124769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summarize the following conversation. Megan: hey, did anyone see a stray pair of earphones in the room 306? i think i left them there after the class :( Derek: nope sorry Sally: me neither Megan: shit, i jut got em... serves me right for being such a mess i guess xd Derek: hahah it happens to me all the time Cher: where they blue? i found some :) Megan: yes! omg thank you so much! Cher: i didn't know who they belonged to tho Cher: i gave them to Prof. Johnson so you can pick them up on Monda...</td>\n",
       "      <td>&lt;pad&gt; The pair of earphones Megan left in room 306 left there after the class. Cher attached the earplugs to Prof. Johnson so Megan can pick them up on Monday. Harry found 20 pairs of blue earplugs she gave to Prof. Johnson.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Girl can not find her husband's earphones.&lt;/s&gt;</td>\n",
       "      <td>1.334728</td>\n",
       "      <td>1.124544</td>\n",
       "      <td>-0.210183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summarize the following conversation. Matt: I'll be there at 8:15pm or so. Matt: Waiting for you in the lobby. Jane: Cool. I'll come down. Jane: Let me know when you do get here. Matt: BTW any idea where we're going out to eat tonight? Jane: Nope. I asked the guys at reception but they reckon it is all dead expensive around here. Jane: Thought you might have some ideas. ;-) Matt: I was thinking of just going for a walk and see what we find. Jane: Kinda lovely idea but it's like minus 5 out t...</td>\n",
       "      <td>&lt;pad&gt; Matt will be waiting for Jane in the lobby. Jane hasn't seen Matt.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Matt will be at the hotel at 8:15pm. Jane will come down at 8:15pm. Matt has a question about the place that Jane and Jane are to make dinner.&lt;/s&gt;</td>\n",
       "      <td>2.589391</td>\n",
       "      <td>2.360463</td>\n",
       "      <td>-0.228927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summarize the following conversation. Kate: Good morning. Kai: Hi! How official! Kate: I wrote it at 4am Kai: I've noticed. Why? Kate: I had to get up early to catch the bus to the airport Kai: Where are you flying? Kate: To Antwerp! I'm fed up with Cambridge Kai: poor thing. Why? Kate: Just a stupid, elitist place without a soul. Or with a soul made of money. Kai: Try to rest a bit in Belgium, do not work too much. Kate: I have to work, but at least not in this soulless place. Kai: When are...</td>\n",
       "      <td>&lt;pad&gt; Kate wrote a mysterious letter at 4am. Kate is leaving Belgium for Antwerp to visit her new colleague. She'll meet her supervisor on Monday.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Kate writes a letter to Kai. Kate got up early thus she will go to Antwerp. She has to stop a look on drunk. Kate is tired of Cambridge. Kai has to see her supervisor in Belgium.&lt;/s&gt;</td>\n",
       "      <td>1.398047</td>\n",
       "      <td>0.938702</td>\n",
       "      <td>-0.459345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summarize the following conversation. Ben: are you going to the edinburgh comedy festival this summer? Holly: of course :-D Holly: i never miss it Ben: can I tag along? i've never been Holly: are you serious? Holly: my whole year centers around it Ben: lol WOW, so it's THAT good? :-) Holly: YES!! Holly: and please tag along Holly: you are going to love it Ben: what are the highlights this year? Holly: there's a brother and a sister who tell jokes while juggling together Ben: mmm... ok -- not...</td>\n",
       "      <td>&lt;pad&gt; Ben is likely to go to Edinburgh Comedy Festival but he's never been there. His whole year focuses on it.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Ben'll join Holly for the Edinburgh comedy festival. Holly always too old to tag along :)&lt;/s&gt;</td>\n",
       "      <td>2.812017</td>\n",
       "      <td>2.165207</td>\n",
       "      <td>-0.646810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summarize the following conversation. Susan: so your flyin 2moro? Patel: :( i have to Susan: what time is your flight? Patel: 5.30 in the morning Susan: I guess i better say goodbye now Patel: I miss you already. I'll be back in two weeks tho, &lt;unk&gt;file_gif&gt; Susan: it's gonna be very long two weeks. Patel: I know. but we haev to go through it somehow Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Patel needs to stay a week at home. He's planning a flight around 5.30 in the morning. He'll be back in several weeks.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Patel's ailing brain. Pam will arrive in two weeks.&lt;/s&gt;</td>\n",
       "      <td>2.196533</td>\n",
       "      <td>1.344703</td>\n",
       "      <td>-0.851830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summarize the following conversation. Charlotte: Have we got any plans for Friday evening? Ethan: I don't think so. Ethan: Let me check my diary. Ethan: Ah, right! I'm working late, till 20:00, but I have nothing planned afterwards. Ethan: Ideas? Charlotte: There this new movie I told you about. Charlotte: Would you like to see it? Ethan: Yeah, why not. Charlotte: Okey, I'll check in cinemas. :) Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Charlotte will check in cinemas for Friday evening. Charlotte works late.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Mama doesn't know what to do about Friday. She and Ethan will surely plan somewhere.&lt;/s&gt;</td>\n",
       "      <td>3.489472</td>\n",
       "      <td>2.628898</td>\n",
       "      <td>-0.860574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summarize the following conversation. Mimi: Hey there! We've just arrived! Who's up for a quick coffee? Eli: Me! Why don't you pop in this afternoon? Piero: I'll come as well if you do. Mimi: OK, what time? 5 pm? Eli: Great. Mimi: I'll bring some cookies! Piero: And I'll grab few beers for me and Damiano! Damiano: Sweet man. Eli: How long are you staying? Mimi: Until Xmas, then we're going back for few days to my parents' place. Damiano: We haven't decided about NY Eve. Maybe we can do somet...</td>\n",
       "      <td>&lt;pad&gt; Mimi and Eli will meet this afternoon at 5 pm for a coffee if Ramo and Damiano want to.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Mimi has just arrived and she'll make some cookies and be able to have some drinks with Eli. Piero will come with him to the five o'clock for a coffee with Eli. Damiano doesn't want to meet her.&lt;/s&gt;</td>\n",
       "      <td>3.546094</td>\n",
       "      <td>2.507572</td>\n",
       "      <td>-1.038522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-785b36ec-456b-4644-b5fc-7f40c37b03b8')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-785b36ec-456b-4644-b5fc-7f40c37b03b8 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-785b36ec-456b-4644-b5fc-7f40c37b03b8');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7fead504-94ee-4f83-8403-ff08abb6550d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fead504-94ee-4f83-8403-ff08abb6550d')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7fead504-94ee-4f83-8403-ff08abb6550d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0                                                                                                                                                                                                         Summarize the following conversation. Missy: hey Missy: i was just thinking Clara: haha really? Missy: about that song that we heard in the radio Missy: hey not funny! Clara: sorry, what about this song? Missy: what was the name of it? Clara: it was Feel It Still Missy: found it! thanks Summary: </s>   \n",
       "1                                                                                                                                                                Summarize the following conversation. Gloria: Sean I need to take your car, I cannot start mine! Sean: Did you remember to buy petrol Honey? XD Gloria: Of course I did! There is something wrong! I Sean: Ok, drive safe then and be careful Gloria: I'll do my best, I wouldn't dare to hurt your baby :P Sean: Thanks a lot Honey! :* Summary: </s>   \n",
       "2                                     Summarize the following conversation. Kate: good news Kate: <unk>file_photo> Emma: $1000!!! Kate: yes Melanie: how much did you expect to get from them? Kate: I applied for 1500 but I knew they never give the maximum amount Kate: so 1000 is kinda fine Melanie: sure, you're pay the student house at least Kate: exactly Kate: and a chocolate bar hahaha Melanie: not much left after that? Kate: the college is almost 1000 per trimester Melanie: Bastards Summary: </s>   \n",
       "3                                                                                                                                                                                                Summarize the following conversation. Pauline: Can I reserve a table for 6 for tonight? Restaurant: Of course, what time? Pauline: 7 PM, if possible. Restaurant: No problem. It's done. Can I help with anything else? Pauline: No, thank you. Restaurant: We're looking forward to seeing you tonight. Summary: </s>   \n",
       "4   Summarize the following conversation. Isaac: it's white outside <unk>3 Ivy: maybe where you are :<unk> Isaac: yeah it is, but i think it's just hoarfrost and not snow Ivy: where's my snow :<unk> Isaac: i checked the forecast and it no longer says that it's gonna be snowing ;<unk> Isaac: buuut my friend from lublin just messaged me that it's actually snowing there right now Isaac: so maaaybe it will come! Ivy: XD Ivy: not really sure I'd like that XD Isaac: i hope it will! but it's so early t...   \n",
       "5                                                                                                                        Summarize the following conversation. Lilly: you're not returning my calls again Lilly: please let me know if everything is okay Michelle: everything is fine, don't worry Michelle: I told you that I will be on a training course this week Michelle: can't really talk on the phone while in here, I'll call you after it ends Lilly: oh, completely forgot about that, sorry Summary: </s>   \n",
       "6   Summarize the following conversation. Robert: I'm so sorry for your loss Karen Karen: Thank you Robert Tom: I'm sorry Karen, Harry was an amazing man :( Michael: Are you planning on a memorial service? Karen: Yes, it's not easy at this stage to get my head around everything, but yes Karen: We reserved an Italian place, the one Harry loved - La Tomatina Tom: That's a lovely idea Robert: I've heard from Janet that the service will be in St. Thomas's Church Karen: Yes, at 11 am. I hope you will...   \n",
       "7   Summarize the following conversation. Luke: Hey, you coming to the movies with us tonight? Grace: Hey sorry for replying so late it was a busy day for me. Sure why not Luke: we're planning on meeting at my place at 7-ish. Carla told you we're dressing up right? Grace: yup, she told me all about it. I don't think I have anything to wear, though. Luke: I can lend you something, even just a long black cape would be alright, it's just for fun Grace: cool, thx. I don't know, I heard Jason's going...   \n",
       "8                                                                                                                                                                                                               Summarize the following conversation. Dean: I feel sick Scott: hungover? Dean: no, like I ate something bad Scott: what did you eat yesterday? Dean: breakfast at Coffee Lovers' Scott: this is a rather safe place Dean: and Chinese from TaoTao for dinner Scott: now we have a suspect Summary: </s>   \n",
       "9                                                                                                                                                                                                   Summarize the following conversation. Jane: When is the report due? Miriam: 2nd of August Augusto: And then there will be a control visit? Miriam: As usual Jane: I can take care of that Miriam: Ok Miriam: Show them all the projects and choose the right people they will talk to Jane: I'm on it Summary: </s>   \n",
       "10  Summarize the following conversation. Chris: Hello Anne, there's no water in the flat Anne: Hi, have you called the administrator? Or a plumber? Chris: No...? I've thought you may help with this issue. Anne: Ok, I think we need to establish something first. I'm renting you the flat, yes, and thank you for choosing it, but I'm not your housekeeper or anything. Chris: I get it, but you rented me a fully functional flat and now I'm paying for a flat without water now? Anne: Sorry, but... what? ...   \n",
       "11  Summarize the following conversation. Kenny: what time do you wanna meet the guys tm? Peter: hmm not sure Kenny: im free after 6 Peter: where we going Kenny: I just thought going downtown Peter: it may cost a lot Kenny: did you have a different idea? Peter: we could always go to my place Kenny: yeah we could do that Peter: may be boring but if we get a lot of people to come Kenny: yeah no could be fun lets do that a then Peter: ok ill send a text to everyone and see if they wanna come Kenny:...   \n",
       "12  Summarize the following conversation. Fiona: <unk>file_photo> Eliza: who's that hottie? <unk>3 Fiona: <unk>file_photo> Eliza: sweet! new hair? Fiona: yeah, got it done today! Eliza: going blonde was the best decision you ever made hun! Fiona: i know :D but i miss being a brunette sometimes Eliza: don't even say that, never go back!! Fiona: but I seemed smarter as a brunette :D hahahah Eliza: well... only seemed hahah :D Fiona: you witch!!! I will get you!!! Eliza: just kidding sweetheart :* ...   \n",
       "13  Summarize the following conversation. Megan: hey, did anyone see a stray pair of earphones in the room 306? i think i left them there after the class :( Derek: nope sorry Sally: me neither Megan: shit, i jut got em... serves me right for being such a mess i guess xd Derek: hahah it happens to me all the time Cher: where they blue? i found some :) Megan: yes! omg thank you so much! Cher: i didn't know who they belonged to tho Cher: i gave them to Prof. Johnson so you can pick them up on Monda...   \n",
       "14  Summarize the following conversation. Matt: I'll be there at 8:15pm or so. Matt: Waiting for you in the lobby. Jane: Cool. I'll come down. Jane: Let me know when you do get here. Matt: BTW any idea where we're going out to eat tonight? Jane: Nope. I asked the guys at reception but they reckon it is all dead expensive around here. Jane: Thought you might have some ideas. ;-) Matt: I was thinking of just going for a walk and see what we find. Jane: Kinda lovely idea but it's like minus 5 out t...   \n",
       "15  Summarize the following conversation. Kate: Good morning. Kai: Hi! How official! Kate: I wrote it at 4am Kai: I've noticed. Why? Kate: I had to get up early to catch the bus to the airport Kai: Where are you flying? Kate: To Antwerp! I'm fed up with Cambridge Kai: poor thing. Why? Kate: Just a stupid, elitist place without a soul. Or with a soul made of money. Kai: Try to rest a bit in Belgium, do not work too much. Kate: I have to work, but at least not in this soulless place. Kai: When are...   \n",
       "16  Summarize the following conversation. Ben: are you going to the edinburgh comedy festival this summer? Holly: of course :-D Holly: i never miss it Ben: can I tag along? i've never been Holly: are you serious? Holly: my whole year centers around it Ben: lol WOW, so it's THAT good? :-) Holly: YES!! Holly: and please tag along Holly: you are going to love it Ben: what are the highlights this year? Holly: there's a brother and a sister who tell jokes while juggling together Ben: mmm... ok -- not...   \n",
       "17                                                                                                                                       Summarize the following conversation. Susan: so your flyin 2moro? Patel: :( i have to Susan: what time is your flight? Patel: 5.30 in the morning Susan: I guess i better say goodbye now Patel: I miss you already. I'll be back in two weeks tho, <unk>file_gif> Susan: it's gonna be very long two weeks. Patel: I know. but we haev to go through it somehow Summary: </s>   \n",
       "18                                                                                         Summarize the following conversation. Charlotte: Have we got any plans for Friday evening? Ethan: I don't think so. Ethan: Let me check my diary. Ethan: Ah, right! I'm working late, till 20:00, but I have nothing planned afterwards. Ethan: Ideas? Charlotte: There this new movie I told you about. Charlotte: Would you like to see it? Ethan: Yeah, why not. Charlotte: Okey, I'll check in cinemas. :) Summary: </s>   \n",
       "19  Summarize the following conversation. Mimi: Hey there! We've just arrived! Who's up for a quick coffee? Eli: Me! Why don't you pop in this afternoon? Piero: I'll come as well if you do. Mimi: OK, what time? 5 pm? Eli: Great. Mimi: I'll bring some cookies! Piero: And I'll grab few beers for me and Damiano! Damiano: Sweet man. Eli: How long are you staying? Mimi: Until Xmas, then we're going back for few days to my parents' place. Damiano: We haven't decided about NY Eve. Maybe we can do somet...   \n",
       "\n",
       "                                                                                                                                                                                                                         response_before  \\\n",
       "0                                                                                                                                                                                     <pad> Missy's watch was funny. Clara is right.</s>   \n",
       "1                                                                                                                                                                                                <pad> Gloria needs Sean's carve in.</s>   \n",
       "2                                            <pad> Kate has won a full scholarship to St. Lawrence University. Kate applied for 1500 and was told the tuition was not handed out to her. The university earned $1,000 per trimester.</s>   \n",
       "3                                                                                                                                                                                       <pad> Restaurant wants 6 people for tonight.</s>   \n",
       "4                                                                                                             <pad> Ivy's snow pile in the basement is going to be white. Isaac thinks it will snow. Ivy's just not sure about this.</s>   \n",
       "5                                                                                                                        <pad> Michelle is not returning Lilly's phone calls. Michelle's going to be on a training course this week.</s>   \n",
       "6                                           <pad> Karen says that Harry Salvatore died suddenly. They have to celebrate the life of Harry, and have to plan a memorial service. They say they will attend an Italian place at 11 am.</s>   \n",
       "7                                                                                                                                                                                    <pad> Luke and Grace are going to The Avengers.</s>   \n",
       "8                                                                                                                                                                                      <pad> Dean taken his sick partner, a suspect.</s>   \n",
       "9                                                                                                                   <pad> Jane will take care of the report due on 2nd of August, then one control visit and then the control visit.</s>   \n",
       "10                                            <pad> Chris needs to buy a new house for his £25 owing to his lack of water, leaving her with as little money as possible. He is now responsible for bills which he is taking care of.</s>   \n",
       "11                                                                                    <pad> Peter and Kenny are going out tomorrow at Kenny's place. Peter wants the guys to come. He will let everyone bring drinks and some pizza.</s>   \n",
       "12                                  <pad> Fiona got expensive new brunettes' hair for a while. She misses being a brunette especially when she's brunette. Eliza is glad they are not going blonde to celebrate Fiona's anniversary.</s>   \n",
       "13  <pad> The pair of earphones Megan left in room 306 left there after the class. Cher attached the earplugs to Prof. Johnson so Megan can pick them up on Monday. Harry found 20 pairs of blue earplugs she gave to Prof. Johnson.</s>   \n",
       "14                                                                                                                                                          <pad> Matt will be waiting for Jane in the lobby. Jane hasn't seen Matt.</s>   \n",
       "15                                                                                <pad> Kate wrote a mysterious letter at 4am. Kate is leaving Belgium for Antwerp to visit her new colleague. She'll meet her supervisor on Monday.</s>   \n",
       "16                                                                                                                   <pad> Ben is likely to go to Edinburgh Comedy Festival but he's never been there. His whole year focuses on it.</s>   \n",
       "17                                                                                                      <pad> Patel needs to stay a week at home. He's planning a flight around 5.30 in the morning. He'll be back in several weeks.</s>   \n",
       "18                                                                                                                                                   <pad> Charlotte will check in cinemas for Friday evening. Charlotte works late.</s>   \n",
       "19                                                                                                                                     <pad> Mimi and Eli will meet this afternoon at 5 pm for a coffee if Ramo and Damiano want to.</s>   \n",
       "\n",
       "                                                                                                                                                                                                  response_after  \\\n",
       "0                                                                                                                                                                <pad> It was the song called Feel It Still.</s>   \n",
       "1                                                                                                                                                <pad> Gloria sends Sean to the baby concerned for his baby.</s>   \n",
       "2                                                                   <pad> Kate won a professorship scholarship at a university for $1000 grant. Melanie has written a delicious note of her students' night.</s>   \n",
       "3                                                                                                    <pad> The restaurant's invitation is'Top Shine' and seats 6 of 7 for special occasions tonight at 7 PM.</s>   \n",
       "4                                                                                                                                                                             <pad> It is snowing there now.</s>   \n",
       "5                                                                                                                                              <pad> Michelle doesn't return Lilly's number, as she planned.</s>   \n",
       "6                                     <pad> Harry was a great man. He used to prefer something Italian than a can ’t by many people. Karen normally reserved an apotheestage at the church on their waiting.</s>   \n",
       "7                                                                                                                                         <pad> Grace is going to show up with Luke after ending a long day.</s>   \n",
       "8                                                                                                                                                                <pad> Dean eats Chinese food, as did Scott.</s>   \n",
       "9                                                                                                                           <pad> The report consists of a control visit--Sanlin, the speaker--September 23.</s>   \n",
       "10                                                                               <pad> Chris has problems with water in the flat. Anne is unavailable. Anne is renting. Anne's providing the flat for Chris.</s>   \n",
       "11                                     <pad> Kenny will go with Peter during the funeral party at Peter's place after 6or he may meet a lot of people at Peter's too. Peter will have all bring some drinks.</s>   \n",
       "12                                                                                                                             <pad> Fiona gotten her blonder hair. Eliza approves Fiona'll try to kiss her.</s>   \n",
       "13                                                                                                                                                          <pad> Girl can not find her husband's earphones.</s>   \n",
       "14                                                      <pad> Matt will be at the hotel at 8:15pm. Jane will come down at 8:15pm. Matt has a question about the place that Jane and Jane are to make dinner.</s>   \n",
       "15                  <pad> Kate writes a letter to Kai. Kate got up early thus she will go to Antwerp. She has to stop a look on drunk. Kate is tired of Cambridge. Kai has to see her supervisor in Belgium.</s>   \n",
       "16                                                                                                           <pad> Ben'll join Holly for the Edinburgh comedy festival. Holly always too old to tag along :)</s>   \n",
       "17                                                                                                                                                 <pad> Patel's ailing brain. Pam will arrive in two weeks.</s>   \n",
       "18                                                                                                                <pad> Mama doesn't know what to do about Friday. She and Ethan will surely plan somewhere.</s>   \n",
       "19  <pad> Mimi has just arrived and she'll make some cookies and be able to have some drinks with Eli. Piero will come with him to the five o'clock for a coffee with Eli. Damiano doesn't want to meet her.</s>   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        1.998531      2.820851     0.822320  \n",
       "1        1.349928      2.080774     0.730845  \n",
       "2        1.810878      2.365676     0.554798  \n",
       "3        3.485358      3.784537     0.299179  \n",
       "4        2.024485      2.260841     0.236357  \n",
       "5        2.789807      2.873711     0.083904  \n",
       "6        3.929939      3.933781     0.003842  \n",
       "7        2.366690      2.366966     0.000275  \n",
       "8        1.702894      1.696505    -0.006389  \n",
       "9        3.913431      3.893194    -0.020237  \n",
       "10       1.732846      1.692643    -0.040203  \n",
       "11       2.860176      2.793499    -0.066677  \n",
       "12       1.218263      1.093494    -0.124769  \n",
       "13       1.334728      1.124544    -0.210183  \n",
       "14       2.589391      2.360463    -0.228927  \n",
       "15       1.398047      0.938702    -0.459345  \n",
       "16       2.812017      2.165207    -0.646810  \n",
       "17       2.196533      1.344703    -0.851830  \n",
       "18       3.489472      2.628898    -0.860574  \n",
       "19       3.546094      2.507572    -1.038522  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences you can observe a significant difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the labs, we saw implementation for zero-shot, one-shot and few-shot inference and how it affects the output. We also touched upon a bit of prompt engineering. We used different google models as our base model and build on to learn the concepts of in-context learning. We also implemented instruct model, also known as instruction fine-tuned model, and compared the results both manually and quantatively (ROUGE scores). We implemented PEFT models and discovered the difference in summarization outputs. We also fine-tuned our model to detoxify summaries by methods of reinforcement learning (feedback/rewards) and measured the reward difference qualitatively. \n",
    "\n",
    "In conclusion, the labs provided a comprehensive journey through various strategies for enhancing language models' capabilities, from prompt engineering to instruction fine-tuning, PEFT models, and reinforcement learning. This holistic exploration equipped us with a toolkit of techniques to tailor language models for specific tasks, showcasing the versatility and adaptability of modern natural language processing methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Generative AI with Large Language Models](https://www.coursera.org/learn/generative-ai-with-llms/home/welcome)\n",
    "- [samsum](https://huggingface.co/datasets/samsum)\n",
    "- [RohitKeswani/flan-t5-base-peft-samsum](https://huggingface.co/RohitKeswani/flan-t5-base-peft-samsum)\n",
    "- [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "- [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)\n",
    "- [mrm8488/flan-t5-small-finetuned-samsum](https://huggingface.co/mrm8488/flan-t5-small-finetuned-samsum)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
