{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content dictionary validation passed.\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "\"Grounds v. Ralph\" is a legal case that took place in 1875. The case was presented in the Arizona Supreme Court. The dispute involved money demand, where Ralph obtained a judgment for one hundred and fifty dollars in a lower court. Grounds appealed to the district court, where the judgment came in favor of Ralph again, but for sixty-two dollars and fifty cents only, along with costs. Grounds appealed again to the Arizona Supreme Court. The court discussed several legal principles and rules during the case, including grounds of objection, why they should be stated, and the appellate jurisdiction. The court concluded that the facts found by the court below are sufficient to sustain the judgment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"ENTER_YOUR_API_KEY\"\n",
    "\n",
    "# File paths\n",
    "FAISS_INDEX_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/faiss_index.index\"\n",
    "ID_MAPPING_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/id_mapping.pkl\"\n",
    "CONTENT_DICT_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/content_dict.pkl\"\n",
    "\n",
    "def load_faiss_index(faiss_index_path):\n",
    "    \"\"\"Load the FAISS index from the given path.\"\"\"\n",
    "    return faiss.read_index(faiss_index_path)\n",
    "\n",
    "def load_id_mapping(id_mapping_path):\n",
    "    \"\"\"Load the ID mapping from the given path.\"\"\"\n",
    "    with open(id_mapping_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_content_dict(content_dict_path):\n",
    "    \"\"\"Load the content dictionary from the given path.\"\"\"\n",
    "    with open(content_dict_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def generate_embedding_for_query(query):\n",
    "    \"\"\"Generate embedding for the user query using OpenAI API.\"\"\"\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=query\n",
    "        )\n",
    "        return np.array(response[\"data\"][0][\"embedding\"], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for query: {e}\")\n",
    "        return None\n",
    "\n",
    "def perform_semantic_search(query_embedding, faiss_index, content_dict, id_mapping, top_k=5):\n",
    "    \"\"\"Perform semantic search using FAISS to retrieve the top-k similar embeddings and distances.\"\"\"\n",
    "    # Search for the nearest neighbors\n",
    "    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(id_mapping):  # Check if the index is valid\n",
    "            result_id = id_mapping[idx] if isinstance(id_mapping, list) else id_mapping.get(idx, None)\n",
    "            if result_id:\n",
    "                # Retrieve corresponding file_name_chunk_no\n",
    "                for item in content_dict:\n",
    "                    if \"id\" in item and item[\"id\"] == result_id:\n",
    "                        results.append({\n",
    "                            \"file_name_chunk_no\": item.get(\"text_chunk\", \"No file_name_chunk_no available\"),\n",
    "                            \"distance\": dist\n",
    "                        })\n",
    "                        break\n",
    "    return results\n",
    "\n",
    "def validate_content_dict(content_dict):\n",
    "    \"\"\"Validate that the content_dict contains readable strings.\"\"\"\n",
    "    for item in content_dict:\n",
    "        assert isinstance(item.get(\"file_name_chunk_no\", \"\"), str), f\"Non-string file_name_chunk_no found: {item}\"\n",
    "    print(\"Content dictionary validation passed.\")\n",
    "\n",
    "def generate_response_with_rag(query, context):\n",
    "    \"\"\"Generate a response using RAG with OpenAI GPT.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions using the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Unable to generate response.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load FAISS index, ID mapping, and content dictionary\n",
    "    faiss_index = load_faiss_index(FAISS_INDEX_PATH)\n",
    "    id_mapping = load_id_mapping(ID_MAPPING_PATH)\n",
    "    content_dict = load_content_dict(CONTENT_DICT_PATH)\n",
    "\n",
    "    # Step 2: Validate the content dictionary\n",
    "    validate_content_dict(content_dict)\n",
    "\n",
    "    # Step 3: Take user input\n",
    "    user_query = input(\"Enter your question or query: \")\n",
    "\n",
    "    # Step 4: Generate embedding for the user query\n",
    "    query_embedding = generate_embedding_for_query(user_query)\n",
    "\n",
    "    if query_embedding is not None:\n",
    "        # Step 5: Perform semantic search\n",
    "        results = perform_semantic_search(query_embedding, faiss_index, content_dict, id_mapping, top_k=5)\n",
    "\n",
    "\n",
    "        # Step 6: Prepare the context from retrieved results\n",
    "        context = \"\\n\\n\".join([result[\"file_name_chunk_no\"] for result in results if result[\"file_name_chunk_no\"]])\n",
    "\n",
    "\n",
    "        # Step 7: Generate a response using RAG\n",
    "        response = generate_response_with_rag(user_query, context)\n",
    "\n",
    "        # Step 8: Display the generated response\n",
    "        print(\"\\nGenerated Response:\\n\")\n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"Failed to generate query embedding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'embedding'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index file found: C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/faiss_index.index\n",
      "ID mapping file found: C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/id_mapping.pkl\n",
      "Content dictionary file found: C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/output_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define file paths\n",
    "FAISS_INDEX_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/faiss_index.index\"\n",
    "ID_MAPPING_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/id_mapping.pkl\"\n",
    "CONTENT_DICT_PATH = \"C:/Users/chiranjit usa/OneDrive - Northeastern University/Desktop/vector_database_project/output_embeddings.pkl\"\n",
    "\n",
    "# Check if files exist\n",
    "def check_file_exists(file_path, file_description):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"{file_description} found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Error: {file_description} not found at: {file_path}\")\n",
    "\n",
    "# Check each file\n",
    "check_file_exists(FAISS_INDEX_PATH, \"FAISS index file\")\n",
    "check_file_exists(ID_MAPPING_PATH, \"ID mapping file\")\n",
    "check_file_exists(CONTENT_DICT_PATH, \"Content dictionary file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
