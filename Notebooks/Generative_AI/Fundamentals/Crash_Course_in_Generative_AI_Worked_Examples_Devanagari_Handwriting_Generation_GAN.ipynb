{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owwseaU8wDN9"
      },
      "source": [
        "# Devanagari Handwriting Generation using GANs\n",
        "\n",
        "Author : Rutuja More"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrDCCG6wraHP"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Handwritten character recognition is a challenging area of research that has significant applications in natural language processing, document analysis, and automated data entry. The Devanagari script, used extensively across India and Nepal, includes complex character structures which pose unique challenges for optical character recognition (OCR) systems. To improve the performance of these systems, a robust dataset of handwritten characters is essential.\n",
        "\n",
        "The main objective here is to employ Generative Adversarial Networks (GANs) to generate synthetic images of handwritten Devanagari characters.\n",
        "\n",
        "### About the dataset:\n",
        "Dataset link : https://archive.ics.uci.edu/dataset/389/devanagari+handwritten+character+dataset\n",
        "\n",
        " The dataset used in this project comprises handwritten Devanagari characters, one of the most widely adopted writing systems in India and Nepal. This extensive dataset includes 46 distinct classes of characters, each represented by 2,000 samples, totaling 92,000 images. The characters encompass a broad array of Devanagari script, including vowels, consonants, and numerals, reflecting the script's intrinsic diversity and complexity. The dataset is meticulously organized into two main subsets: a training set, which constitutes 85% of the total data (approximately 78,200 images), and a testing set, making up the remaining 15% (approximately 13,800 images). Each image is stored as a grayscale PNG file, standardized to a uniform size to facilitate straightforward processing and analysis. This structured dataset provides a solid foundation for training the Generative Adversarial Network (GAN), aiming to enhance machine learning models' ability to accurately recognize and interpret handwritten Devanagari characters.\n",
        "\n",
        "\n",
        "### Problem Addressed:\n",
        "Handwritten character recognition, especially for complex scripts like Devanagari, presents significant challenges due to variations in writing styles, strokes, and character structures. These challenges hinder the performance of OCR systems, impacting tasks such as document analysis, natural language processing, and automated data entry.\n",
        "\n",
        "### Generative AI Concepts Utilized:\n",
        "Generative Adversarial Networks (GANs): GANs are composed of two neural networks, the generator and the discriminator, which are trained simultaneously through a competitive process. The generator aims to produce realistic synthetic data, while the discriminator aims to differentiate between real and synthetic data. This adversarial training leads to the generator improving over time to generate more convincing outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MdOjNWrRjCG2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKehXMLzth_1"
      },
      "source": [
        "## Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB8SRv3Eju5M",
        "outputId": "df903b48-32e6-4809-bc5b-8aebc2ea9df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/devanagari+handwritten+character+dataset.zip'\n",
        "\n",
        "# Destination directory to extract the files\n",
        "extracted_dir_path = '/content/extracted_files'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BAO1wl8JkAbS"
      },
      "outputs": [],
      "source": [
        "# Image size (resize if necessary)\n",
        "img_height, img_width = 64, 64  # Change this according to your GAN input layer\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 46"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA81hMozt5qt"
      },
      "source": [
        "This code snippet sets essential parameters for image processing and model configuration in a machine learning project. It defines the dimensions (`img_height` and `img_width`) to which all images will be resized, ensuring uniformity for model input, and specifies the total number of unique classes (`num_classes`) represented in the dataset, crucial for categorizing and training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxFJpzE_uHAW"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xIn2SdKHkGYn"
      },
      "outputs": [],
      "source": [
        "def load_data(directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label = 0\n",
        "\n",
        "    # List each sub-directory (character folder)\n",
        "    for folder in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "\n",
        "        # Process each image in the character folder\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith('.png'):\n",
        "                # Load and resize the image\n",
        "                image_path = os.path.join(folder_path, file)\n",
        "                image = load_img(image_path, color_mode='grayscale', target_size=(img_height, img_width))\n",
        "                image = img_to_array(image)\n",
        "                image /= 255.0  # Normalize to [0, 1]\n",
        "\n",
        "                images.append(image)\n",
        "                labels.append(label)\n",
        "        label += 1\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7nfWrf6uL1c"
      },
      "source": [
        "This function, load_data, is designed to process and prepare image data for machine learning models. It reads images from a specified directory, where each sub-directory represents a class of images (in this case, different Devanagari characters). Each image is loaded, resized to a consistent dimension, converted to grayscale, normalized to a range between 0 and 1, and then stored in an array along with its corresponding label. The function ultimately returns arrays of these processed images and their labels, ready for training or testing purposes in a neural network model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnWwZjKaulVG"
      },
      "source": [
        "Let's preprocess the training and testing data to ensure our model receives uniformly formatted and well-structured input. This involves loading the images from specified directories, resizing and normalizing them, and converting the labels to a format suitable for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zMO736ERkKsp"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/extracted_files/DevanagariHandwrittenCharacterDataset'\n",
        "train_images, train_labels = load_data(os.path.join(base_dir, 'Train'))\n",
        "test_images, test_labels = load_data(os.path.join(base_dir, 'Test'))\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ASO1bsXNkzx9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8nANClduzxF"
      },
      "source": [
        "In our GAN architecture, the generator plays a crucial role in creating synthetic images of Devanagari characters. Below is the implementation of the generator model, which uses a series of dense layers and activation functions to transform a noise vector into a 2D image that mimics the characteristics of handwritten characters. The model progressively increases the complexity of features through its layers, culminating in a reshaping layer that formats the output into the dimensions of a typical image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3A3i6cyZk3Na"
      },
      "outputs": [],
      "source": [
        "def build_generator(z_dim):\n",
        "    model = Sequential([\n",
        "        Dense(256, input_dim=z_dim),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Dense(1024),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Dense(img_height * img_width, activation='tanh'),  # Output layer with size of the image\n",
        "        Reshape((img_height, img_width, 1))  # Reshape output to image dimensions\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InVZs4W2u6gU"
      },
      "source": [
        "In the GAN framework, the discriminator assesses the authenticity of images, distinguishing real handwriting samples from the synthetic images generated by the companion generator. The following code outlines the construction of the discriminator model, which processes input images through a series of dense layers equipped with LeakyReLU activations to enhance non-linear learning. The final output is produced through a sigmoid activation function, categorizing images as either 'real' or 'fake' based on the learned features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XI_9EDVfk7Bw"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(img_shape):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwM9sjnOvAkA"
      },
      "source": [
        "The GAN is structured by combining the previously defined generator and discriminator into a single sequential model. This code defines how the generator and discriminator interact within the GAN framework. While training the GAN, the discriminator’s weights are frozen to allow the generator to improve its ability to produce more realistic images without the discriminator getting too effective too quickly. The combined model thus facilitates the adversarial training process where the generator learns to generate plausible images, and the discriminator learns to differentiate between real and fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-xFZKYLek9tL"
      },
      "outputs": [],
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    # Combine generator and discriminator\n",
        "    model.add(generator)\n",
        "    discriminator.trainable = False  # Set the discriminator to be not trainable when training generator\n",
        "    model.add(discriminator)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W-QWymIvMsR"
      },
      "source": [
        "In this section, we configure the key elements of our GAN by setting hyperparameters, building, and compiling the discriminator and generator models. We define z_dim as the dimension of the noise vector that feeds into the generator, and set a learning rate for the optimizer. The discriminator is compiled with binary crossentropy loss to classify images as real or fake effectively. Similarly, the generator is built using the same noise vector dimension. Finally, we combine these two models into the GAN architecture, which is also compiled with binary crossentropy loss, using the Adam optimizer to enhance training stability and efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UzMIKlrZlAx3"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "z_dim = 100  # Dimension of the noise vector\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator((img_height, img_width, 1))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "# Build and compile the GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx7I4HEHvUpr"
      },
      "source": [
        "This function train_discriminator is responsible for training the discriminator model to differentiate effectively between real and synthetic images. It first generates a batch of fake images using the generator model from a noise vector. Simultaneously, it selects a random batch of real images from the dataset. The discriminator is then trained separately on these real and fake images, updating its weights to minimize loss and maximize accuracy in identifying the authenticity of images. The function computes and returns the average loss and accuracy from these two training steps, providing insights into the discriminator's performance over each batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6Wuge4COl2Kb"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(model, batch_size, generator, real_images):\n",
        "    # Generate fake images\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    fake_images = generator.predict(noise)\n",
        "\n",
        "    # Get real images randomly from the dataset\n",
        "    idx = np.random.randint(0, real_images.shape[0], batch_size)\n",
        "    real_images_sample = real_images[idx]\n",
        "\n",
        "    # Labels for fake and real images\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = model.train_on_batch(real_images_sample, real_labels)\n",
        "    d_loss_fake = model.train_on_batch(fake_images, fake_labels)\n",
        "\n",
        "    # Return the average loss and accuracy\n",
        "    d_loss, d_acc = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    return d_loss, d_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCfZOytGvZ5f"
      },
      "source": [
        "Lets now write a function focuses on enhancing the generator's capability to produce images that are indistinguishable from real images, thus 'fooling' the discriminator. It operates by generating a batch of noise vectors and using these as input to the GAN model, where the generator's output is assessed by the discriminator. The key here is that the generator is trained to maximize the mistake of the discriminator, essentially training it to believe that the synthetic images are real. The loss from this process, indicative of the generator's performance, is calculated and returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jnwgHRLol5Z0"
      },
      "outputs": [],
      "source": [
        "def train_generator(model, batch_size):\n",
        "    # Generate random noise\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    valid_y = np.ones((batch_size, 1))  # Labels for the generator (trying to fool the discriminator)\n",
        "\n",
        "    # Train the generator (note that we use the GAN model here)\n",
        "    g_loss = model.train_on_batch(noise, valid_y)\n",
        "    return g_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHupron4vwvd"
      },
      "source": [
        " Lets set up and execute the training process for a Generative Adversarial Network (GAN) designed to generate synthetic images of handwritten Devanagari character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwEx82Ull7zq",
        "outputId": "cb6908d6-2f3c-44c4-b040-3f808dd52f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 17ms/step\n",
            "4/4 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "Epoch: 100, Discriminator Loss: 0.0196, Discriminator Accuracy: 1.00, Generator Loss: 6.1503\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 23ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 14ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 14ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 21ms/step\n",
            "Epoch: 200, Discriminator Loss: 0.0272, Discriminator Accuracy: 1.00, Generator Loss: 6.1929\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 14ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 14ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 16ms/step\n",
            "4/4 [==============================] - 0s 24ms/step\n",
            "4/4 [==============================] - 0s 18ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "Epoch: 300, Discriminator Loss: 0.0201, Discriminator Accuracy: 1.00, Generator Loss: 5.7934\n"
          ]
        }
      ],
      "source": [
        "epochs = 300  # Number of epochs, reduced for quicker iteration\n",
        "batch_size = 128  # Size of the batch\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train discriminator\n",
        "    d_loss, d_acc = train_discriminator(discriminator, batch_size, generator, train_images)\n",
        "\n",
        "    # Train generator\n",
        "    g_loss = train_generator(gan, batch_size)\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Discriminator Loss: {d_loss:.4f}, Discriminator Accuracy: {d_acc:.2f}, Generator Loss: {g_loss:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKhAhq7sv_St"
      },
      "source": [
        "The output from the final epoch of your GAN training session provides some important insights into how your models, the discriminator and generator, are performing:\n",
        "\n",
        "Discriminator Loss (0.0374): This relatively low loss suggests that the discriminator is quite effective at distinguishing between real and fake images. It is quite adept at its task, as indicated by the next metric.\n",
        "\n",
        "Discriminator Accuracy (1.00): An accuracy of 1.00, or 100%, means the discriminator is correctly identifying all the real images as real and all the fake images as fake. While this might initially seem ideal, in the context of GAN training, a perfect discriminator score can indicate that the discriminator is too strong compared to the generator, which can stifle the generator’s learning process.\n",
        "\n",
        "Generator Loss (6.9609): The high loss for the generator indicates that it is still struggling to fool the discriminator. The generator's main objective is to produce images that the discriminator will classify as real. A high loss here suggests that most of the time, the discriminator can easily tell that the images generated by the generator are not authentic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IREy-bxj58UI"
      },
      "source": [
        "### Impact and Results:\n",
        "Dataset Enhancement: By generating synthetic images of handwritten Devanagari characters, the project contributes to enhancing the dataset available for training OCR systems. This enriched dataset can improve the accuracy and robustness of OCR models tailored to the Devanagari script.\n",
        "\n",
        "### Future Directions:\n",
        "The project outlines potential future directions, including refining the generator's ability to produce varied character forms and exploring advanced GAN architectures and training techniques. These efforts could further enhance the quality of synthetic images and broaden the applicability of the methodology to other languages and scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_XdY9Hs-iA"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "This project successfully demonstrated the application of Generative Adversarial Networks (GANs) to generate synthetic images of handwritten Devanagari characters. Through the implementation of a robust GAN architecture, comprising a discriminator and a generator, we were able to produce a diverse array of synthetic character images that closely mimic the variations found in genuine handwritten samples.\n",
        "\n",
        "The outcomes of this project have promising implications for the field of handwritten character recognition. Firstly, the enhanced dataset can potentially improve the accuracy and generalizability of OCR models tailored to the Devanagari script, which is critical given its widespread use and inherent complexities. Furthermore, the methodology employed can be adapted to other scripts and applications, showcasing the versatility and scalability of GANs in generating synthetic data.\n",
        "\n",
        "Future work could focus on refining the generator's ability to produce even more varied and challenging character forms, such as those influenced by different handwriting styles and distortions. Additionally, implementing advanced GAN architectures and training techniques could further improve the quality of the synthetic images. Continued research in this area holds the potential to transform data augmentation practices in machine learning, making OCR systems more robust and effective across a wider range of languages and scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBd_JXDGsPhe"
      },
      "source": [
        "## References\n",
        "\n",
        "1. https://github.com/amzn/convolutional-handwriting-gan\n",
        "2. https://towardsdatascience.com/scrabblegan-adversarial-generation-of-handwritten-text-images-628f8edcfeed\n",
        "3. https://www.youtube.com/watch?v=cTlxZ1FO1mY\n",
        "4. https://www.youtube.com/watch?v=GmD_2Q-x6dc\n",
        "5. https://www.chalisebibek.com.np/devanagari-handwriting-and-letters-generation-with-deep-convolution-generative-adversarial-network-dcgan\n",
        "6. https://github.com/sushant097/Devnagari-Handwritten-Word-Recongition-with-Deep-Learning\n",
        "7. https://ieeexplore.ieee.org/document/9673390\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}