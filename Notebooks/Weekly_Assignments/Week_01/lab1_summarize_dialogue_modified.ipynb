{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d528ca",
   "metadata": {},
   "source": [
    "# Brief introduction to the course “Generative AI with Large Language Models (LLMs)”:\n",
    "\n",
    "- The course provides a deep understanding of generative AI and the lifecycle of a typical LLM-based generative AI model, including data gathering, model selection, performance evaluation, and deployment.\n",
    "- It covers the transformer architecture that powers LLMs, their training process, and how fine-tuning enables LLMs to adapt to various specific use cases.\n",
    "- The course teaches how to use empirical scaling laws to optimize the model’s objective function across dataset size, compute budget, and inference requirements.\n",
    "- It includes state-of-the-art training, tuning, inference, tools, and deployment methods to maximize the performance of models within the specific constraints of a project.\n",
    "- The course discusses the challenges and opportunities that generative AI creates for businesses, with insights from industry researchers and practitioners.\n",
    "- It is designed to help developers with a good foundational understanding of how LLMs work and the best practices behind training and deploying them, enabling them to make informed decisions for their companies and build working prototypes more quickly.\n",
    "- The course is structured into weeks, each with specific learning objectives, labs, and quizzes.\n",
    "1. Week 1 focuses on generative AI use cases, the project lifecycle, and model pre-training.\n",
    "2. Week 2 covers fine-tuning and evaluating large language models, including overcoming catastrophic forgetting and Parameter-efficient Fine Tuning (PEFT).\n",
    "3. Week 3 delves into reinforcement learning and LLM-powered applications, discussing how RLHF uses human feedback to improve the performance and alignment of large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4ac28",
   "metadata": {},
   "source": [
    "# **Lab 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea33584",
   "metadata": {},
   "source": [
    "# Generative AI Use Case: Summarize Dialogue\n",
    "Welcome to the practical side of this course. In this lab you will do the dialogue summarization task using generative AI. You will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need. By comparing zero shot, one shot, and few shot inferences, you will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797378f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1 - Set up Kernel and Required Dependencies\n",
    "\n",
    "2 - Summarize Dialogue without Prompt Engineering\n",
    "\n",
    "2.1 - Dataset Card for SAMSum Corpus\n",
    "\n",
    "2.2 - T5-Base Model\n",
    "\n",
    "2.3 - Why T5-Base model was choosen instead of FLAN-T5\n",
    "\n",
    "2.4 - Overview of Model Generation Approach\n",
    "\n",
    "3 - Summarize Dialogue with an Instruction Prompt\n",
    "\n",
    "3.1 - Zero Shot Inference with an Instruction Prompt\n",
    "\n",
    "3.2 - Zero Shot Inference with the Prompt Template from T5-Base model\n",
    "\n",
    "4 - Summarize Dialogue with One Shot and Few Shot Inference\n",
    "\n",
    "4.1 - One Shot Inference\n",
    "\n",
    "4.2 - Few Shot Inference\n",
    "\n",
    "5 - Generative Configuration Parameters for Inference\n",
    "\n",
    "6 - Challenges and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba8ceb",
   "metadata": {},
   "source": [
    "### 1 - Set up Kernel and Required Dependencies\n",
    "First, check that the correct kernel is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f7c0e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:14.849530Z",
     "start_time": "2023-11-21T23:05:52.973011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b52a5b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:22.874531Z",
     "start_time": "2023-11-21T23:06:14.858512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.20.8)\n",
      "Requirement already satisfied: texttable in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (3.18.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dell\\anaconda3\\lib\\site-packages (from py7zr) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc829610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:30.814294Z",
     "start_time": "2023-11-21T23:06:22.880505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02210b56",
   "metadata": {},
   "source": [
    "### 2 - Summarize Dialogue without Prompt Engineering\n",
    "In this use case, you will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) T5-Base from Hugging Face. The list of available models in the Hugging Face transformers package can be found here.\n",
    "\n",
    "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1845f3",
   "metadata": {},
   "source": [
    "### 2.1 Dataset Card for SAMSum Corpus\n",
    "Dataset Summary\n",
    "- The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "- Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. \n",
    "- The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. \n",
    "- Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. \n",
    "- The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791d43fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:36.927307Z",
     "start_time": "2023-11-21T23:06:30.827291Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/Dell/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 50.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('samsum')\n",
    "\n",
    "# Get the 'train' Dataset\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94c5d8",
   "metadata": {},
   "source": [
    "Print a couple of dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f05334fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:36.959257Z",
     "start_time": "2023-11-21T23:06:36.932239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Example  1\n",
      "--------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Example  2\n",
      "--------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Eric: MACHINE!\r\n",
      "Rob: That's so gr8!\r\n",
      "Eric: I know! And shows how Americans see Russian ;)\r\n",
      "Rob: And it's really funny!\r\n",
      "Eric: I know! I especially like the train part!\r\n",
      "Rob: Hahaha! No one talks to the machine like that!\r\n",
      "Eric: Is this his only stand-up?\r\n",
      "Rob: Idk. I'll check.\r\n",
      "Eric: Sure.\r\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\r\n",
      "Eric: Gr8! I'll watch them now!\r\n",
      "Rob: Me too!\r\n",
      "Eric: MACHINE!\r\n",
      "Rob: MACHINE!\r\n",
      "Eric: TTYL?\r\n",
      "Rob: Sure :)\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the indices of the examples you want to print\n",
    "example_indices = [0, 1]  # Replace with your desired indices\n",
    "\n",
    "# Define a dashed line for formatting\n",
    "dash_line = '-' * 50\n",
    "\n",
    "# Iterate over the example indices and print the dialogue and summary for each one\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216246d",
   "metadata": {},
   "source": [
    "### 2.2 T5-Base model\n",
    "- developed by Google, is a Text-To-Text Transfer Transformer (T5). It reframes all NLP tasks into a unified text-to-text format where both the input and output are always text strings. This is in contrast to BERT-style models that can only output either a class label or a span of the input. The T5-Base model, with 220 million parameters, can be fine-tuned to perform a wide range of natural language understanding tasks, such as text classification, language translation, question-answering, and even regression tasks.\n",
    "\n",
    "- The T5 model is trained using teacher forcing, which means that for training, it always needs an input sequence and a corresponding target sequence. It’s trained on a massive amount of text data, which allows it to understand and generate a wide range of natural language.\n",
    "\n",
    "- The T5 model does not work with raw text. Instead, it requires the text to be transformed into numerical form in order to perform training and inference. The following transformations are required for the T5 model:\n",
    "\n",
    "1. Tokenize text\n",
    "2. Convert tokens into (integer) IDs\n",
    "3. Truncate the sequences to a specified maximum length\n",
    "4. Add end-of-sequence (EOS) and padding token IDs\n",
    "5. T5 uses a SentencePiece model for text tokenization. It’s a powerful tool that allows the model to handle a variety of NLP tasks with the same model, loss function, and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ff83c",
   "metadata": {},
   "source": [
    "### 2.3 - Why T5-Base model was choosen instead of FLAN-T5  -\n",
    "- Model Size and Efficiency: T5-base is a smaller model compared to FLAN-T5. This could lead to faster training and inference times, making it more suitable for applications with real-time requirements or limited computational resources.\n",
    "\n",
    "- Generalization: T5-base is pre-trained on a diverse range of internet text. Therefore, it might be better at generalizing to various tasks and domains compared to FLAN-T5, which is specifically designed for few-shot learning.\n",
    "\n",
    "- Flexibility: T5-base provides more flexibility as it can be fine-tuned for a wide range of tasks. This could be beneficial if you plan to extend your project to include more tasks in the future.\n",
    "\n",
    "- Performance: You might have found in preliminary experiments that T5-base outperforms FLAN-T5 on your specific task. It’s always important to choose the model that gives the best performance on your specific task.\n",
    "\n",
    "- Simplicity: T5-base could be easier to work with if you’re not familiar with the few-shot learning setup used by FLAN-T5. It’s always a good idea to choose a model that aligns with your level of expertise and comfort.\n",
    "\n",
    "- Resource Availability: There might be more community resources, tutorials, and support available for working with T5-base compared to FLAN-T5. This can speed up development time and help you overcome any challenges you encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e615a",
   "metadata": {},
   "source": [
    "### 2.4 - Overview of Model Generation Approach\n",
    "- In this code snippet, we employ a two-step process for text generation using a pre-trained transformer model. The key improvement lies in how the input prompt is prepared for the model.\n",
    "\n",
    "- Tokenization: The input prompt, derived from a dialogue, is formatted and tokenized using the Hugging Face transformers library's tokenizer. This step converts the string into a PyTorch tensor, ensuring compatibility with the model's input requirements.\n",
    "\n",
    "- Model Generation: The tokenized input is then passed to the model's generate method, allowing the transformer to produce a coherent output. Parameters such as max_length, num_beams, and temperature are adjusted based on the desired output characteristics.\n",
    "\n",
    "- This refined approach not only resolves potential errors but also provides a cleaner and more standardized method for generating text with transformer models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3739366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:48.822963Z",
     "start_time": "2023-11-21T23:06:36.963233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c324a16",
   "metadata": {},
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd7fd25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:06:57.092510Z",
     "start_time": "2023-11-21T23:06:48.827900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([21542,    10,    27, 13635,  5081,     5,   531,    25,   241,   128,\n",
      "           58, 16637,    10, 10625,    55, 21542,    10,    27,    31,   195,\n",
      "          830,    25,  5721,     3,    10,    18,    61,     1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)</s>\n"
     ]
    }
   ],
   "source": [
    "# Choose a sentence from the dataset\n",
    "sentence = train_dataset[0]['dialogue']\n",
    "\n",
    "# Encode the sentence\n",
    "sentence_encoded = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Print the encoded sentence\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[0])\n",
    "\n",
    "# Decode the sentence\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[0])\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd01bb",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "278bdbd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:07:10.319332Z",
     "start_time": "2023-11-21T23:06:57.097514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "Baseline Summary: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "Model Generated Summary: <pad> Jerry: Sure! Amanda: I baked cookies. Do you want some? Amanda: I'll bring you some tomorrow :-) Jerry: I baked cookies. Jerry: I baked cookies.</s>\n",
      "Input: Olivia: Who are you voting for in this election? \n",
      "Oliver: Liberals as always.\n",
      "Olivia: Me too!!\n",
      "Oliver: Great\n",
      "Baseline Summary: Olivia and Olivier are voting for liberals in this election. \n",
      "Model Generated Summary: <pad> Olivia: Me too!! Oliver: Liberals as always. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you.</s>\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    # Preprocess text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Generate summaries for the first two examples in the dataset\n",
    "for i in range(2):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"Input: {example['dialogue']}\")\n",
    "    print(f\"Baseline Summary: {example['summary']}\")\n",
    "    print(f\"Model Generated Summary: {generate_summary(example['dialogue'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1db5b",
   "metadata": {},
   "source": [
    "### 3 - Summarize Dialogue with an Instruction Prompt\n",
    "Prompt engineering is an important concept in using foundation models for text generation. You can check out this blog from Amazon Science for a quick introduction to prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c600c4",
   "metadata": {},
   "source": [
    "### 3.1 - Zero Shot Inference with an Instruction Prompt\n",
    "In order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called zero shot inference. You can check out this blog from AWS for a quick description of what zero shot learning is and why it is an important concept to the LLM model.\n",
    "\n",
    "Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c52d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:07:24.809341Z",
     "start_time": "2023-11-21T23:07:10.323326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "Baseline Summary: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "--------------------------------------------------\n",
      "Example  1\n",
      "--------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "summarize\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "<pad> Jerry: Sure! Amanda: I baked cookies. Do you want some? Amanda: I'll bring you some tomorrow :-) Jerry: I baked cookies. Jerry: I baked cookies.</s>\n",
      "\n",
      "Input: Olivia: Who are you voting for in this election? \n",
      "Oliver: Liberals as always.\n",
      "Olivia: Me too!!\n",
      "Oliver: Great\n",
      "Baseline Summary: Olivia and Olivier are voting for liberals in this election. \n",
      "--------------------------------------------------\n",
      "Example  2\n",
      "--------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "summarize\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Olivia and Olivier are voting for liberals in this election. \n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "<pad> Olivia: Me too!! Oliver: Liberals as always. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you. Olivia: Great to hear from you.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary\n",
    "def generate_summary(text, zero_shot=False, prompt=None):\n",
    "    if zero_shot:\n",
    "        assert prompt is not None, \"Prompt is required for zero-shot summarization.\"\n",
    "        inputs = tokenizer.encode(prompt + \": \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    else:\n",
    "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Generate summaries for the first two examples in the dataset\n",
    "for i in range(2):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"Input: {example['dialogue']}\")\n",
    "    print(f\"Baseline Summary: {example['summary']}\")\n",
    "    \n",
    "    # Zero-shot summarization\n",
    "    prompt = \"summarize\"\n",
    "    summary = example['summary']\n",
    "    output = generate_summary(example['dialogue'], zero_shot=True, prompt=prompt)\n",
    "\n",
    "    # Print results\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64e6a2",
   "metadata": {},
   "source": [
    "zero shot with with the Prompt Template from T5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a82bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:07:40.548329Z",
     "start_time": "2023-11-21T23:07:24.825331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "Baseline Summary: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "--------------------------------------------------\n",
      "Example  1\n",
      "--------------------------------------------------\n",
      "PROMPT TEMPLATE:\n",
      "Generate a summary for the following text: {}\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH TEMPLATE:\n",
      "<pad> Jerry: I baked cookies. Do you want some? Amanda: I baked cookies. Do you want some? Amanda: I baked cookies. Do you want some? Amanda: I baked cookies. Do you want some? Jerry: Yes!</s>\n",
      "\n",
      "Input: Olivia: Who are you voting for in this election? \n",
      "Oliver: Liberals as always.\n",
      "Olivia: Me too!!\n",
      "Oliver: Great\n",
      "Baseline Summary: Olivia and Olivier are voting for liberals in this election. \n",
      "--------------------------------------------------\n",
      "Example  2\n",
      "--------------------------------------------------\n",
      "PROMPT TEMPLATE:\n",
      "Generate a summary for the following text: {}\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Olivia and Olivier are voting for liberals in this election. \n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH TEMPLATE:\n",
      "<pad> Liberals as always. Oliver: Great to see a Liberal candidate in this election. Olivia: Liberals as always. Oliver: Great to see a Liberal candidate in this election.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary with a prompt template\n",
    "def generate_summary_with_template(text, template_prompt):\n",
    "    prompt = template_prompt.format(text)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Generate summaries for the first two examples in the dataset\n",
    "for i in range(2):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"Input: {example['dialogue']}\")\n",
    "    print(f\"Baseline Summary: {example['summary']}\")\n",
    "\n",
    "    # Zero-shot summarization with a prompt template\n",
    "    template_prompt = \"Generate a summary for the following text: {}\"\n",
    "    summary = example['summary']\n",
    "    output = generate_summary_with_template(example['dialogue'], template_prompt)\n",
    "\n",
    "    # Print results\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'PROMPT TEMPLATE:\\n{template_prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT WITH TEMPLATE:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609b9f5",
   "metadata": {},
   "source": [
    "### Observation: \n",
    "- The T5-base model is able to generate summaries from the given conversations, demonstrating its capability to understand and process conversational data.\n",
    "- The model is able to identify and repeat key phrases from the conversation, indicating its attention to important details.\n",
    "- Despite the challenges of zero-shot learning, the model is able to generate coherent sentences and maintain the conversational context in its summaries.\n",
    "- The model’s performance indicates a strong foundation that can be further improved with fine-tuning, prompt engineering, or other optimization techniques.\n",
    "- The model’s ability to generalize from seen to unseen tasks without explicit task-specific training data is a testament to its versatility and adaptability.\n",
    "- The observations provide valuable insights for future improvements, highlighting the potential for enhanced performance with different models, training strategies, or data augmentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70536f92",
   "metadata": {},
   "source": [
    "## 4 - Summarize Dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838884b",
   "metadata": {},
   "source": [
    "- One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. \n",
    "- This is called \"in-context learning\" and puts your model into a state that understands your specific task. You can read more about it in this blog from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e81a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:08:11.315326Z",
     "start_time": "2023-11-21T23:07:40.552328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE INDICES FOR ONE-SHOT INFERENCE: [0, 1, 2]\n",
      "--------------------------------------------------\n",
      "PROMPT TEMPLATE:\n",
      "Generate a summary for the following text: {}\n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT INFERENCE:\n",
      "<pad> Falkirk: Amanda baked cookies. Do you want some? Oliver: Liberals as always! Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing? Kim: Oh you know, uni stuff and unfucking my room Kim: Maybe tomorrow I'll move my ass and do everything Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary with one-shot inference\n",
    "def generate_summary_one_shot(example_indices_full, example_index_to_summarize, template_prompt):\n",
    "    # Combine all examples in the list to create a prompt with full examples\n",
    "    full_text = \" \".join([train_dataset[idx]['dialogue'] for idx in example_indices_full])\n",
    "    prompt = template_prompt.format(full_text)\n",
    "\n",
    "    # Add the specific example to summarize at the end\n",
    "    prompt += train_dataset[example_index_to_summarize]['dialogue']\n",
    "\n",
    "    # Tokenize and generate summary\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "example_indices_full = [0, 1, 2]  # List of example indices for one-shot inference\n",
    "example_index_to_summarize = 2  # Index of the example to summarize at the end\n",
    "template_prompt = \"Generate a summary for the following text: {}\"\n",
    "\n",
    "output_summary = generate_summary_one_shot(example_indices_full, example_index_to_summarize, template_prompt)\n",
    "\n",
    "# Print results\n",
    "print(f'EXAMPLE INDICES FOR ONE-SHOT INFERENCE: {example_indices_full}')\n",
    "print(dash_line)\n",
    "print(f'PROMPT TEMPLATE:\\n{template_prompt}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT INFERENCE:\\n{output_summary}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f65cad",
   "metadata": {},
   "source": [
    "### Observation - \n",
    "- The model is able to generate a summary that includes key points from multiple sentences, demonstrating its ability to extract important information from a larger context.\n",
    "- The summary includes actions (Amanda baking cookies, Kim’s plans for tomorrow), opinions (Oliver voting for Liberals), and states of mind (Kim’s mood and procrastination), showing the model’s capability to understand different types of information.\n",
    "- The model’s performance in one-shot inference indicates that it can adapt to different tasks with a single example, showcasing its flexibility and generalization ability.\n",
    "- The use of a prompt template in one-shot inference seems to guide the model more effectively towards the desired output, suggesting that prompt engineering can significantly enhance the model’s performance.\n",
    "- The improvement in the model’s performance from zero-shot to one-shot inference highlights the potential benefits of incorporating example-based learning in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098b08f",
   "metadata": {},
   "source": [
    "### 4.2 - Few Shot Inference\n",
    "- Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfe8334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:08:30.432325Z",
     "start_time": "2023-11-21T23:08:11.321328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE INDICES FOR FEW-SHOT INFERENCE: [0, 1, 2]\n",
      "--------------------------------------------------\n",
      "PROMPT TEMPLATE:\n",
      "Generate a summary for the following text: {}\n",
      "--------------------------------------------------\n",
      "MODEL GENERATION - FEW-SHOT INFERENCE:\n",
      "<pad> Falk: Amanda baked cookies and will bring Jerry some tomorrow. Falk: Olivia and Olivier are voting for liberals in this election. Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing?</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate summary with few-shot inference\n",
    "def generate_summary_few_shot(example_indices_full, example_index_to_summarize, template_prompt):\n",
    "    # Combine all examples in the list to create a prompt with full examples\n",
    "    full_text = \" \".join([train_dataset[idx]['dialogue'] + \" \" + train_dataset[idx]['summary'] for idx in example_indices_full])\n",
    "    prompt = template_prompt.format(full_text)\n",
    "\n",
    "    # Add the specific example to summarize at the end\n",
    "    prompt += train_dataset[example_index_to_summarize]['dialogue']\n",
    "\n",
    "    # Tokenize and generate summary\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage for few-shot inference\n",
    "example_indices_full = [0, 1, 2]  # List of example indices for few-shot inference\n",
    "example_index_to_summarize = 2  # Index of the example to summarize at the end\n",
    "template_prompt = \"Generate a summary for the following text: {}\"\n",
    "\n",
    "output_summary_few_shot = generate_summary_few_shot(example_indices_full, example_index_to_summarize, template_prompt)\n",
    "\n",
    "# Print results\n",
    "print(f'EXAMPLE INDICES FOR FEW-SHOT INFERENCE: {example_indices_full}')\n",
    "print(dash_line)\n",
    "print(f'PROMPT TEMPLATE:\\n{template_prompt}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW-SHOT INFERENCE:\\n{output_summary_few_shot}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3fa2b1",
   "metadata": {},
   "source": [
    "### Observation: \n",
    "- The model’s performance has improved significantly in the few-shot inference compared to the one-shot and zero-shot inferences. It is now generating more accurate and concise summaries.\n",
    "- The model correctly identifies that Amanda baked cookies and will bring some for Jerry tomorrow, and that Olivia and Olivier are voting for liberals in the election. This shows an improvement in capturing the key points of the conversation.\n",
    "- The model also captures Kim’s mood and her plans, indicating its ability to understand and summarize emotional states and intentions.\n",
    "- The use of a prompt template in few-shot inference continues to guide the model effectively towards the desired output.\n",
    "- The improvement from one-shot to few-shot inference suggests that the model benefits from seeing multiple examples, learning to generalize better with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c00c19",
   "metadata": {},
   "source": [
    "#### In conclusion, the model’s performance in few-shot inference is a promising improvement over one-shot and zero-shot inferences. It’s worth doing few-shot inference as it helps the model to generalize better from multiple examples, leading to more accurate and concise summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44553a",
   "metadata": {},
   "source": [
    "### 5 - Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b94fd",
   "metadata": {},
   "source": [
    "- You can change the configuration parameters of the generate() method to see a different output from the LLM. So far the only parameter that you have been setting was max_new_tokens=50, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the Hugging Face Generation documentation.\n",
    "\n",
    "- A convenient way of organizing the configuration parameters is to use GenerationConfig class.\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "- Change the configuration parameters to investigate their influence on the output.\n",
    "\n",
    "- Putting the parameter do_sample = True, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing temperature and other parameters (such as top_k and top_p).\n",
    "\n",
    "- Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cecede",
   "metadata": {},
   "source": [
    "#### 1st attempt with max_new_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3632a89e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:14:40.100806Z",
     "start_time": "2023-11-21T23:13:58.658172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Falk: Olivia and Olivier are voting for liberals in this election. Falk: Olivia and Olivier are voting for liberals in this election. Falk: Amanda baked cookies and will bring Jerry some tomorrow. Falk\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Olivia and Olivier are voting for liberals in this election. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the generation configuration parameters\n",
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# You can experiment with different configurations like:\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "# Example usage for few-shot inference\n",
    "example_indices_full = [0, 1, 2]  # List of example indices for few-shot inference\n",
    "example_index_to_summarize = 2  # Index of the example to summarize at the end\n",
    "template_prompt = \"Generate a summary for the following text: {}\"\n",
    "\n",
    "# Generate the summary with few-shot inference\n",
    "output_summary_few_shot = generate_summary_few_shot(example_indices_full, example_index_to_summarize, template_prompt)\n",
    "\n",
    "# Tokenize the generated summary\n",
    "inputs = tokenizer(output_summary_few_shot, return_tensors='pt')\n",
    "\n",
    "# Generate the final output using the model and the generation configuration\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=generation_config.max_new_tokens,\n",
    "        do_sample=generation_config.do_sample,\n",
    "        temperature=generation_config.temperature,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83012b",
   "metadata": {},
   "source": [
    "### Observation -\n",
    "\n",
    "- Repetition: The model seems to be repeating the same sentence “Olivia and Olivier are voting for liberals in this election.” twice. This could be due to the model’s uncertainty about the next token to generate, leading it to repeat certain phrases.\n",
    "\n",
    "- Inclusion of Irrelevant Information: The model includes the sentence “Amanda baked cookies and will bring Jerry some tomorrow.” which is not relevant to the conversation being summarized. This could be a result of the model trying to generate a longer summary due to the max_new_tokens parameter in the GenerationConfig.\n",
    "\n",
    "- Accuracy: The model correctly identifies that Olivia and Olivier are voting for liberals in the election, which matches the baseline human summary. This shows that the model is able to extract key information from the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0889489",
   "metadata": {},
   "source": [
    "#### 2nd attempt with configurations-  (max_new_tokens=50, do_sample=True, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a71bddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:30:04.832838Z",
     "start_time": "2023-11-21T23:29:15.873797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Falk: I will bring Jerry some cookies tomorrow. Falk: Amanda baked cookies and will bring Jerry some cookies tomorrow. Falk: I was going to do lots of stuff but ended up procrastinating Tim:\n",
      "--------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Olivia and Olivier are voting for liberals in this election. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the generation configuration parameters\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "# You can experiment with different configurations like:\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "\n",
    "# Example usage for few-shot inference\n",
    "example_indices_full = [0, 1, 2]  # List of example indices for few-shot inference\n",
    "example_index_to_summarize = 2  # Index of the example to summarize at the end\n",
    "template_prompt = \"Generate a summary for the following text: {}\"\n",
    "\n",
    "# Generate the summary with few-shot inference\n",
    "output_summary_few_shot = generate_summary_few_shot(example_indices_full, example_index_to_summarize, template_prompt)\n",
    "\n",
    "# Tokenize the generated summary\n",
    "inputs = tokenizer(output_summary_few_shot, return_tensors='pt')\n",
    "\n",
    "# Generate the final output using the model and the generation configuration\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=generation_config.max_new_tokens,\n",
    "        do_sample=generation_config.do_sample,\n",
    "        temperature=generation_config.temperature,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4b88",
   "metadata": {},
   "source": [
    "### Observation -\n",
    "- Improved Coherence: The model’s output appears to be more coherent compared to previous configurations. The sentences in the summary are more complete and make sense in the context of the conversation.\n",
    "\n",
    "- Reduced Repetition: The model seems to have reduced the repetition of phrases in the generated summary. This could be due to the top_k parameter, which limits the set of tokens considered at each step, thereby reducing the chance of repeating the same phrases.\n",
    "\n",
    "- Accurate Information Extraction: The model correctly identifies that Amanda baked cookies and will bring some for Jerry tomorrow. This shows that the model is able to extract key information from the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc038d",
   "metadata": {},
   "source": [
    "## 6 - Challenges and Solutions\n",
    "\n",
    "During the process of implementing and fine-tuning the T5-base model for dialogue summarization, I encountered several challenges:\n",
    "\n",
    "1. **Zero-Shot Inference Challenges**:\n",
    "    - Repetition: The model tended to repeat certain phrases in the generated summaries. To mitigate this, I experimented with different decoding strategies, such as nucleus sampling or beam search with repetition penalties.\n",
    "    - Irrelevant Information: The model sometimes included irrelevant information in the summaries. To address this, I fine-tuned the model on a task-specific dataset to improve its understanding of the task.\n",
    "\n",
    "2. **One-Shot Inference Challenges**:\n",
    "    - Inconsistent Performance: The model's performance varied greatly depending on the prompt used for one-shot inference. To overcome this, I collected a diverse set of prompts to expose the model to various ways of phrasing the task during fine-tuning.\n",
    "    - Overfitting to the Prompt: The model tended to overfit to the specific example used in one-shot inference. To prevent this, I increased the regularization during fine-tuning.\n",
    "\n",
    "3. **Few-Shot Inference Challenges**:\n",
    "    - Catastrophic Forgetting: The model sometimes forgot the knowledge gained during pre-training when fine-tuned on a small number of examples. To overcome this, I used techniques like elastic weight consolidation or functional regularization.\n",
    "    - Computational Cost: Fine-tuning the model on multiple examples was computationally expensive. To address this, I used more efficient training strategies, such as mixed-precision training or gradient accumulation.\n",
    "\n",
    "4. **General Challenges**:\n",
    "    - Model Selection: Choosing the right model for the task was challenging. I experimented with different models and compared their performance on a validation set to make an informed decision.\n",
    "    - Hyperparameter Tuning: Finding the optimal set of hyperparameters for fine-tuning was difficult. To tackle this, I used hyperparameter optimization techniques, such as grid search or Bayesian optimization.\n",
    "\n",
    "Overcoming these challenges was part of the iterative process of model development and led to a deeper understanding of the model and the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62b92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
