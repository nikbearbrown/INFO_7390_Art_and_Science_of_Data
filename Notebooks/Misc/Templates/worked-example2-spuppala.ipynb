{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e8e717-89e2-41ce-a4ca-470367e195a6",
   "metadata": {},
   "source": [
    "### Problem:\n",
    "* How do we deal with imbalanced class distribution for textual data classification?\n",
    "\n",
    "An imbalanced dataset in Natural Language Processing is a dataset whose number of data samples is not the same in the different classes. One class has more data samples than the other class.\n",
    "\n",
    "For example, one class has 3000 samples, and the other may have 300. The class with more data samples is known as the majority class, while the other one is known as the minority class.\n",
    "\n",
    "When we train a model with an imbalanced dataset, the model will be biased towards the majority class. The model may make wrong predictions and give inaccurate results. It has a negative impact when we use the model in production, and the stakeholders depend on it for business operations.\n",
    "\n",
    "In Natural Language Processing (NLP), we have various libraries that can handle text data that have an imbalance. We will use the Imbalanced-learn library. This library will balance the classes in the dataset. It will also reduce model bias and enhance the NLP performance.\n",
    "\n",
    "We will first build a spam classifier model with natural language processing without balancing the classes in the dataset. We will implement the same model but use Imbalanced-Learn to balance the classes. Then, we will compare the two models (before and after balancing) and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bdd77-89b2-4a8c-a616-28fe62b31718",
   "metadata": {},
   "source": [
    "#### Data\n",
    "* Spam classification dataset:\n",
    "    We will use the SMS collection dataset to train the NLP model. It has two labeled classes. (spam and ham). The spam class contains all the spam SMS. The ham class has all the SMS that are not spam.\n",
    "    \n",
    "    The NLP model will classify an SMS as either spam or not spam. Ensure you can download the SMS collection dataset from here. The link will give you the complete SMS collection dataset.\n",
    "    \n",
    "    We load the dataset using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c647e34-89db-4a46-8f57-c99798b81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bcdecdb-a25e-4759-845a-5d1a9d064ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4d9cd8f-9684-4ee7-87f6-3124b6d59202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam_classification.csv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b33b020c-f666-4fba-826e-14373e2fbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.rename(columns={0: 'label', 1: 'text'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a4721f-02d6-4e1a-8d65-d65ea06065bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74abab-b756-4935-854e-ada5bb4cb4f5",
   "metadata": {},
   "source": [
    "Calculating the length of each data sample\n",
    "\n",
    "We will create a new length column that will show the length of each data sample. This new column will help us with preprocessing the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fac6b331-8dfd-4981-aabc-cdc67835ed47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['text'].apply(lambda x: len(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84db9d7-e594-4afa-bbd4-4fdce8205b87",
   "metadata": {},
   "source": [
    "We will begin cleaning the text.\n",
    "\n",
    "Text cleaning\n",
    "Before building the spam classification model, we will clean the dataset to have the required format. Many text cleaning steps will format the text. It includes removing unnecessary words, punctuation, stop words, white spaces, and unnecessary symbols from the text dataset.\n",
    "\n",
    "For this tutorial, we will implement the following steps:\n",
    "\n",
    "Removing stop words: Stop words do not contribute to the meaning of a sentence since they are common in a language. Stop words for the English language are pronouns, conjunctions, and articles. Removing stop words enables the NLP model to focus on unique words in the SMS messages that will add value.\n",
    "\n",
    "Converting all the SMS messages to lower case: It ensures that we have a uniform dataset.\n",
    "\n",
    "Removing numbers and other numeric values: It ensures that only text that remains in the dataset adds value to the model.\n",
    "\n",
    "Removing punctuations: It involves removing full stops and other punctuation marks. These are the unnecessary symbols in the dataset.\n",
    "\n",
    "Removing extra white spaces: White space occupies the dataset, but they do not carry information. Removing the extra white spaces ensures we only remain with the text that the model will use.\n",
    "\n",
    "Lemmatizing the texts: Stemming reduces inflected forms of a text/word into its lemma or dictionary form. For example, the words/texts “running”, “ran”, and “runs” are all reduced to the root form “run”.\n",
    "\n",
    "Tokenization: It is the splitting/breaking of the raw texts into smaller words or phrases known as tokens. We will implement the text cleaning steps using Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72683431-26bc-4556-ad5d-6b39b3c8c3e5",
   "metadata": {},
   "source": [
    "NLTK has smaller sub-libraries that perform specific text cleaning tasks. These smaller libraries also have methods for text cleaning.\n",
    "\n",
    "The next step is to download the smaller sub-libraries from NLTK as follows:\n",
    "\n",
    "It will tokenize the text in the dataset.                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf06fe4-027c-4bd6-bcf7-88c906497798",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad16281a-6913-4d47-adf9-342aa8a70c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sumana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "645e1f3e-12b2-42b5-a333-3b5eae408274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "df['text'] = df['text'].apply(lambda x: convert_to_lower(x))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "df['text'] = df['text'].apply(lambda x: remove_numbers(x))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "df['text'] = df['text'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "\n",
    "# def lemmatizing(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = word_tokenize(text)\n",
    "#     for i in range(len(tokens)):\n",
    "#         lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "#         tokens[i] = lemma_word\n",
    "#     return \" \".join(tokens)\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a52ee962-8d8d-47b1-a5b8-191e607d9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the class labels into integer values\n",
    "label_map = {\n",
    "    'ham': 0,\n",
    "    'spam': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c88e6f3-7017-4305-b396-70dae64898c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f17ac89b-2c3f-49a9-b368-d6a43e32769c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif oni</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in wkly comp to win fa cup final tk...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor c already then say</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah dont think he goes to usf he lives around ...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  length\n",
       "0      0  go until jurong point crazy available only in ...     111\n",
       "1      0                              ok lar joking wif oni      29\n",
       "2      1  free entry in wkly comp to win fa cup final tk...     155\n",
       "3      0          u dun say so early hor c already then say      49\n",
       "4      0  nah dont think he goes to usf he lives around ...      61"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eee51c-9466-41fa-acc8-866fcaf382ff",
   "metadata": {},
   "source": [
    "Implementing text vectorization\n",
    "It converts the raw text into a format the NLP model can understand and use. Vectorization will create a numerical representation of the text strings called a sparse matrix or word vectors. The model works with numbers and not raw text. We will use TfidfVectorizer to create the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8fc032a0-a6a6-4f8f-a613-c20daeda0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_wb= TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7df2a27-f717-4263-bbd1-024ef961c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = tf_wb.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ea4bf5e-e34f-4664-b073-82c8c5bba200",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = X_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5ba1cfd-e858-42a2-9554-2118fcf40194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "974056d9-52dd-4baf-9ed9-bdb159bf9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c948ed8-28c3-4f25-a9f9-6963228b42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, df['label'].values, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10f0c9af-d85d-4a0c-a2e3-6e0da8b747c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 0]\n",
      "0.8929425837320574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train_tf, y_train_tf)\n",
    "\n",
    "NB_pred= NB.predict(X_test_tf)\n",
    "print(NB_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test_tf, NB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4dd449e-b770-4284-8a13-bd7edd3bc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99462dfc-3cb9-4287-9d40-0c086ea86f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626e883-9e28-48e2-8f8f-6855de09193c",
   "metadata": {},
   "source": [
    "#### Implementing Imbalanced-Learn\n",
    "RandomOverSampler will increase the data samples in the minority class (spam). It makes the minority class have the same data samples as the majority class (ham). The function synthesizes new dummy data samples in the minority class to enable class balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73cf2273-8363-4465-85d4-623876726cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'].values, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "af5c456c-c908-4c41-8313-fc3830f1bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3361, 1: 539})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "747461ef-0f79-4f74-b463-ee3a1e489a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "X_train_tf = X_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b24a4dab-e956-4479-a81c-6011ae5b47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = vectorizer.transform(X_test)\n",
    "X_test_tf = X_test_tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa88a91-4ec3-48c3-81d3-aaa24b6fc2b2",
   "metadata": {},
   "source": [
    "### Applying RandomOverSampler function\n",
    "#### The function uses the sampling_strategy parameter to balance the class. We set the parameter’s value to 1 to ensure the dataset classes have 1:1 data samples. We then apply the function to the training set. It will generate the new data samples to ensure both classes are balanced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "418d70da-40d0-4071-b89d-dc1c2e6ce519",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROS = RandomOverSampler(sampling_strategy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc0d783e-18e2-4fbe-9b63-f4a884980f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ros, y_train_ros = ROS.fit_resample(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c3abbb56-65e2-4e4c-a9cf-8190839e21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3361, 1: 3361})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07519140-1b8d-4216-bca3-a21b511cd7b3",
   "metadata": {},
   "source": [
    "### Using the balanced dataset to build the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e52d940-f278-4dec-8549-168883e851a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "866fbf98-c4b1-4e3c-8055-66b9fec61ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 0]\n",
      "0.8995215311004785\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "y_preds = nb.predict(X_test_tf)\n",
    "print(y_preds)\n",
    "\n",
    "print(accuracy_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836d2ff-dfc6-4579-baed-864a63755f05",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "We used Imbalanced-learn to handle imbalanced text data in natural language processing. We cleaned the text dataset and implemented the text preprocessing steps using the NLTK library. We implemented text vectorization and fed the model the sparse matrix.\n",
    "\n",
    "We then implemented a spam classifier model without balancing the dataset and calculated the accuracy score. We also implemented the same model but used Imbalanced-Learn to balance the classes.\n",
    "\n",
    "Finally, we compared the two models (before and after balancing), with a slghtly increased accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
