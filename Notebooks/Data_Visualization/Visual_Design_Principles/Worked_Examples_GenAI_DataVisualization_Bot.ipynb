{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generative AI Worked Example: Data Visualization Bot**"
      ],
      "metadata": {
        "id": "gJCGojtlnEim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Utkarsha Suresh Shirke     \n",
        "NUID: 002797914"
      ],
      "metadata": {
        "id": "LaesS1RXnePA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**"
      ],
      "metadata": {
        "id": "vMCiZLoynlsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This project aims to create Data Visualization Bot that represents a cutting-edge Generative AI application designed to transform the way users interact with and understand their data. At its core, this innovative tool accepts CSV files, employing sophisticated algorithms to not only read the data but also to interpret and summarize its contents comprehensively. Users can engage with the bot to ask a variety of questions, ranging from requests for data summaries to inquiries about the specific columns and their data types, enhancing their grasp of the dataset's structure and contents.\n",
        "\n",
        "A standout feature of Data Visualization Bot is its adeptness at identifying and addressing missing values within the dataset. Utilizing advanced imputation techniques such as MICE (Multiple Imputation by Chained Equations) and Mode Imputation, the bot ensures the integrity and completeness of the data, thereby enabling more accurate analyses and decisions. This capability is crucial for handling real-world data that often contains gaps and inconsistencies.\n",
        "\n",
        "Beyond data cleaning and preparation, Data Visualization Bot excels in its ability to generate meaningful and insightful visualizations. Users can specify the type of visualization they need, or alternatively, allow the bot to autonomously generate advanced visualizations based on its understanding of the data's context and significance. This feature not only simplifies the data exploration process but also uncovers hidden patterns and trends that might not be immediately apparent.\n",
        "\n",
        "Overall, Data Visualization Bot stands as a testament to the potential of Generative AI in enhancing data analysis and visualization. By offering an intuitive, interactive, and intelligent platform for handling data, it promises to revolutionize the field, making sophisticated data analysis accessible to a wider range of users, regardless of their technical expertise."
      ],
      "metadata": {
        "id": "CkcjRj3usHQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application Link**\n",
        "\n",
        "The Data Visualization Bot is hosted on streamlit application. Please find the below link for the same:\n",
        "\n",
        "https://datavisualizationbot.streamlit.app/"
      ],
      "metadata": {
        "id": "sULdK_RSdqrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/63bc63fb-353f-4bfc-8d13-177b49e2799a)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZZO68Z5sjBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction to Generative AI**\n",
        "\n",
        "\n",
        "\n",
        "Generative AI represents a cutting-edge frontier in the broader field of artificial intelligence, standing out for its ability to create new, original content that mirrors the complexity and nuance of human-generated work. Unlike traditional AI systems that primarily focus on understanding or interpreting data, generative AI ventures into the realm of creation, generating text, images, music, and code that were previously believed to be the exclusive domain of human creativity. This transformative capability is built on the foundation of machine learning, where algorithms learn from vast datasets to recognize patterns, styles, and structures, enabling them to produce outputs that, although novel, resonate with the characteristics of the data they were trained on.\n",
        "\n",
        "### **Core Principles Behind Generative AI**\n",
        "\n",
        "At its heart, generative AI leverages deep learning and neural network architectures to digest and process the enormity of data it is exposed to. These models are adept at understanding the underlying distribution of the data they learn from, enabling them to generate outputs that can be remarkably similar to the original examples in quality and essence.\n",
        "\n",
        "#### **Learning from Data**\n",
        "\n",
        "Generative AI models undergo extensive training, where they are fed large amounts of data. This data can range from images and texts to sounds and code snippets. Through this training process, the models learn to identify and internalize the complex patterns and structures inherent in the data. This learning enables them to produce new content that adheres to the same patterns and structures, effectively mimicking the original data's style and essence.\n",
        "\n",
        "#### **Creativity through Algorithms**\n",
        "\n",
        "What sets generative AI apart is its algorithmic creativity. By encoding the essence of the data into a mathematical model, generative AI can explore the vast space of potential outputs that fit within the learned patterns. This exploration is not random but is guided by the intricate structures the model has learned, allowing it to create content that is not just new but often surprisingly innovative and coherent.\n",
        "\n",
        "### **The Evolution of Generative AI**\n",
        "\n",
        "Generative AI has rapidly evolved, thanks in part to advancements in computational power and algorithmic efficiency. Early forms of generative AI were relatively limited in scope and capability, often producing outputs that, while novel, were easily distinguishable from human-generated content. However, as the technology advanced, the outputs became increasingly sophisticated, blurring the lines between human and machine-generated content. This evolution has opened up new possibilities and applications across various fields, from content creation and design to scientific discovery and personalized digital experiences.\n",
        "\n",
        "By extending the capabilities of AI beyond analysis and interpretation to include the creation of new, original works, generative AI represents a significant leap forward in the quest to emulate human intelligence and creativity. This not only challenges our understanding of what machines are capable of but also offers a glimpse into the future of human-machine collaboration, where AI's generative capabilities can augment human creativity, leading to innovations that were previously unimaginable."
      ],
      "metadata": {
        "id": "EbBlqGOqA8sK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/f3176b2f-8adf-4e68-8f34-5e18b9915c27)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tFRU2rMXCHn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Generative AI Key Technologies and Models**\n",
        "\n",
        "**Generative Adversarial Networks (GANs)**    \n",
        "GANs represent a breakthrough in generating photorealistic images, where two neural networks—the generator and the discriminator—engage in a game-theoretic competition. The generator aims to produce data indistinguishable from real data, while the discriminator strives to differentiate between real and generated data. This dynamic tension drives both networks to improve continuously, leading to the generation of highly realistic images. Applications of GANs extend beyond image creation to video generation, style transfer, and more, showcasing their versatility.\n",
        "\n",
        "**Variational Autoencoders (VAEs)**     \n",
        "VAEs are pivotal in the generative AI landscape for their ability to learn the distribution of data, enabling the generation of new data points that are variations of the input data. This is achieved by encoding input data into a lower-dimensional latent space and then decoding from this space to reconstruct the input. VAEs are particularly effective in tasks that require a degree of variation and creativity, such as image generation and modification, while maintaining a strong connection to the original data distribution.\n",
        "\n",
        "**Transformer-based Models**    \n",
        "Transformers have revolutionized the field of natural language processing and beyond, with their ability to handle sequential data and capture long-range dependencies within the data. Generative models like GPT (Generative Pre-trained Transformer) and DALL-E leverage transformers to generate coherent and contextually relevant text and images, respectively. These models are trained on vast datasets, enabling them to generate content that is not only novel but also rich in context and detail, spanning a wide range of styles and themes.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "44pme7qVC_Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Impacts of Generative AI**"
      ],
      "metadata": {
        "id": "4PcWgU0EEpJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/6a68a234-aaff-4e28-bae6-2a0cdad83090)\n",
        "\n",
        "\n",
        "The impact of generative AI extends across various sectors, catalyzing innovation and transforming traditional processes, yet it also introduces significant challenges and limitations that necessitate careful consideration and management. Understanding these impacts and limitations is crucial for harnessing the potential of generative AI responsibly and effectively.\n",
        "\n",
        " **Accelerating Creativity and Innovation**\n",
        "Generative AI has the unique ability to produce a wide array of content, from art and music to text and designs, thus accelerating creative processes across industries. This not only enhances productivity but also inspires new forms of creativity, as AI-generated ideas can serve as a springboard for human creativity, leading to novel concepts and innovations that were previously unattainable.\n",
        "\n",
        "**Enhancing Efficiency and Reducing Costs**\n",
        "By automating the generation of content and data, generative AI can significantly reduce the time and resources required to produce diverse materials. This efficiency can lower costs in content production, research and development, and design, making these processes more accessible to a wider range of entities and individuals.\n",
        "\n",
        "**Personalizing User Experiences**\n",
        "Generative AI enables the creation of personalized content at scale, tailoring experiences to individual preferences and behaviors. This personalization can enhance user engagement and satisfaction in digital platforms, e-commerce, education, and entertainment, offering more relevant and meaningful interactions.\n",
        "\n",
        "**Advancing Scientific Research and Discovery**\n",
        "In scientific fields, generative AI can simulate data and model complex systems, facilitating breakthroughs in drug discovery, material science, and environmental science. By generating novel data and scenarios, it enables researchers to explore a broader range of hypotheses and accelerate the pace of discovery.\n"
      ],
      "metadata": {
        "id": "l3VhWgL1Emlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Limitations and Challenges**\n",
        "\n",
        "**Quality and Coherence**\n",
        "While generative AI can produce content that mimics human output, ensuring consistent quality and coherence, especially in complex or nuanced tasks, remains challenging. The output can sometimes lack the depth, subtlety, or context that a human creator might provide, limiting its applicability in certain scenarios.\n",
        "\n",
        "**Ethical and Societal Concerns**\n",
        "The capability of generative AI to create realistic and persuasive content raises ethical concerns, including the potential for misuse in creating misleading information, deepfakes, or violating privacy. Addressing these concerns requires robust ethical frameworks, transparency in AI development, and mechanisms to ensure accountability.\n",
        "\n",
        "**Bias and Fairness**\n",
        "AI systems, including generative models, can perpetuate and amplify biases present in their training data, leading to outputs that reinforce stereotypes or discriminate against certain groups. Mitigating bias in AI-generated content involves critical examination of data sources, model training processes, and continuous monitoring for biased outcomes.\n",
        "\n",
        "**Intellectual Property and Copyright Issues**\n",
        "The rise of AI-generated content poses complex questions about copyright, ownership, and the role of creativity. Determining the authorship and rights to AI-generated works involves navigating uncharted legal and ethical territories, challenging existing intellectual property laws and conventions.\n",
        "\n",
        "**Dependency and Devaluation of Human Skills**\n",
        "There's a concern that as generative AI becomes more pervasive, it could lead to a dependency on automated systems for creative and intellectual tasks, potentially devaluing human skills and creativity. Balancing the use of AI with the cultivation and appreciation of human talents is essential for a harmonious coexistence."
      ],
      "metadata": {
        "id": "Qm5s-eVDFR2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applications**\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/4c023a3c-1710-414e-b599-82c088e1f4b1)\n",
        "\n",
        "\n",
        "**Content Creation**\n",
        "Generative AI is transforming the creative industries by enabling the automated generation of text, images, music, and synthetic voices. This technology allows for the creation of content at scale, opening up new possibilities for personalized and dynamic content across media, entertainment, and marketing.\n",
        "\n",
        "**Data Augmentation**\n",
        "In domains where data is scarce, expensive, or sensitive, generative AI can create synthetic datasets that augment the original data, facilitating the training of machine learning models without compromising on data diversity or volume. This is particularly valuable in healthcare, finance, and other sectors where data privacy and availability are major concerns.\n",
        "\n",
        "**Personalization**\n",
        "Generative AI can tailor content to individual preferences and behaviors, enhancing user engagement and satisfaction across digital platforms. This personalization extends to advertising, content recommendation, and even personalized learning, where content adapts to the user's learning pace and style.\n",
        "\n",
        "**Simulation and Modeling**\n",
        "Generative AI plays a critical role in simulating outcomes and modeling scenarios in fields such as drug discovery, climate modeling, and urban planning. By generating data that mimics real-world phenomena, researchers can explore a vast array of scenarios and outcomes, accelerating innovation and decision-making processes."
      ],
      "metadata": {
        "id": "ROJa6Rt_DXDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Crafting Generative Data**"
      ],
      "metadata": {
        "id": "rYVKtMkqIopn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/71315e86-b85f-442b-a7a0-78edd90c5c9a)\n"
      ],
      "metadata": {
        "id": "FyahdNF6Jazf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task Generation**\n",
        "\n",
        "Our objective is to develop an advanced Data Visualization Bot that transforms CSV files into insightful visual representations and analyses. This bot not only interprets and summarizes datasets but also addresses data quality issues and generates dynamic visualizations to enhance user understanding and decision-making.\n",
        "\n",
        "1. **CSV Data Interpretation and Summary**: Our application reads CSV files, summarizing their contents and structure, including column names, data types, and basic statistical insights, using sophisticated data processing algorithms.\n",
        "\n",
        "2. **Data Cleaning and Imputation**: The bot identifies missing values and employs techniques like MICE and Mode Imputation to ensure data completeness, thereby improving the reliability of analyses.\n",
        "\n",
        "3. **Dynamic Data Visualization**: Users can request specific types of visualizations or let the bot autonomously generate visualizations based on its analysis of the data's context and significance, using advanced AI algorithms.\n",
        "\n",
        "# **Format of the Generated Data**\n",
        "\n",
        "**CSV Data Interpretation and Summary**:\n",
        "\n",
        "- Input: CSV files containing various datasets.\n",
        "- Output: Summaries detailing column names, data types, and basic statistics in text format.\n",
        "\n",
        "**Data Cleaning and Imputation**:\n",
        "\n",
        "- Input: CSV files with missing or incomplete data.\n",
        "- Output: Modified CSV files with imputed values, ensuring data completeness.\n",
        "\n",
        "**Dynamic Data Visualization**:\n",
        "\n",
        "- Input: User requests for data visualizations or CSV files for autonomous visualization generation.\n",
        "- Output: Visualizations created for highlighting key patterns and insights.\n",
        "\n",
        "# **Constraints**\n",
        "\n",
        "**Data Quality**: The CSV files should be well-structured and without corrupted data to ensure accurate interpretation and visualization.\n",
        "\n",
        "**Complexity of Data**: The datasets should not exceed the bot's processing capabilities in terms of size and complexity to maintain performance and accuracy.\n",
        "\n",
        "**Visualization Accuracy**: The generated visualizations must accurately represent the underlying data, with correct interpretations of data patterns and trends.\n",
        "\n",
        "**User Interaction**: The bot's responses and generated visualizations must be relevant and intuitive to user queries, ensuring a seamless user experience.\n",
        "\n",
        "**Format Compatibility**: The output files, especially visualizations, must be compatible with standard viewing and editing tools to ensure user accessibility.\n",
        "\n",
        "The illustrative example screenshots will be provided below while demonstrating the application.\n"
      ],
      "metadata": {
        "id": "UJDABcEBrCcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demonstrating Data Generation**"
      ],
      "metadata": {
        "id": "I2xFmyjar4gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed for a Streamlit application that leverages OpenAI's GPT-4 Turbo to provide insightful visual representations and analyses.Additionally, it enables users to query the chatbot about specifics from the csv file.\n",
        "\n",
        "Note: It's important to note that Streamlit applications cannot be executed within Google Colab environments. For optimal performance, it's advised to run this application locally on your own computer."
      ],
      "metadata": {
        "id": "wJ6kJKGKsFkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas matplotlib seaborn openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QujH8yPRskor",
        "outputId": "72c47864-25c5-46a5-8c39-a16430212dd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Collecting openai\n",
            "  Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.10.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m718.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, smmap, h11, pydeck, httpcore, gitdb, httpx, gitpython, openai, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.2 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.33.0 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "import openai\n",
        "\n",
        "\n",
        "# Function to apply MICE and mode imputation\n",
        "def impute_data(df):\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "    categorical_cols = df.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    # Apply MICE imputation for numeric columns\n",
        "    mice_imputer = IterativeImputer()\n",
        "    df[numeric_cols] = mice_imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Apply mode imputation for categorical columns\n",
        "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df[categorical_cols] = mode_imputer.fit_transform(df[categorical_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Chatbot function\n",
        "def ask_chatbot(question, context=\"\"):\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"davinci\",\n",
        "      prompt=f\"{context}\\n\\n{question}\\n\",\n",
        "      temperature=0.7,\n",
        "      max_tokens=150,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Streamlit UI\n",
        "def main():\n",
        "    st.title(\"Data Analysis and Chatbot Application\")\n",
        "\n",
        "    openai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #Enter your OPENAI API Key here\n",
        "\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload your CSV file\", type=[\"csv\"])\n",
        "    process_button = st.sidebar.button(\"Process\")\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded CSV file\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "\n",
        "\n",
        "        # Chatbot Interaction\n",
        "        question = st.text_input(\"Ask a question about your data:\")\n",
        "        if question:\n",
        "            summary = \"Your dataset contains the following columns: \" + \", \".join(df.columns) + \".\"\n",
        "            answer = ask_chatbot(question, context=summary)\n",
        "            st.text_area(\"Answer\", value=answer, height=100, max_chars=None)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ajooRpKxsrz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec076bb0-e5e1-4c82-bc34-3623b1d3cab2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-06 21:00:51.585 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code Implementation**\n",
        "\n",
        "This Python code is designed to create a web application using Streamlit, a popular library for building interactive web apps for data science and machine learning projects. The application focuses on data analysis and features a chatbot for answering questions related to the uploaded dataset. Here's a breakdown of the code and its functionalities:\n",
        "\n",
        "**Imports**:\n",
        "- **Streamlit (`st`)**: Used for creating the web app interface.\n",
        "- **Pandas (`pd`)**: For data manipulation and analysis.\n",
        "- **Seaborn (`sns`)** and **Matplotlib (`plt`)**: For data visualization.\n",
        "- **Sklearn's `IterativeImputer`** and **`SimpleImputer`**: For imputing missing values in the dataset.\n",
        "- **OpenAI's `openai`**: For accessing the GPT model for the chatbot functionality.\n",
        "\n",
        "**Functions:**\n",
        "\n",
        "**impute_data(df):**  \n",
        "This function imputes missing data in a dataframe (`df`). It first separates the columns into numeric and categorical. For numeric columns, it uses MICE (Multiple Imputation by Chained Equations) via the `IterativeImputer`. For categorical columns, it fills missing values with the mode (most frequent value) using `SimpleImputer`.\n",
        "\n",
        "\n",
        "**ask_chatbot(question, context=\"\"):**   \n",
        "This function uses OpenAI's API to generate answers to questions about the data. It sends a prompt composed of a context (summary of the dataset) and the question to the OpenAI model and returns the model's response.\n",
        "\n",
        "**Streamlit UI (`main()` function):**\n",
        "This part of the code creates the user interface for the web application.\n",
        "\n",
        "1. **Title**: Displays the application title.\n",
        "2. **API Key**: The OpenAI API key is set (though it's just a placeholder here for security reasons).\n",
        "3. **File Upload**: Users can upload a CSV file for analysis.\n",
        "4. **Chatbot Interaction**: Users can ask questions about their dataset. The application uses the `ask_chatbot` function, providing a summary of the dataset as context, to generate and display answers from the OpenAI model.\n",
        "\n",
        "The if __name__ == \"__main__\": block ensures that the **main()** function runs only when the script is executed directly, not when imported as a module in another script.\n",
        "\n",
        "This code exemplifies a practical application of combining data science with AI for interactive data analysis, featuring data imputation, visualization, and conversational AI capabilities."
      ],
      "metadata": {
        "id": "2hWbQtzb3GZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application Snippets**"
      ],
      "metadata": {
        "id": "pOo8NcjF337e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatBot Functionality**\n",
        "\n",
        "The image displays the interface of the Data Visualization Assistant, a tool designed to facilitate data analysis through visual representations. On the user-friendly left-hand panel, one can simply upload a dataset by dragging and dropping a CSV file into the designated area, which has a maximum file size limit of 200MB. Once the dataset, as indicated by the file name \"Death_rates_for_suicide_by_sex.csv\" in this case, is uploaded, the user can initiate the processing of the data by clicking the \"Process\" button. This action prompts the assistant to prepare the data for analysis and visualization. The main section of the screen invites users to interact with the assistant by entering questions related to the dataset or specific types of graphs they wish to generate. The dialogue box provides an intuitive interface where users can communicate their requests or choose to exit the session.\n",
        "\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/f80cec54-9ab9-43df-ae7a-53cb56c91674)\n",
        "\n",
        "## **Summary Generation**\n",
        "Now, the user can request for a dataset summary from the Data Visualization Assistant. The Assistant provides an immediate snapshot: the dataset has 6,390 rows, 13 columns with 7 numerical and 6 categorical, and a concerning equal number of missing values, suggesting every row may have incomplete data. No duplicate rows are found, and the dataset uses over 3.2 MB of memory. An example column named 'INDICATOR' is listed as categorical.  The interface reveals that the dataset has significant missing data: 906 entries (14.18%) from the 'ESTIMATE' column and 5,484 entries (85.82%) from the 'FLAG' column are missing, indicating particular incompleteness in the 'FLAG' column that could impact data analysis.\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/31aed5e1-927e-47a3-96e4-f474d4d0545b)\n",
        "\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/c0909fb5-1205-4a3f-ac48-04433595770b)\n",
        "\n",
        "\n",
        "## **Imputing Missing Values**\n",
        "\n",
        "After summarizing the dataset, the bot focuses on a critical step in data cleaning, focusing on the imputation of missing values within the dataset. It identifies two problematic columns: 'ESTIMATE', a numerical column that is missing 906 values, and 'FLAG', a categorical column with a substantial 5,484 missing entries. The Assistant selects tailored imputation methods for each—MICE (Multiple Imputation by Chained Equations) for the numerical 'ESTIMATE', utilizing existing inter-variable relationships to predict the missing values, and the most common value, or mode, for the categorical 'FLAG' column. This approach ensures that the previously missing data is replaced with statistically inferred values for 'ESTIMATE' and the most likely category for 'FLAG', rendering the dataset fully populated and primed for in-depth analysis.\n",
        "\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/f69a14bf-27e2-41f8-b8a8-baa1d530299a)\n",
        "\n",
        "# **Data Visualization**\n",
        "\n",
        "The user later asks the bot to provide to create some visualization based upon the dataset. The below image depicts a Data Visualization Bot's interface after it has generated a heatmap to display the correlations between numerical columns in a dataset. The bot calculates the correlation coefficients, which measure the linear relationship between pairs of numerical columns, and visualizes these relationships in a color-coded matrix. Warm colors (like red) typically denote a strong positive correlation, while cool colors (like blue) indicate a negative correlation, and neutral colors reflect no significant correlation. The diagonal, always a perfect correlation of 1, confirms the matrix's validity. This visual tool allows users to quickly grasp complex relationships within their data, facilitating further analysis.\n",
        "\n",
        "![image](https://github.com/UtkarshaShirke/DataScience/assets/114371417/189154e4-2c0d-47d7-9724-550f0db75a59)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ub-taN4g37al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "\n",
        "In conclusion, the Data Visualization Bot represents a transformative leap in the field of data analysis and visualization, leveraging the power of Generative AI to democratize access to sophisticated data handling and insight generation. With its ability to ingest, clean, and interpret complex datasets through advanced algorithms and imputation techniques, this tool bridges the gap between raw data and actionable insights. Its intuitive interface and autonomous visualization capabilities make it an indispensable asset for both novices and experts alike, facilitating deeper understanding and more informed decision-making. As the boundaries of technology continue to expand, the Data Visualization Bot stands as a pioneering solution that promises to redefine the landscape of data analysis and visualization, making it more accessible, efficient, and insightful than ever before."
      ],
      "metadata": {
        "id": "gCBSjzS8GvjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "For understanding the concepts related to Generative AI, the following sites and links were used:\n",
        "\n",
        "Towards Data Science  \n",
        "Geeks for Geeks  \n",
        "OpenAI  \n",
        "WhisperAPI   \n",
        "Streamlit   \n",
        "Medium Article\n",
        "\n",
        "# **MIT License**\n",
        "\n",
        "Copyright (c) 2024 UtkarshaShirke\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software."
      ],
      "metadata": {
        "id": "_mH5dXisGWOE"
      }
    }
  ]
}