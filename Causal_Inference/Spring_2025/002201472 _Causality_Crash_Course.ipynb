{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "from IPython.display import Image\n",
    "mo.as_html(Image(url=\"https://imgs.xkcd.com/comics/correlation_2x.png\")).center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Understanding Causal Inference with IHDP: From Theory to Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In predictive modeling, we often focus on finding correlations between variables. However, for decision-making, we need to understand the *causal* relationship between actions and outcomes.\n",
    "\n",
    "The fundamental problem of causal inference is that we can never observe both potential outcomes for the same unit - we can't simultaneously observe what happens when a person receives a treatment and doesn't receive a treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.callout(\"\"\"\n",
    "In causal inference, a confounder is a variable that affects both the treatment (or independent variable) and the outcome (or dependent variable), potentially creating a spurious association if not controlled for. For example, when studying the effect of alcohol consumption on lung cancer risk, smokers tend to drink more and smoking is a direct cause of lung cancer, so smoking acts as a confounder that can distort the observed relationship between alcohol and cancer.\n",
    "\"\"\", kind=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data to illustrate correlation vs causation\n",
    "n = 1000\n",
    "\n",
    "# Common cause (confounder)\n",
    "confounder = np.random.normal(0, 1, n)\n",
    "\n",
    "# Treatment influenced by confounder\n",
    "treatment = 0.7 * confounder + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Outcome influenced by both treatment and confounder\n",
    "outcome = 0.3 * treatment + 0.7 * confounder + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Treatment': treatment,\n",
    "    'Outcome': outcome,\n",
    "    'Confounder': confounder\n",
    "})\n",
    "\n",
    "# Create model for the regression line\n",
    "model = LinearRegression()\n",
    "model.fit(data[['Treatment']], data['Outcome'])\n",
    "\n",
    "\n",
    "fig_1 = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Treatment vs Outcome (shows correlation)\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.scatter(data['Treatment'], data['Outcome'], alpha=0.5)\n",
    "\n",
    "# Add regression line\n",
    "x_range = np.linspace(data['Treatment'].min(), data['Treatment'].max(), 100)\n",
    "ax1.plot(x_range, model.predict(x_range.reshape(-1, 1)), 'r-', linewidth=2)\n",
    "ax1.set_title('Correlation: Treatment vs Outcome\\nCorrelation = {:.2f}'.format(\n",
    "    np.corrcoef(data['Treatment'], data['Outcome'])[0, 1]))\n",
    "ax1.set_xlabel('Treatment')\n",
    "ax1.set_ylabel('Outcome')\n",
    "\n",
    "# Plot 2: Treatment vs Outcome with confounder as color\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "scatter = ax2.scatter(data['Treatment'], data['Outcome'], c=data['Confounder'], cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(scatter, label='Confounder')\n",
    "ax2.set_title('Causal Structure: Treatment, Outcome, and Confounder')\n",
    "ax2.set_xlabel('Treatment')\n",
    "ax2.set_ylabel('Outcome')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Return interactive plot for marimo\n",
    "mo.mpl.interactive(fig_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
    "<div>\n",
    "<h3>Left Plot</h3>\n",
    "Shows the correlation between treatment and outcome (0.78), with a regression line indicating a strong positive relationship. This is what you might see in an observational study without accounting for confounders.\n",
    "</div>\n",
    "<div>\n",
    "<h3>Right Plot</h3>\n",
    "The same data points, but colored by the confounder value. This reveals the underlying structure - points with similar confounder values cluster together, showing that the apparent treatment-outcome relationship is largely driven by the confounder.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.callout(\"\"\"\n",
    "The simulation demonstrates that despite seeing a strong correlation (0.78), the actual causal effect of the treatment on the outcome is weaker (0.3 in the data generation). The rest of the association is due to the confounder creating a spurious correlation - a classic example of \"correlation does not imply causation.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 2. Theoretical Foundations of Causal Inference {#foundations}\"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating application domain columns\n",
    "healthcare = mo.md(\n",
    "\"\"\"\n",
    "### Healthcare\n",
    "- Evaluating treatment effectiveness\n",
    "- Understanding disease progression\n",
    "- Personalizing medical decisions\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "policy = mo.md(\n",
    "\"\"\"\n",
    "### Policy\n",
    "- Program evaluation\n",
    "- Social interventions assessment\n",
    "- Education policy design\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "business = mo.md(\n",
    "\"\"\"\n",
    "### Business\n",
    "- Marketing strategy optimization\n",
    "- Product feature evaluations\n",
    "- Customer retention strategies\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Real-world applications section with columns\n",
    "applications_title = mo.md(\"### 2.1 Real-World Applications {#applications}\\n\\nCausal inference is crucial in various domains:\")\n",
    "applications_columns = mo.hstack([healthcare, policy, business])\n",
    "\n",
    "# Key concepts section\n",
    "concepts = mo.md(\n",
    "\"\"\"\n",
    "### 2.2 Key Concepts in Causal Inference {#concepts}\n",
    "\n",
    "**The Potential Outcomes Framework**\n",
    "\n",
    "Developed by Rubin, this framework formalizes causal inference through potential outcomes. For each unit i:\n",
    "- Y_i(1): Outcome if unit i receives treatment\n",
    "- Y_i(0): Outcome if unit i doesn't receive treatment\n",
    "\n",
    "The individual treatment effect is defined as:\n",
    "\n",
    "\\\\[ \\\\tau_i = Y_i(1) - Y_i(0) \\\\]\n",
    "\n",
    "However, we can only observe one of these outcomes for each unit, which is known as the **fundamental problem of causal inference**.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Display all sections\n",
    "mo.vstack([applications_title, applications_columns, concepts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 2.3 Types of Treatment Effects in Causal Inference {#effects}\n",
    "\n",
    "### What are \"Treatments\" in Causal Inference?\n",
    "\n",
    "In causal inference, a **treatment** refers to the intervention or manipulation being studied to determine its causal effect on an outcome of interest. Despite the medical-sounding terminology, treatments extend far beyond healthcare settings:\n",
    "\n",
    "- Medical interventions (medications, surgical procedures)\n",
    "- Policy changes (minimum wage increases, educational reforms)\n",
    "- Business decisions (pricing strategies, marketing campaigns)\n",
    "- Social interventions (training programs, behavioral modifications)\n",
    "\n",
    "The **treatment variable** typically represents whether subjects received the intervention, usually coded as a binary variable (1=received treatment, 0=control/placebo), though it can sometimes be categorical for different treatment types or doses.\n",
    "\n",
    "### Why Calculate Treatment Effects?\n",
    "\n",
    "Treatment effects measure the causal effect of a treatment on an outcome. We calculate them to:\n",
    "\n",
    "1. **Establish causality, not just correlation**: Determine the independent effect of a treatment when other factors are controlled for\n",
    "2. **Understand counterfactuals**: Estimate what would have happened to treated units had they not received treatment (and vice versa)\n",
    "3. **Quantify impact**: Measure not just whether an intervention worked, but how well it worked and for whom\n",
    "4. **Inform decision-making**: Make better decisions about implementing interventions and targeting specific populations\n",
    "\n",
    "### Key Treatment Effect Measures\n",
    "\n",
    "**Average Treatment Effect (ATE):**\n",
    "The average effect of the treatment across the entire population.\n",
    "\n",
    "\\[ ATE = E[Y(1) - Y(0)] \\]\n",
    "\n",
    "Where Y(1) represents the potential outcome if treated, and Y(0) represents the potential outcome if not treated. This measures the expected difference in outcomes if everyone in the population received treatment versus if no one did.\n",
    "\n",
    "**Conditional Average Treatment Effect (CATE):**\n",
    "The average effect of the treatment conditional on specific covariates or characteristics.\n",
    "\n",
    "\\[ CATE(X=x) = E[Y(1) - Y(0) | X=x] \\]\n",
    "\n",
    "This measures how treatment effects vary across different subgroups defined by characteristics X. CATE helps identify which groups benefit most from treatment, enabling more targeted interventions.\n",
    "\n",
    "**Average Treatment Effect on the Treated (ATT/ATET):**\n",
    "The average effect among those who actually received the treatment.\n",
    "\n",
    "\\[ ATT = E[Y(1) - Y(0) | T=1] \\]\n",
    "\n",
    "This answers: \"How much did those who received the treatment actually benefit?\" It's particularly useful when evaluating programs that were targeted at specific populations or when treatment assignment wasn't random.\n",
    "\n",
    "### Challenges in Estimation\n",
    "\n",
    "A fundamental challenge is that we never observe both potential outcomes for the same unitâ€”known as the \"fundamental problem of causal inference\". Various methods address this challenge, including:\n",
    "\n",
    "- Randomized experiments\n",
    "- Regression adjustment\n",
    "- Matching methods\n",
    "- Instrumental variables\n",
    "- Inverse probability weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create the header\n",
    "header = mo.md(\"### 2.4 Key Assumptions in Causal Inference {#assumptions}\")\n",
    "\n",
    "# Create separate elements for each assumption as callouts with fixed height\n",
    "assumption1 = mo.callout(\n",
    "    mo.md(\n",
    "        \"\"\"\n",
    "        ### 1. Unconfoundedness (Ignorability)\n",
    "        Treatment assignment is independent of potential outcomes given observed covariates.\n",
    "\n",
    "        \\\\[ (Y(0), Y(1)) \\\\perp T | X \\\\]\n",
    "        \"\"\"\n",
    "    ),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "assumption2 = mo.callout(\n",
    "    mo.md(\n",
    "        \"\"\"\n",
    "        ### 2. Positivity (Overlap)\n",
    "        Every unit has a non-zero probability of receiving either treatment or control.\n",
    "\n",
    "        \\\\[ 0 < P(T=1|X=x) < 1 \\\\text{ for all } x \\\\]\n",
    "        \"\"\"\n",
    "    ),\n",
    "    kind=\"warn\"\n",
    ")\n",
    "\n",
    "assumption3 = mo.callout(\n",
    "    mo.md(\n",
    "        \"\"\"\n",
    "        ### 3. SUTVA\n",
    "        **Stable Unit Treatment Value Assumption**\n",
    "\n",
    "        * No interference: One unit's treatment doesn't affect another unit's outcome\n",
    "        * No hidden variations of treatment\n",
    "        \"\"\"\n",
    "    ),\n",
    "    kind=\"success\"\n",
    ")\n",
    "\n",
    "# Stack the header and the columns of assumptions\n",
    "mo.vstack([\n",
    "    header,\n",
    "    mo.hstack([assumption1, assumption2, assumption3])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Visualize the overlap assumption\n",
    "    # Use local variables with unique names to avoid conflicts\n",
    "    sample_size = 1000  # Instead of reusing 'n'\n",
    "\n",
    "    # Create two features\n",
    "    X1 = np.random.normal(0, 1, sample_size)\n",
    "    X2 = np.random.normal(0, 1, sample_size)\n",
    "\n",
    "    # Scenario 1: Good overlap\n",
    "    p_good = 1 / (1 + np.exp(-(0.5 * X1)))\n",
    "    treatment_good = np.random.binomial(1, p_good, sample_size)\n",
    "\n",
    "    # Scenario 2: Poor overlap\n",
    "    p_poor = 1 / (1 + np.exp(-(3 * X1 + 2 * X2)))\n",
    "    treatment_poor = np.random.binomial(1, p_poor, sample_size)\n",
    "\n",
    "    # Plot - use a different variable name than 'fig'\n",
    "    overlap_fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Good overlap\n",
    "    axes[0].scatter(X1, X2, c=treatment_good, cmap='coolwarm', alpha=0.6)\n",
    "    axes[0].set_title('Good Overlap')\n",
    "    axes[0].set_xlabel('X1')\n",
    "    axes[0].set_ylabel('X2')\n",
    "\n",
    "    # Poor overlap\n",
    "    axes[1].scatter(X1, X2, c=treatment_poor, cmap='coolwarm', alpha=0.6)\n",
    "    axes[1].set_title('Poor Overlap (Positivity Violation)')\n",
    "    axes[1].set_xlabel('X1')\n",
    "    axes[1].set_ylabel('X2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Use marimo's display method instead of plt.show()\n",
    "    return mo.mpl.interactive(overlap_fig)\n",
    "\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a two-column explanation\n",
    "left_column = mo.md(\"\"\"\n",
    "### Good Overlap (Left Plot)\n",
    "\n",
    "In this scenario, treatment assignment is weakly related to feature X1:\n",
    "- Blue points (control) and red points (treatment) are well-mixed throughout\n",
    "- Units across the entire covariate space have a reasonable probability of being in either treatment or control group\n",
    "- Treatment probability is calculated using a mild logistic function: p=11+exp(-(0.5â‹…X1))p = 1/(1 + exp(-(0.5 * X1)))\n",
    "- This satisfies the positivity assumption because 0<P(T=1âˆ£X=x)<10 < P(T=1|X=x) < 1 for all values of x\n",
    "- Makes reliable causal inference possible because counterfactuals exist for all covariate values\n",
    "- Allows for unbiased estimation of treatment effects (ATE, CATE, ATT)\n",
    "\"\"\").callout(kind=\"success\")\n",
    "\n",
    "right_column = mo.md(\"\"\"\n",
    "### Poor Overlap (Right Plot)\n",
    "\n",
    "In this scenario, treatment assignment is strongly determined by X1 and X2:\n",
    "- Clear separation between blue points (control) and red points (treatment)\n",
    "- Left region has almost exclusively control units\n",
    "- Right region has almost exclusively treatment units\n",
    "- Treatment probability uses a steep logistic function: `p = 1/(1 + exp(-(3 * X1 + 2 * X2)))`\n",
    "- Violates the positivity assumption because for some values of x, P(T=1|X=x) â‰ˆ 0 or P(T=1|X=x) â‰ˆ 1\n",
    "- Makes causal inference unreliable in regions without overlap\n",
    "- Requires strong modeling assumptions or extrapolation to estimate treatment effects\n",
    "- May lead to biased estimates, especially in subgroups with poor representation in one treatment condition\n",
    "\"\"\").callout(kind=\"danger\")\n",
    "\n",
    "# Display the two columns side by side\n",
    "mo.hstack([left_column, right_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create a header for the DAG section\n",
    "    header = mo.md(\"### 2.5 Interactive Causal Diagram\")\n",
    "\n",
    "    # Create a mermaid diagram for the IHDP causal structure\n",
    "    diagram = mo.mermaid(\"\"\"\n",
    "    graph TB\n",
    "        subgraph \"Covariates\"\n",
    "            BR[\"Birth Related\\nx_0, x_1, x_2, x_5, x_6\"]\n",
    "            M[\"Mother's Characteristics\\nx_3, x_4, x_8, x_13-x_16\"]\n",
    "            P[\"Pregnancy Behaviors\\nx_9, x_10, x_11, x_18, x_19\"]\n",
    "            S[\"Socioeconomic\\nx_17\"]\n",
    "            L[\"Location\\nx_20-x_24\"]\n",
    "        end\n",
    "\n",
    "        BR --> T[\"Treatment\\nIHDP Intervention\"]\n",
    "        M --> T\n",
    "        P --> T\n",
    "        S --> T\n",
    "        L --> T\n",
    "\n",
    "        BR --> Y[\"Outcome\\nCognitive Score\"]\n",
    "        M --> Y\n",
    "        P --> Y\n",
    "        S --> Y\n",
    "\n",
    "        T --> Y\n",
    "\n",
    "        style T fill:#ff9999\n",
    "        style Y fill:#99ccff\n",
    "        style BR fill:#f9f9f9\n",
    "        style M fill:#f9f9f9\n",
    "        style P fill:#f9f9f9\n",
    "        style S fill:#f9f9f9\n",
    "        style L fill:#f9f9f9\n",
    "    \"\"\")\n",
    "\n",
    "    explanation = mo.md(\"\"\"\n",
    "    This directed acyclic graph (DAG) represents the assumed causal structure in the IHDP dataset:\n",
    "\n",
    "    - **Covariates** (various characteristics) affect both treatment assignment and outcomes\n",
    "    - **Treatment** (IHDP intervention) affects the outcome\n",
    "    - The arrows represent causal relationships\n",
    "\n",
    "    This structure illustrates why we need causal inference methods - the treatment effect is confounded by covariates that affect both treatment assignment and outcomes.\n",
    "    \"\"\")\n",
    "\n",
    "    # Replace output with the combined elements\n",
    "    mo.output.replace(mo.vstack([header, diagram, explanation]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create a header for the advanced causal inference methods section\n",
    "    header = mo.md(\"### Causal Inference Methods\")\n",
    "\n",
    "    # Create a description of propensity score methods\n",
    "    ps_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Propensity Score Methods\n",
    "\n",
    "        Propensity score methods are statistical techniques for reducing selection bias in observational data. They work by balancing treatment groups on confounding factors to increase the validity of causal inference. The propensity score represents the probability of receiving treatment given a set of observed covariates.\n",
    "\n",
    "        **Key applications include:**\n",
    "        - **Matching**: Pairing treated and control units with similar propensity scores\n",
    "        - **Stratification**: Grouping units into strata based on propensity scores\n",
    "        - **Inverse Probability Weighting**: Weighting observations by the inverse of their propensity score\n",
    "        - **Covariate adjustment**: Including propensity scores as covariates in regression models\n",
    "\n",
    "        While propensity score methods can be powerful, they only adjust for observed confounders and may have limitations when the functional form is misspecified.\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Create a description of difference-in-differences design\n",
    "    did_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Difference-in-Differences (DiD) Design\n",
    "\n",
    "        The Difference-in-Differences design is a quasi-experimental approach that estimates causal effects by comparing the changes in outcomes over time between a treatment group and a control group.\n",
    "\n",
    "        **This method is particularly useful when:**\n",
    "        - Random assignment to treatment is not feasible\n",
    "        - Pre-treatment data is available for both groups\n",
    "        - The parallel trends assumption holds (both groups would follow the same trend in the absence of treatment)\n",
    "\n",
    "        DiD isolates the effect of a treatment by removing biases from permanent differences between groups and from shared time trends.\n",
    "        \"\"\"),\n",
    "        kind=\"warn\"\n",
    "    )\n",
    "\n",
    "    # Replace output with the combined elements\n",
    "    mo.output.replace(mo.vstack([header, ps_methods, did_methods]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create descriptions of more advanced methods\n",
    "    rdd_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Regression Discontinuity Design (RDD)\n",
    "\n",
    "        Regression discontinuity design is an important quasi-experimental approach that can be implemented when treatment assignment is determined by whether a continuous variable crosses a specific threshold.\n",
    "\n",
    "        RDD exploits the fact that units just above and just below the cutoff threshold are similar in all respects except for treatment assignment, creating a situation similar to random assignment around the threshold. This design estimates causal treatment effects by comparing outcomes for units near this threshold.\n",
    "\n",
    "        **Key elements of RDD:**\n",
    "        - A continuous **running variable** (or assignment variable)\n",
    "        - A clear **cutoff threshold** that determines treatment\n",
    "        - **Sharp RDD**: Treatment is deterministically assigned based on the threshold\n",
    "        - **Fuzzy RDD**: Threshold increases probability of treatment but doesn't determine it completely\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Create a description of synthetic control methods\n",
    "    scm_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Synthetic Control Methods\n",
    "\n",
    "        Synthetic control methods allow for causal inference when we have as few as one treated unit and many control units observed over time.\n",
    "\n",
    "        **The approach:**\n",
    "        - Creates a weighted combination of control units that resembles the treated unit before intervention\n",
    "        - Uses this \"synthetic control\" to estimate what would have happened to the treated unit without treatment\n",
    "        - Is especially useful for policy interventions affecting entire regions or populations\n",
    "\n",
    "        This method has been described as \"the most important development in program evaluation in the last decade\" by some researchers and is particularly valuable for case studies with a small number of treated units.\n",
    "        \"\"\"),\n",
    "        kind=\"warn\"\n",
    "    )\n",
    "\n",
    "    # Replace output with the combined elements\n",
    "    mo.output.replace(mo.vstack([rdd_methods, scm_methods]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create descriptions of more advanced methods\n",
    "    sensitivity_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Sensitivity Analysis for Unobserved Confounding\n",
    "\n",
    "        Unobserved confounding is a central barrier to drawing causal inferences from observational data. Sensitivity analysis explores how sensitive causal conclusions are to potential unobserved confounding, helping researchers understand the robustness of their findings.\n",
    "\n",
    "        **Key approaches include:**\n",
    "        - **Rosenbaum bounds**: Quantifies how strong an unobserved confounder would need to be to invalidate results\n",
    "        - **E-values**: Measures the minimum strength of association an unmeasured confounder would need to have with both treatment and outcome to explain away an observed association\n",
    "        - **Simulation-based methods**: Creating plausible scenarios with simulated confounders to test result stability\n",
    "\n",
    "        While methods like propensity score matching can adjust for observed confounding, sensitivity analysis helps address the \"Achilles heel\" of most nonexperimental studies - the potential impact of unmeasured confounding.\n",
    "        \"\"\"),\n",
    "        kind=\"danger\"\n",
    "    )\n",
    "\n",
    "    # Create a description of heterogeneous treatment effects\n",
    "    hte_methods = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Heterogeneous Treatment Effects\n",
    "\n",
    "        Treatment effects often vary across different subpopulations, a phenomenon known as treatment effect heterogeneity. Understanding this heterogeneity is crucial for targeting interventions effectively.\n",
    "\n",
    "        **Methods for estimating heterogeneous effects include:**\n",
    "        - **Subgroup analysis**: Estimating treatment effects within predefined subgroups\n",
    "        - **Interaction terms**: Including treatment-covariate interactions in regression models\n",
    "        - **Causal trees/forests**: Machine learning approaches that adaptively identify subgroups with different treatment effects\n",
    "        - **Meta-learners**: Two-stage approaches that separate the estimation of outcome and treatment effect models\n",
    "\n",
    "        Discovering heterogeneous effects allows for personalized interventions and can reveal important insights about treatment mechanisms that might be masked when looking only at average effects.\n",
    "        \"\"\"),\n",
    "        kind=\"success\"\n",
    "    )\n",
    "\n",
    "    # Replace output with the combined elements\n",
    "    mo.output.replace(mo.vstack([sensitivity_methods, hte_methods]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create a header for the key terms section\n",
    "    header = mo.md(\"### 2.7 Key Terms in Causal Inference {#key-terms}\")\n",
    "\n",
    "    # Create a tabular layout of key causal inference terms\n",
    "    left_terms = mo.md(\"\"\"\n",
    "    | Term | Definition |\n",
    "    |------|------------|\n",
    "    | **Potential Outcomes Framework** | The formal mathematical framework for causal inference where each unit has potential outcomes under different treatment conditions |\n",
    "    | **Average Treatment Effect (ATE)** | The expected difference between potential outcomes if the entire population received treatment versus control |\n",
    "    | **Average Treatment Effect on the Treated (ATT/ATET)** | The average effect for those who actually received the treatment |\n",
    "    | **Conditional Average Treatment Effect (CATE)** | Treatment effects for specific subgroups defined by covariates |\n",
    "    | **Unconfoundedness/Ignorability** | The assumption that treatment assignment is independent of potential outcomes given observed covariates |\n",
    "    | **Positivity/Overlap** | The assumption that every unit has a non-zero probability of receiving each treatment condition |\n",
    "    | **Stable Unit Treatment Value Assumption (SUTVA)** | The assumption that one unit's treatment doesn't affect another unit's outcome |\n",
    "    | **Instrumental Variables** | Variables that affect treatment assignment but not outcomes directly |\n",
    "    \"\"\")\n",
    "\n",
    "    right_terms = mo.md(\"\"\"\n",
    "    | Term | Definition |\n",
    "    |------|------------|\n",
    "    | **Mediation Analysis** | The study of how treatments affect outcomes through intermediate variables |\n",
    "    | **Heterogeneous Treatment Effects** | Variation in treatment effects across different subpopulations |\n",
    "    | **Doubly Robust Estimation** | Methods that remain consistent if either the outcome model or the treatment assignment model is correctly specified |\n",
    "    | **Selection Bias** | Bias arising when treatment groups differ systematically in ways that affect outcomes |\n",
    "    | **Confounding Bias** | Bias due to variables that affect both treatment assignment and outcomes |\n",
    "    | **Common Support/Overlap Region** | The range of propensity scores where both treated and control units exist |\n",
    "    | **G-methods** | A class of causal inference methods for time-varying treatments (g-formula, marginal structural models) |\n",
    "    | **Causal Diagram/DAG** | Directed acyclic graphs that visually represent causal relationships between variables |\n",
    "    \"\"\")\n",
    "\n",
    "    # Add note about the importance of terminology\n",
    "    note = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        Understanding these terms is crucial for effectively applying causal inference methods and correctly interpreting results. Many of these concepts are interrelated and build upon each other to form the foundation of causal reasoning from observational data.\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Replace output with the combined elements\n",
    "    mo.output.replace(mo.vstack([header, mo.hstack([left_terms, right_terms]), note]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 3. The IHDP Dataset {#ihdp-intro}\"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 3.1 Overview of the IHDP Dataset\n",
    "\n",
    "The Infant Health and Development Program (IHDP) was conducted from 1985 to 1988 and was designed to evaluate the effect of educational and family support services along with pediatric follow-up on the development of low birth weight infants.\n",
    "\n",
    "The intervention consisted of:\n",
    "- Home visits by specialists\n",
    "- Child development center attendance\n",
    "- Parent group meetings\n",
    "\n",
    "For causal inference studies, the dataset has been modified by Jennifer Hill (2011) to create a semi-synthetic version where:\n",
    "- Some participants from the treatment group were removed to create selection bias\n",
    "- The outcomes were simulated while preserving the relationships with covariates\n",
    "\n",
    "This modification allows researchers to know the \"ground truth\" causal effects, making it an ideal benchmark dataset for causal inference methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.callout(\"\"\"\n",
    "Dataset Context: The Infant Health and Development Program was a randomized controlled intervention designed to evaluate the effect of home visits by specialists on the cognitive development of premature infants.\"\"\", kind=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Function to download and load the IHDP dataset\n",
    "def load_ihdp_data():\n",
    "    \"\"\"\n",
    "    Load the IHDP dataset for causal inference\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with treatment, outcome, and covariates\n",
    "    \"\"\"\n",
    "    # Create a directory for the data if it doesn't exist    \n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    # Download the data if it doesn't exist\n",
    "    if not os.path.exists('data/ihdp_npci_1.csv'):\n",
    "        print(\"Downloading IHDP dataset...\")\n",
    "        url = \"https://raw.githubusercontent.com/AMLab-Amsterdam/CEVAE/master/datasets/IHDP/csv/ihdp_npci_1.csv\"\n",
    "        urllib.request.urlretrieve(url, 'data/ihdp_npci_1.csv')\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_csv('data/ihdp_npci_1.csv')\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    column_names = ['treatment']\n",
    "    column_names.extend([f'y_{i}' for i in range(2)])  # factual and counterfactual outcomes\n",
    "    column_names.extend([f'mu_{i}' for i in range(2)])  # expected outcomes without noise\n",
    "    column_names.extend([f'x_{i}' for i in range(25)])  # covariates\n",
    "\n",
    "    data.columns = column_names\n",
    "\n",
    "    # Rename for more intuitive understanding\n",
    "    data.rename(columns={\n",
    "        'y_0': 'y_factual',\n",
    "        'y_1': 'y_cfactual',\n",
    "        'mu_0': 'mu_0',\n",
    "        'mu_1': 'mu_1'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the IHDP dataset\n",
    "ihdp_data = load_ihdp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 4. Exploratory Data Analysis {#eda}\"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"### 4.1 IHDP Dataset Preview {#overview}\")\n",
    "    m2 = mo.ui.dataframe(ihdp_data.head())\n",
    "    mo.output.replace(mo.vstack([m1,m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"### 4.2 Dataset Information\")\n",
    "    info_md = f\"\"\"\n",
    "    - **Number of samples:** {ihdp_data.shape[0]}\n",
    "    - **Number of variables:** {ihdp_data.shape[1]}\n",
    "    - **Treatment assignment rate:** {ihdp_data['treatment'].mean():.2f}\n",
    "    \"\"\"\n",
    "    m3 = mo.md(info_md)\n",
    "    mo.output.replace(mo.vstack([m1,m3]))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"### 4.3 Column Types\")\n",
    "\n",
    "    # Create intermediate dataframe\n",
    "    dtype_df = pd.DataFrame({\n",
    "        'Column': list(ihdp_data.dtypes.index),\n",
    "        'Data Type': [str(x) for x in ihdp_data.dtypes.values]\n",
    "    })\n",
    "\n",
    "    m2 = mo.ui.dataframe(dtype_df)\n",
    "    mo.output.replace(mo.vstack([m1,m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"### 4.4 Treatment Distribution\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.countplot(x='treatment', data=ihdp_data, ax=ax)\n",
    "    ax.set_title('Distribution of Treatment Assignment')\n",
    "    ax.set_xlabel('Treatment (0=Control, 1=Treated)')\n",
    "    ax.set_ylabel('Count')\n",
    "    # Use mo.mpl.interactive instead of just passing the figure\n",
    "    interactive_fig = mo.mpl.interactive(fig)\n",
    "    mo.output.replace(mo.vstack([m1, interactive_fig]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Visualize outcome distributions by treatment\n",
    "def _():\n",
    "    m1 = mo.md(\"### 4.5 Outcome Distributions by Treatment\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.boxplot(x='treatment', y='y_factual', data=ihdp_data, ax=ax)\n",
    "    ax.set_title('Factual Outcome Distribution by Treatment Group')\n",
    "    ax.set_xlabel('Treatment (0=Control, 1=Treated)')\n",
    "    ax.set_ylabel('Outcome')\n",
    "    # Convert to interactive plot\n",
    "    interactive_fig = mo.mpl.interactive(fig)\n",
    "    mo.output.replace(mo.vstack([m1, interactive_fig]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"### 4.6 Summary Statistics\")\n",
    "    m2 = mo.ui.dataframe(ihdp_data.describe())\n",
    "    mo.output.replace(mo.vstack([m1, m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define covariate descriptions\n",
    "covariate_descriptions = {\n",
    "    'x_0': \"Birth weight\",\n",
    "    'x_1': \"Birth order\",\n",
    "    'x_2': \"Head circumference at birth\",\n",
    "    'x_3': \"Mother's age\",\n",
    "    'x_4': \"Mother's education level\",\n",
    "    'x_5': \"Child is first born\",\n",
    "    'x_6': \"Child is male\",\n",
    "    'x_7': \"Twin\",\n",
    "    'x_8': \"Mother's race/ethnicity\",\n",
    "    'x_9': \"Mother smoked during pregnancy\",\n",
    "    'x_10': \"Mother drank alcohol during pregnancy\",\n",
    "    'x_11': \"Mother had drugs during pregnancy\",\n",
    "    'x_12': \"Neonatal health index\",\n",
    "    'x_13': \"Mother is married\",\n",
    "    'x_14': \"Mother worked during pregnancy\",\n",
    "    'x_15': \"Mother had prenatal care\",\n",
    "    'x_16': \"Mother's weight gain during pregnancy\",\n",
    "    'x_17': \"Family income\",\n",
    "    'x_18': \"Preterm birth\",\n",
    "    'x_19': \"Birth complications\",\n",
    "    'x_20': \"Site 1\",\n",
    "    'x_21': \"Site 2\",\n",
    "    'x_22': \"Site 3\",\n",
    "    'x_23': \"Site 4\",\n",
    "    'x_24': \"Site 5\"\n",
    "}\n",
    "\n",
    "# Create a DataFrame for UI display\n",
    "covariates_df = pd.DataFrame({\n",
    "    'Variable': list(covariate_descriptions.keys()),\n",
    "    'Description': list(covariate_descriptions.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create markdown header for covariate descriptions\n",
    "    m1 = mo.md(\"### 4.7 Covariate Descriptions {#covariates}\")\n",
    "\n",
    "    # Display covariate descriptions as an interactive dataframe\n",
    "    m2 = mo.ui.dataframe(covariates_df)\n",
    "\n",
    "    # Replace output with vertically stacked components\n",
    "    mo.output.replace(mo.vstack([m1, m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Define numerical covariates\n",
    "    numerical_covs = [f'x_{i}' for i in range(5)] + ['x_12']\n",
    "\n",
    "    # Create markdown header for numerical statistics\n",
    "    m1 = mo.md(\"### 4.8 Numerical Covariate Statistics\")\n",
    "\n",
    "    # Display summary statistics as an interactive dataframe\n",
    "    m2 = mo.ui.dataframe(ihdp_data[numerical_covs].describe())\n",
    "\n",
    "    # Replace output with vertically stacked components\n",
    "    mo.output.replace(mo.vstack([m1, m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Define binary covariates\n",
    "    binary_covs = [f'x_{i}' for i in range(5, 25) if i != 12]\n",
    "\n",
    "    # Calculate binary rates\n",
    "    binary_rates = ihdp_data[binary_covs].mean().reset_index()\n",
    "    binary_rates.columns = ['Variable', 'Rate']\n",
    "    binary_rates['Description'] = binary_rates['Variable'].map(covariate_descriptions)\n",
    "\n",
    "    # Create markdown header for binary rates\n",
    "    m1 = mo.md(\"### 4.9 Binary Covariate Rates\")\n",
    "\n",
    "    # Display binary rates as an interactive dataframe\n",
    "    m2 = mo.ui.dataframe(binary_rates)\n",
    "\n",
    "    # Replace output with vertically stacked components\n",
    "    mo.output.replace(mo.vstack([m1, m2]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dropdown for covariate selection\n",
    "available_covs = [f'x_{i}' for i in range(5)] + ['x_12']\n",
    "covariate_selector = mo.ui.dropdown(\n",
    "    options=available_covs,\n",
    "    value='x_0',\n",
    "    label=\"Select covariate to visualize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Visualize distribution of selected covariate\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    def plot_distribution(selected_cov):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.histplot(ihdp_data[selected_cov], ax=ax, kde=True)\n",
    "        desc = covariate_descriptions.get(selected_cov, \"\")\n",
    "        if len(desc) > 30:\n",
    "            desc = desc[:30] + \"...\"\n",
    "        ax.set_title(f\"{selected_cov}: {desc}\")\n",
    "        ax.set_xlabel(selected_cov)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        return fig\n",
    "\n",
    "    # Create header and layout\n",
    "    m1 = mo.md(\"### 4.10 Distribution of Key Numerical Covariates\")\n",
    "\n",
    "    # Get description for the selected covariate\n",
    "    selected_desc = mo.md(f\"**Selected variable**: {covariate_selector.value} - {covariate_descriptions.get(covariate_selector.value, '')}\")\n",
    "\n",
    "    # Create plot based on selected covariate\n",
    "    plot_fig = plot_distribution(covariate_selector.value)\n",
    "    interactive_plot = mo.mpl.interactive(plot_fig)\n",
    "\n",
    "    # Replace output with interactive components\n",
    "    mo.output.replace(mo.vstack([\n",
    "        m1,\n",
    "        mo.hstack([covariate_selector]),\n",
    "        selected_desc,\n",
    "        interactive_plot\n",
    "    ]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    m1 = mo.md(\"\"\"\n",
    "    1. Strong positive correlation (0.85) between x0x_0 (child's birth weight) and x1x_1 (child's birth order). This indicates that higher birth order (later-born children) tends to be associated with higher birth weight.\n",
    "    2. Strong negative correlation (-0.76 and -0.7) between x2x_2 (head circumference) and both birth weight (x0x_0) and birth order (x1x_1). This is somewhat counterintuitive, as we might expect larger babies to have larger head circumferences. This inverse relationship could suggest measurement issues or certain medical conditions in the dataset.\n",
    "    3. Most demographic variables (x3x_3 - mother's age, x4x_4 - mother's education) show weak correlations with other variables, suggesting independence.\n",
    "    4. Child's neonatal health index (x12x_12) has mostly weak correlations with other variables, with the strongest being a mild positive correlation (0.13) with mother's age.\n",
    "    \"\"\")\n",
    "    m2 = mo.callout(\"For causal inference, these correlations are important because strongly correlated variables can create confounding issues. For example, if treatment assignment is related to birth weight, birth order might inadvertently become a confounder due to its strong correlation with birth weight.\", kind=\"info\")\n",
    "    mo.output.replace(mo.vstack([m1, m2]))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 5. Setting Up for Causal Analysis {#analysis-setup}\"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 5.1 Data Preparation {#data-prep}\n",
    "\n",
    "    > ðŸ”§ **Step 1**: Properly prepare the data for causal analysis\n",
    "\n",
    "    Before implementing causal inference methods, we need to prepare our data appropriately. This includes:\n",
    "\n",
    "    1. Splitting the data into training and test sets\n",
    "    2. Scaling continuous features\n",
    "    3. Identifying the types of variables (continuous vs. binary)\n",
    "    4. Handling any missing values (if present)\n",
    "\n",
    "    This preparation ensures that our causal inference methods will work properly and produce reliable estimates.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Identify continuous and binary variables\n",
    "continuous_vars = ['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_12']\n",
    "binary_vars = [f'x_{i}' for i in range(5, 25) if i != 12]\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = ihdp_data.isnull().sum()\n",
    "print(f\"Missing values in dataset: {missing_values.sum()}\")\n",
    "\n",
    "# Split the data into features, treatment, and outcomes\n",
    "X = ihdp_data[[f'x_{i}' for i in range(25)]]\n",
    "T = ihdp_data['treatment']\n",
    "Y = ihdp_data['y_factual']\n",
    "\n",
    "# Also track true potential outcomes for evaluation (not available in real-world scenarios)\n",
    "Y0 = ihdp_data['mu_0']\n",
    "Y1 = ihdp_data['mu_1']\n",
    "\n",
    "# Split into training and test sets (80/20 split)\n",
    "X_train, X_test, T_train, T_test, Y_train, Y_test, Y0_train, Y0_test, Y1_train, Y1_test = train_test_split(\n",
    "    X, T, Y, Y0, Y1, test_size=0.2, random_state=42\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "for var in continuous_vars:\n",
    "    X_train_scaled[var] = scaler.fit_transform(X_train[[var]])\n",
    "    X_test_scaled[var] = scaler.transform(X_test[[var]])\n",
    "\n",
    "# Print information about the split\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Treatment rate in training set: {T_train.mean():.2f}\")\n",
    "print(f\"Treatment rate in test set: {T_test.mean():.2f}\")\n",
    "print(f\"True ATE in training set: {(Y1_train - Y0_train).mean():.4f}\")\n",
    "print(f\"True ATE in test set: {(Y1_test - Y0_test).mean():.4f}\")\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "def _():\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_context(\"notebook\", font_scale=1.2)    \n",
    "\n",
    "    # Scale continuous features\n",
    "    # Visualize the data split and scaling effects\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot 1: Training vs Test split by treatment\n",
    "    train_counts = pd.DataFrame({\n",
    "        'Set': ['Training'] * 2,\n",
    "        'Treatment': ['Control', 'Treated'],\n",
    "        'Count': [(T_train == 0).sum(), (T_train == 1).sum()]\n",
    "    })\n",
    "\n",
    "    test_counts = pd.DataFrame({\n",
    "        'Set': ['Test'] * 2,\n",
    "        'Treatment': ['Control', 'Treated'],\n",
    "        'Count': [(T_test == 0).sum(), (T_test == 1).sum()]\n",
    "    })\n",
    "\n",
    "    counts_df = pd.concat([train_counts, test_counts])\n",
    "\n",
    "    sns.barplot(x='Set', y='Count', hue='Treatment', data=counts_df, ax=axes[0])\n",
    "    axes[0].set_title('Sample Distribution in Training and Test Sets')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "\n",
    "    # Plot 2: Effect of scaling on a continuous variable\n",
    "    sns.histplot(X_train['x_0'], kde=True, label='Before scaling', ax=axes[1], alpha=0.5)\n",
    "    sns.histplot(X_train_scaled['x_0'], kde=True, label='After scaling', ax=axes[1], alpha=0.5)\n",
    "    axes[1].set_title('Effect of Scaling on Birth Weight (x_0)')\n",
    "    axes[1].set_xlabel('Value')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Replace plt.show() with mo.mpl.interactive for interactivity\n",
    "    return mo.mpl.interactive(fig)\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    analysis_md = mo.md(\"\"\"\n",
    "    **Analysis of Data Preparation:**\n",
    "\n",
    "    The dataset preparation above accomplishes several important steps for causal inference:\n",
    "\n",
    "    1. **Splitting the data** into training (80%) and test (20%) sets allows us to evaluate the performance of our causal inference methods on unseen data, which is crucial for assessing their generalizability.\n",
    "\n",
    "    2. **Standardizing continuous variables** ensures that variables with different scales don't unduly influence our models. This is particularly important for methods that are sensitive to the scale of the input features, such as matching based on distances or regularized regression.\n",
    "\n",
    "    3. **Preserving the treatment assignment rate** across training and test sets maintains the same level of class imbalance, which is important for methods that are sensitive to treatment prevalence.\n",
    "\n",
    "    4. **Verifying the absence of missing values** confirms that we don't need to implement imputation strategies, which could introduce additional complexity and potential bias.\n",
    "\n",
    "    The visualization on the left shows the distribution of treated and control units in both training and test sets, confirming that the treatment assignment rate is similar between the splits. The visualization on the right illustrates the effect of standardization on the birth weight variable, transforming it to have zero mean and unit variance, which makes it more suitable for many statistical and machine learning methods.\n",
    "    \"\"\")\n",
    "\n",
    "    mo.callout(analysis_md, kind=\"info\")\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 5.2 Formulating the Causal Question {#causal-question}\n",
    "\n",
    "    > ðŸ“‹ **Step 2**: Define what causal effect you want to estimate\n",
    "\n",
    "    Causal inference begins with a clear formulation of the causal question. For the IHDP dataset, our primary question is:\n",
    "\n",
    "    **\"What is the effect of specialist home visits (treatment) on the cognitive test scores (outcome) of premature infants?\"**\n",
    "\n",
    "    To formalize this question, we need to define:\n",
    "\n",
    "    1. **Treatment variable (T)**: Binary indicator for receiving home visits\n",
    "    2. **Outcome variable (Y)**: Cognitive test scores\n",
    "    3. **Covariates (X)**: Baseline characteristics that may influence treatment assignment or outcomes\n",
    "    4. **Target population**: Premature infants with low birth weight\n",
    "    5. **Causal estimand**: The specific causal quantity we want to estimate\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgWD",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Define the causal question components\n",
    "    treatment_var = 'Treatment (IHDP intervention)'\n",
    "    outcome_var = 'Cognitive test scores'\n",
    "    covariates_var = 'Baseline characteristics (25 variables)'\n",
    "\n",
    "    # Calculate true causal effects (available in this simulated dataset)\n",
    "    true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "    # Calculate naive estimate\n",
    "    naive_ate = Y_train[T_train == 1].mean() - Y_train[T_train == 0].mean()\n",
    "\n",
    "    # Calculate true ATT (Average Treatment Effect on the Treated)\n",
    "    true_att = (Y1_train[T_train == 1] - Y0_train[T_train == 1]).mean()\n",
    "\n",
    "    # Create a DataFrame for visualization\n",
    "    estimands_df = pd.DataFrame({\n",
    "        'Estimand': ['ATE (Average Treatment Effect)', 'ATT (Average Treatment Effect on Treated)', 'Naive Difference in Means'],\n",
    "        'Value': [true_ate, true_att, naive_ate],\n",
    "        'Description': [\n",
    "            'Expected effect if everyone received treatment vs. none',\n",
    "            'Average effect among those who actually received treatment',\n",
    "            'Simple difference in mean outcomes (biased estimate)'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Visualize causal estimands\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(estimands_df['Estimand'], estimands_df['Value'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    plt.title('Causal Estimands in the IHDP Dataset')\n",
    "    plt.xlabel('Effect Size')\n",
    "\n",
    "    # Add value labels to the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{estimands_df[\"Value\"].iloc[i]:.4f}', \n",
    "                 va='center')\n",
    "\n",
    "    # Add grid lines for readability\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create a table of key causal components\n",
    "    causal_components = pd.DataFrame({\n",
    "        'Component': ['Treatment Variable', 'Outcome Variable', 'Covariates', 'Target Population', 'Primary Causal Estimand'],\n",
    "        'Definition': [treatment_var, outcome_var, covariates_var, \n",
    "                    'Premature infants with low birth weight', 'Average Treatment Effect (ATE)']\n",
    "    })\n",
    "\n",
    "    # Create the interactive output\n",
    "    causal_question_layout = mo.vstack([\n",
    "        mo.md(\"#### Causal Components in the IHDP Study\"),\n",
    "        mo.ui.table(causal_components),\n",
    "\n",
    "        mo.md(\"#### Comparison of Causal Estimands\"),\n",
    "        mo.mpl.interactive(plt.gcf()),\n",
    "\n",
    "        mo.md(\"\"\"**Note:** In real-world causal inference, we typically don't know the true causal effects. \n",
    "        The IHDP dataset is semi-synthetic, allowing us to know the ground truth for evaluation.\"\"\")\n",
    "    ])\n",
    "    mo.output.append(causal_question_layout)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yOPj",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    analysis_md = mo.md(\"\"\"\n",
    "    **Analysis of the Causal Question Formulation:**\n",
    "\n",
    "    The causal question has been clearly formulated, which is a crucial first step in any causal inference analysis. Key observations:\n",
    "\n",
    "    1. **Treatment-Outcome Relationship**: We're interested in the effect of the IHDP intervention (home visits) on cognitive test scores, a well-defined relationship that aligns with policy and educational interventions.\n",
    "\n",
    "    2. **Causal Estimands**: We've defined multiple estimands of interest, with the Average Treatment Effect (ATE) as our primary focus. The ATE represents the expected change in cognitive scores if the entire population received the treatment versus if none did.\n",
    "\n",
    "    3. **ATT vs ATE**: The Average Treatment Effect on the Treated (ATT) is very close to the ATE in this dataset (difference of only 0.0041). This suggests that the treatment effect is relatively homogeneous across the population, or that selection into treatment wasn't strongly related to treatment effect heterogeneity.\n",
    "\n",
    "    4. **Naive Estimate**: The naive difference in means is similar to the true ATE in this dataset. This is somewhat unexpected, as we would typically expect selection bias to create a difference between the naive estimate and the true causal effect. This similarity could be a characteristic of how the semi-synthetic dataset was generated.\n",
    "\n",
    "    Formulating these specific causal questions allows us to select appropriate methods for estimation and evaluate their performance against known ground truth values in this unique dataset.\n",
    "    \"\"\")\n",
    "\n",
    "    mo.output.append(mo.callout(analysis_md, kind=\"info\"))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwwy",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 5.3 Propensity Score Analysis {#propensity-analysis}\n",
    "\n",
    "    > ðŸˆ **Step 3**: Analyze propensity scores to check assumptions and prepare for causal methods\n",
    "\n",
    "    Propensity scores are a key concept in causal inference, representing the probability of receiving treatment given observed covariates. They're useful for:\n",
    "\n",
    "    1. **Assessing overlap**: Checking the positivity assumption by examining the distribution of propensity scores\n",
    "    2. **Creating balance**: Helping ensure that treated and control groups are comparable\n",
    "    3. **Estimation**: Using in various estimation methods like inverse probability weighting, matching, and stratification\n",
    "\n",
    "    Let's estimate propensity scores for our dataset and analyze their properties.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LJZf",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.callout(mo.md(\n",
    "    \"\"\"\n",
    "    #### What are Propensity Scores?\n",
    "\n",
    "    Propensity scores represent the probability that a unit receives the treatment, conditional on observed covariates. Mathematically, the propensity score is defined as:\n",
    "\n",
    "    \\[\n",
    "    e(X) = P(T=1|X)\n",
    "    \\]\n",
    "\n",
    "    Where \\(T\\) is the treatment indicator and \\(X\\) represents the covariates.\n",
    "\n",
    "    #### Why Are Propensity Scores Important?\n",
    "\n",
    "    Propensity scores help address the fundamental challenge in causal inference: units are either treated or untreated, never both. By conditioning on the propensity score, we can create balance between treated and control groups, mimicking a randomized experiment.\n",
    "\n",
    "    Key properties of propensity scores include:\n",
    "\n",
    "    1. **Balancing score**: Conditioning on the propensity score balances the distribution of covariates between treatment groups\n",
    "    2. **Dimensionality reduction**: Reduces multiple covariates to a single score\n",
    "    3. **Identification of areas of common support**: Helps identify regions where causal inference is reliable\n",
    "\n",
    "    We'll estimate propensity scores using logistic regression and also explore a machine learning approach with random forests.\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CcZR",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.callout(mo.md(\"\"\"\n",
    "**Summary of Propensity Score Analysis:**\n",
    "\n",
    "1. We've estimated propensity scores using both logistic regression and random forest models.\n",
    "2. The distributions show some separation between treated and control groups, which is expected in observational data.\n",
    "3. The common support analysis confirms that most units fall within regions where causal inference is reliable.\n",
    "4. The covariate balance assessment identifies which variables contribute most to selection bias.\n",
    "\n",
    "These propensity scores will be used in subsequent sections for implementing various causal inference methods including:\n",
    "- Inverse Probability Weighting (IPW)\n",
    "- Propensity Score Matching\n",
    "- Propensity Score Stratification\n",
    "- Doubly Robust methods\n",
    "\n",
    "Each method leverages propensity scores differently to estimate causal effects while accounting for confounding.\n",
    "\"\"\"), kind=\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWSi",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 6. Implementing Causal Inference Methods {#methods}\n",
    "\n",
    "    In this section, we'll implement and evaluate various causal inference methods on the IHDP dataset. We'll start with simple methods, then move to propensity score-based approaches, and finally explore advanced machine learning methods. For each method, we'll:\n",
    "\n",
    "    1. Explain the methodology and key assumptions\n",
    "    2. Implement the method on our training data\n",
    "    3. Evaluate its performance against the known ground truth\n",
    "    4. Discuss strengths, weaknesses, and practical considerations\n",
    "    \"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlud",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"### 6.1 Simple Causal Inference Methods {#simple-methods}\n",
    "\n",
    "    > ðŸ” **Step 1**: Start with simple methods before moving to more complex approaches\n",
    "\n",
    "    We'll begin with straightforward approaches that form the foundation of causal inference. These methods are easy to implement and interpret, making them excellent starting points for causal analysis.\n",
    "    \"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZnO",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a description of naive mean difference approach\n",
    "naive_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Naive Mean Difference\n",
    "\n",
    "    The simplest approach to estimating causal effects is to compare the average outcomes between treated and control groups:\n",
    "\n",
    "    \\[\n",
    "    \\hat{ATE}_{naive} = \\frac{1}{n_1}\\sum_{i:T_i=1}Y_i - \\frac{1}{n_0}\\sum_{i:T_i=0}Y_i\n",
    "    \\]\n",
    "\n",
    "    where \\(n_1\\) is the number of treated units and \\(n_0\\) is the number of control units.\n",
    "\n",
    "    **Key Assumption**: Treatment is randomly assigned (no confounding).\n",
    "\n",
    "    **Limitations**: In observational studies, this estimate is often biased due to confounding factors that affect both treatment assignment and outcomes.\n",
    "    \"\"\"),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "# Create a description of regression adjustment approach\n",
    "regression_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Regression Adjustment\n",
    "\n",
    "    This method controls for confounding by including covariates in a regression model:\n",
    "\n",
    "    \\[\n",
    "    Y_i = \\alpha + \\tau T_i + \\beta X_i + \\epsilon_i\n",
    "    \\]\n",
    "\n",
    "    The coefficient \\(\\tau\\) of the treatment variable \\(T\\) provides an estimate of the ATE.\n",
    "\n",
    "    **Key Assumption**: The regression model is correctly specified (includes all confounders and captures their relationships with the outcome).\n",
    "\n",
    "    **Advantages**: Simple to implement, interpretable, can handle continuous and binary covariates.\n",
    "\n",
    "    **Limitations**: Relies on strong assumptions about the functional form of the relationship between covariates and outcomes.\n",
    "    \"\"\"),\n",
    "    kind=\"warn\"\n",
    ")\n",
    "\n",
    "# Create a description of stratification approach\n",
    "stratification_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Stratification/Subclassification\n",
    "\n",
    "    This method divides the data into subgroups (strata) based on important covariates, estimates treatment effects within each stratum, and takes a weighted average:\n",
    "\n",
    "    \\[\n",
    "    \\hat{ATE}_{strat} = \\sum_{s=1}^{S} w_s (\\bar{Y}_{s,1} - \\bar{Y}_{s,0})\n",
    "    \\]\n",
    "\n",
    "    where \\(\\bar{Y}_{s,1}\\) is the average outcome for treated units in stratum \\(s\\), \\(\\bar{Y}_{s,0}\\) is the average for control units, and \\(w_s\\) is the proportion of units in stratum \\(s\\).\n",
    "\n",
    "    **Key Assumption**: Within each stratum, treatment is effectively randomly assigned.\n",
    "\n",
    "    **Advantages**: Intuitive, handles non-linear relationships, allows examination of effect heterogeneity.\n",
    "\n",
    "    **Limitations**: Can only stratify on a few variables before encountering sparsity issues.\n",
    "    \"\"\"),\n",
    "    kind=\"success\"\n",
    ")\n",
    "\n",
    "# Display all method descriptions together\n",
    "mo.vstack([naive_description, regression_description, stratification_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xvXZ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # 1. Naive Mean Difference\n",
    "    def naive_estimator(T, Y):\n",
    "        \"\"\"Calculate naive mean difference between treated and control outcomes\"\"\"\n",
    "        treated_mean = Y[T == 1].mean()\n",
    "        control_mean = Y[T == 0].mean()\n",
    "        ate = treated_mean - control_mean\n",
    "        return ate\n",
    "\n",
    "    # Calculate naive ATE\n",
    "    naive_ate = naive_estimator(T_train, Y_train)\n",
    "\n",
    "    # Get true ATE for comparison\n",
    "    true_ate = (Y1_train - Y0_train).mean()    \n",
    "\n",
    "    def regression_adjustment(X, T, Y):\n",
    "        \"\"\"Estimate ATE using regression adjustment\"\"\"\n",
    "        # Create a copy of X to avoid modifying the original\n",
    "        X_with_treatment = X.copy()\n",
    "        # Add treatment as a feature\n",
    "        X_with_treatment['treatment'] = T\n",
    "\n",
    "        # Fit linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_with_treatment, Y)\n",
    "\n",
    "        # Extract treatment coefficient\n",
    "        treatment_idx = X_with_treatment.columns.get_loc('treatment')\n",
    "        ate = model.coef_[treatment_idx]\n",
    "\n",
    "        return ate, model\n",
    "\n",
    "    # Calculate regression-adjusted ATE\n",
    "    reg_ate, reg_model = regression_adjustment(X_train_scaled, T_train, Y_train)\n",
    "\n",
    "    # 3. Stratification on an important covariate\n",
    "    def stratification(X, T, Y, stratify_var, n_strata=5):\n",
    "        \"\"\"Estimate ATE by stratifying on a variable\"\"\"\n",
    "        # Create a copy of the data with relevant variables\n",
    "        data = pd.DataFrame({\n",
    "            'T': T,\n",
    "            'Y': Y,\n",
    "            'strat_var': X[stratify_var]\n",
    "        })\n",
    "\n",
    "        # Create equal-sized strata based on the stratification variable\n",
    "        data['stratum'] = pd.qcut(data['strat_var'], n_strata, labels=False)\n",
    "\n",
    "        # Calculate treatment effect within each stratum\n",
    "        strata_effects = []\n",
    "        strata_sizes = []\n",
    "\n",
    "        for s in range(n_strata):\n",
    "            stratum_data = data[data['stratum'] == s]\n",
    "            # Only calculate if we have both treated and control units\n",
    "            if (stratum_data['T'] == 1).sum() > 0 and (stratum_data['T'] == 0).sum() > 0:\n",
    "                stratum_treated = stratum_data[stratum_data['T'] == 1]['Y'].mean()\n",
    "                stratum_control = stratum_data[stratum_data['T'] == 0]['Y'].mean()\n",
    "                stratum_effect = stratum_treated - stratum_control\n",
    "                stratum_size = len(stratum_data)\n",
    "\n",
    "                strata_effects.append(stratum_effect)\n",
    "                strata_sizes.append(stratum_size)\n",
    "\n",
    "        # Calculate weighted average (weighted by stratum size)\n",
    "        total_size = sum(strata_sizes)\n",
    "        weights = [size / total_size for size in strata_sizes]\n",
    "        weighted_ate = sum(effect * weight for effect, weight in zip(strata_effects, weights))\n",
    "\n",
    "        return weighted_ate, strata_effects, weights\n",
    "\n",
    "    # Choose birth weight (x_0) as stratification variable\n",
    "    strat_ate, strat_effects, strat_weights = stratification(X_train_scaled, T_train, Y_train, 'x_0')\n",
    "\n",
    "    # Compile results\n",
    "    methods = ['Naive Mean Difference', 'Regression Adjustment', 'Stratification']\n",
    "    estimates = [naive_ate, reg_ate, strat_ate]\n",
    "    biases = [est - true_ate for est in estimates]\n",
    "    abs_biases = [abs(bias) for bias in biases]\n",
    "\n",
    "    # Print results\n",
    "    print(\"Simple Methods Results:\")\n",
    "    print(f\"True ATE: {true_ate:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    for method, estimate, bias in zip(methods, estimates, biases):\n",
    "        print(f\"{method}: ATE = {estimate:.4f}, Bias = {bias:.4f}\")\n",
    "\n",
    "    # Return all relevant objects for use in visualization\n",
    "        # Create figure for comparing estimates from simple methods\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot 1: Bar chart comparing ATE estimates\n",
    "    bars = axes[0].bar(methods, estimates, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    axes[0].axhline(y=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "    axes[0].set_title('ATE Estimates from Simple Methods')\n",
    "    axes[0].set_ylabel('ATE Estimate')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Add value labels to the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.05,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', \n",
    "            rotation=0\n",
    "        )\n",
    "\n",
    "    # Plot 2: Bar chart for bias\n",
    "    biases_df = pd.DataFrame({\n",
    "        'Method': methods,\n",
    "        'Absolute Bias': abs_biases\n",
    "    }).sort_values('Absolute Bias')\n",
    "\n",
    "    bars = axes[1].barh(biases_df['Method'], biases_df['Absolute Bias'], color=['lightgreen', 'skyblue', 'salmon'])\n",
    "    axes[1].set_title('Absolute Bias of Simple Methods')\n",
    "    axes[1].set_xlabel('Absolute Bias')\n",
    "\n",
    "    # Add value labels to the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        axes[1].text(\n",
    "            width + 0.005, \n",
    "            bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.4f}',\n",
    "            ha='left', va='center'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create visualization for stratification results\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot stratum-specific effects\n",
    "    strata_indices = list(range(len(strat_effects)))\n",
    "    ax2.bar(strata_indices, strat_effects, alpha=0.7)\n",
    "    ax2.axhline(y=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax2.set_title('Treatment Effects by Stratum')\n",
    "    ax2.set_xlabel('Stratum (Birth Weight Quintile)')\n",
    "    ax2.set_ylabel('Treatment Effect')\n",
    "    ax2.set_xticks(strata_indices)\n",
    "    ax2.set_xticklabels([f'Q{i+1}\\n({w:.2f})' for i, w in enumerate(strat_weights)])\n",
    "    ax2.legend()\n",
    "\n",
    "    # Create layout containing both visualizations\n",
    "    header = mo.md(\"#### Simple Methods Comparison\")\n",
    "    methods_comparison = mo.mpl.interactive(fig)\n",
    "\n",
    "    strat_header = mo.md(\"#### Heterogeneous Effects by Birth Weight Stratum\")\n",
    "    strat_note = mo.md(\"*Note: Numbers in parentheses show the weight of each stratum in the overall estimate.*\")\n",
    "    strat_effects_plot = mo.mpl.interactive(fig2)\n",
    "\n",
    "    methods_analysis = mo.callout(\n",
    "        mo.md(f\"\"\"\n",
    "        **Analysis of Simple Methods:**\n",
    "\n",
    "        1. **Naive Mean Difference**: The naive estimate has a bias of {biases[0]:.4f}, which is {abs_biases[0]:.4f} in absolute terms. This is relatively {'small' if abs_biases[0] < 0.1 else 'moderate' if abs_biases[0] < 0.5 else 'large'}, suggesting that selection bias in this dataset may not be very strong.\n",
    "\n",
    "        2. **Regression Adjustment**: This method has a bias of {biases[1]:.4f} (absolute: {abs_biases[1]:.4f}), {'improving upon' if abs_biases[1] < abs_biases[0] else 'performing worse than'} the naive estimator. This {'improvement' if abs_biases[1] < abs_biases[0] else 'decline'} in performance suggests that the linear model {'adequately' if abs_biases[1] < abs_biases[0] else 'inadequately'} captures the relationship between covariates and outcomes.\n",
    "\n",
    "        3. **Stratification**: This approach has a bias of {biases[2]:.4f} (absolute: {abs_biases[2]:.4f}), {'outperforming' if abs_biases[2] < min(abs_biases[0], abs_biases[1]) else 'underperforming compared to'} the other methods. Stratification by birth weight reveals some heterogeneity in treatment effects across strata, which is valuable information for targeting interventions.\n",
    "\n",
    "        Overall, the {'Regression Adjustment' if abs_biases[1] == min(abs_biases) else 'Naive Mean Difference' if abs_biases[0] == min(abs_biases) else 'Stratification'} method performs best in terms of bias reduction for this dataset. However, all methods show relatively small bias, suggesting that the selection mechanism in this semi-synthetic dataset may not induce strong confounding.\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Combine all elements\n",
    "    mo.output.replace(mo.vstack([\n",
    "        header,\n",
    "        methods_comparison,\n",
    "        methods_analysis,\n",
    "        strat_header,\n",
    "        strat_effects_plot,\n",
    "        strat_note\n",
    "    ]))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CLip",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"### 6.2 Propensity Score Methods {#ps-methods}\n",
    "\n",
    "    > ðŸŽ¯ **Step 2**: Apply propensity score-based methods to adjust for confounding\n",
    "\n",
    "    Building on the propensity scores we estimated earlier, we'll now implement methods that use these scores to create balance between treated and control groups.\n",
    "    \"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YECM",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a description of IPW approach\n",
    "ipw_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Inverse Probability Weighting (IPW)\n",
    "\n",
    "    IPW creates a pseudo-population where the confounding influence is eliminated by weighting each observation by the inverse of its probability of receiving the treatment it actually received:\n",
    "\n",
    "    \\[\n",
    "    \\hat{ATE}_{IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{T_i Y_i}{e(X_i)} - \\frac{(1-T_i) Y_i}{1-e(X_i)} \\right)\n",
    "    \\]\n",
    "\n",
    "    where \\(e(X_i)\\) is the propensity score for unit \\(i\\).\n",
    "\n",
    "    **Key Advantages**:\n",
    "    - Uses all data points\n",
    "    - Simple to implement\n",
    "    - Intuitive connection to survey sampling\n",
    "\n",
    "    **Limitations**:\n",
    "    - Sensitive to extreme propensity scores (near 0 or 1)\n",
    "    - Can have high variance\n",
    "    - Requires well-specified propensity model\n",
    "    \"\"\"),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "# Create a description of matching approach\n",
    "matching_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Propensity Score Matching\n",
    "\n",
    "    Matching pairs treated units with control units that have similar propensity scores. The average difference in outcomes between matched pairs provides an estimate of the ATE:\n",
    "\n",
    "    \\[\n",
    "    \\hat{ATE}_{match} = \\frac{1}{n_1} \\sum_{i:T_i=1} (Y_i - Y_{j(i)})\n",
    "    \\]\n",
    "\n",
    "    where \\(j(i)\\) is the index of the control unit matched to treated unit \\(i\\).\n",
    "\n",
    "    **Key Advantages**:\n",
    "    - Intuitive and easy to explain\n",
    "    - Can be combined with exact matching on key variables\n",
    "    - Preserves the original outcome variable scale\n",
    "\n",
    "    **Limitations**:\n",
    "    - Discards units that cannot be matched\n",
    "    - Choice of matching algorithm and caliper can affect results\n",
    "    - Matches may not be perfect, leaving some residual confounding\n",
    "    \"\"\"),\n",
    "    kind=\"warn\"\n",
    ")\n",
    "\n",
    "# Create a description of stratification approach using propensity scores\n",
    "ps_stratification_description = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Propensity Score Stratification\n",
    "\n",
    "    This method divides the data into strata based on propensity scores, estimates treatment effects within each stratum, and computes a weighted average:\n",
    "\n",
    "    \\[\n",
    "    \\hat{ATE}_{strat} = \\sum_{s=1}^{S} w_s (\\bar{Y}_{s,1} - \\bar{Y}_{s,0})\n",
    "    \\]\n",
    "\n",
    "    where \\(w_s\\) is the proportion of units in stratum \\(s\\), and \\(\\bar{Y}_{s,1}\\) and \\(\\bar{Y}_{s,0}\\) are the average outcomes for treated and control units in that stratum.\n",
    "\n",
    "    **Key Advantages**:\n",
    "    - Uses all data points\n",
    "    - Examines effect heterogeneity across propensity score strata\n",
    "    - Usually reduces ~90% of confounding bias with just 5 strata\n",
    "\n",
    "    **Limitations**:\n",
    "    - Less precise than matching for estimating average effects\n",
    "    - Choice of strata boundaries can affect results\n",
    "    - May not fully eliminate confounding within strata\n",
    "    \"\"\"),\n",
    "    kind=\"success\"\n",
    ")\n",
    "\n",
    "# Display all method descriptions together\n",
    "mo.vstack([ipw_description, matching_description, ps_stratification_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEAS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Implement propensity score methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Estimate propensity scores using logistic regression\n",
    "propensity_model = LogisticRegression(max_iter=1000, C=1.0)\n",
    "propensity_model.fit(X_train_scaled, T_train)\n",
    "\n",
    "# Calculate propensity scores (probability of receiving treatment)\n",
    "propensity_scores = propensity_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Also estimate propensity scores using a Random Forest for comparison\n",
    "rf_propensity_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)\n",
    "rf_propensity_model.fit(X_train_scaled, T_train)\n",
    "rf_propensity_scores = rf_propensity_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Create DataFrame with propensity scores\n",
    "ps_df = pd.DataFrame({\n",
    "    'treatment': T_train,\n",
    "    'ps_logistic': propensity_scores,\n",
    "    'ps_rf': rf_propensity_scores\n",
    "})\n",
    "\n",
    "# Evaluate propensity score models\n",
    "logistic_auc = roc_auc_score(T_train, propensity_scores)\n",
    "rf_auc = roc_auc_score(T_train, rf_propensity_scores)\n",
    "\n",
    "print(f\"Logistic Regression AUC: {logistic_auc:.4f}\")\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "\n",
    "# 1. Implement Inverse Probability Weighting (IPW)\n",
    "# Calculate true ATE for comparison\n",
    "true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "# Function to calculate IPW estimate\n",
    "def ipw_estimator(T, Y, ps, stabilized=True, trimming=None):\n",
    "    \"\"\"Calculate ATE using inverse probability weighting\"\"\"\n",
    "    # Calculate IPW weights\n",
    "    if stabilized:\n",
    "        # Stabilized weights\n",
    "        p_treatment = T.mean()\n",
    "        weights = np.where(T == 1, p_treatment / ps, (1 - p_treatment) / (1 - ps))\n",
    "    else:\n",
    "        # Unstabilized weights\n",
    "        weights = np.where(T == 1, 1 / ps, 1 / (1 - ps))\n",
    "\n",
    "    # Trim weights if requested\n",
    "    if trimming is not None:\n",
    "        max_weight = np.percentile(weights, trimming)\n",
    "        weights = np.minimum(weights, max_weight)\n",
    "\n",
    "    # Calculate weighted means\n",
    "    weighted_treated = np.sum(weights[T == 1] * Y[T == 1]) / np.sum(weights[T == 1])\n",
    "    weighted_control = np.sum(weights[T == 0] * Y[T == 0]) / np.sum(weights[T == 0])\n",
    "\n",
    "    # Calculate ATE\n",
    "    ate = weighted_treated - weighted_control\n",
    "\n",
    "    return ate, weights\n",
    "\n",
    "# Calculate IPW estimates using different settings\n",
    "ipw_results = []\n",
    "\n",
    "for _ps_method, _ps_values in [('Logistic', propensity_scores), ('RF', rf_propensity_scores)]:\n",
    "    for stabilized in [True, False]:\n",
    "        for trimming in [None, 95]:\n",
    "            # Calculate IPW estimate\n",
    "            ipw_ate, weights = ipw_estimator(T_train, Y_train, _ps_values, \n",
    "                                           stabilized=stabilized, trimming=trimming)\n",
    "\n",
    "            # Save result\n",
    "            ipw_results.append({\n",
    "                'PS Method': _ps_method,\n",
    "                'Stabilized': stabilized,\n",
    "                'Trimming': trimming,\n",
    "                'ATE': ipw_ate,\n",
    "                'Bias': ipw_ate - true_ate,\n",
    "                'Abs Bias': abs(ipw_ate - true_ate),\n",
    "                'Max Weight': np.max(weights),\n",
    "                'Weight SD': np.std(weights)\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "ipw_results_df = pd.DataFrame(ipw_results)\n",
    "print(\"\\nIPW Estimation Results:\")\n",
    "print(ipw_results_df.sort_values('Abs Bias').head())\n",
    "\n",
    "# Find best IPW method\n",
    "best_ipw_idx = ipw_results_df['Abs Bias'].idxmin()\n",
    "best_ipw = ipw_results_df.loc[best_ipw_idx]\n",
    "best_ipw_method = f'IPW ({best_ipw[\"PS Method\"]}, stabilized={best_ipw[\"Stabilized\"]}, trimming={best_ipw[\"Trimming\"]})'    \n",
    "\n",
    "# Next cells will implement matching and stratification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urSm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create a figure for propensity score distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot 1: Logistic regression propensity score distributions\n",
    "    sns.histplot(\n",
    "        data=ps_df, x='ps_logistic', hue='treatment', \n",
    "        bins=30, element=\"step\", common_norm=False,\n",
    "        ax=axes[0], alpha=0.7\n",
    "    )\n",
    "    axes[0].set_title('Propensity Score Distribution (Logistic Regression)')\n",
    "    axes[0].set_xlabel('Propensity Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    # Plot 2: Random forest propensity score distributions\n",
    "    sns.histplot(\n",
    "        data=ps_df, x='ps_rf', hue='treatment', \n",
    "        bins=30, element=\"step\", common_norm=False,\n",
    "        ax=axes[1], alpha=0.7\n",
    "    )\n",
    "    axes[1].set_title('Propensity Score Distribution (Random Forest)')\n",
    "    axes[1].set_xlabel('Propensity Score')\n",
    "    axes[1].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Return the interactive plot\n",
    "    plot = mo.mpl.interactive(fig)\n",
    "    header = mo.md(\"#### Propensity Score Distributions\")\n",
    "    mo.output.replace(mo.vstack([header, plot]))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxvo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Check for overlap/positivity assumption\n",
    "    # Calculate min and max propensity scores for treated and control groups\n",
    "    ps_stats = pd.DataFrame({\n",
    "        'Model': ['Logistic Regression', 'Random Forest'],\n",
    "        'Min (Treated)': [propensity_scores[T_train == 1].min(), rf_propensity_scores[T_train == 1].min()],\n",
    "        'Max (Treated)': [propensity_scores[T_train == 1].max(), rf_propensity_scores[T_train == 1].max()],\n",
    "        'Min (Control)': [propensity_scores[T_train == 0].min(), rf_propensity_scores[T_train == 0].min()],\n",
    "        'Max (Control)': [propensity_scores[T_train == 0].max(), rf_propensity_scores[T_train == 0].max()]\n",
    "    })\n",
    "\n",
    "    # Calculate common support region\n",
    "    ps_stats['Common Support Min'] = ps_stats[['Min (Treated)', 'Min (Control)']].max(axis=1)\n",
    "    ps_stats['Common Support Max'] = ps_stats[['Max (Treated)', 'Max (Control)']].min(axis=1)\n",
    "\n",
    "    # Calculate percentage of units in common support\n",
    "    in_support_logistic = ((propensity_scores >= ps_stats.loc[0, 'Common Support Min']) & \n",
    "                          (propensity_scores <= ps_stats.loc[0, 'Common Support Max'])).mean() * 100\n",
    "    in_support_rf = ((rf_propensity_scores >= ps_stats.loc[1, 'Common Support Min']) & \n",
    "                    (rf_propensity_scores <= ps_stats.loc[1, 'Common Support Max'])).mean() * 100\n",
    "\n",
    "    ps_stats['Units in Common Support (%)'] = [in_support_logistic, in_support_rf]\n",
    "\n",
    "    # Count extreme propensity scores (< 0.1 or > 0.9)\n",
    "    extreme_ps_logistic = ((propensity_scores < 0.1) | (propensity_scores > 0.9)).mean() * 100\n",
    "    extreme_ps_rf = ((rf_propensity_scores < 0.1) | (rf_propensity_scores > 0.9)).mean() * 100\n",
    "\n",
    "    ps_stats['Extreme PS (%)'] = [extreme_ps_logistic, extreme_ps_rf]\n",
    "\n",
    "    # Create markdown output\n",
    "    header = mo.md(\"#### Overlap and Common Support Analysis\")\n",
    "\n",
    "    explanation = mo.md(\"\"\"\n",
    "    To satisfy the positivity assumption for causal inference, we need sufficient overlap in propensity scores between treated and control groups. A good overlap indicates that units with similar characteristics have a chance of being in either treatment group.\n",
    "\n",
    "    The **common support region** is the range of propensity scores where both treated and control units exist. Ideally, we want most units to fall within this region. Units outside this region may be problematic for causal inference.\n",
    "\n",
    "    **Extreme propensity scores** (close to 0 or 1) indicate units that are very likely to be in one group only, which can cause issues for some causal inference methods.\n",
    "    \"\"\")\n",
    "\n",
    "    table = mo.ui.table(ps_stats.round(4))\n",
    "\n",
    "    # Assessment of positivity assumption\n",
    "    assessment = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        **Assessment of Positivity Assumption:**  \n",
    "\n",
    "        - For the logistic regression model, {:.1f}% of units are within the common support region.  \n",
    "        - The random forest model shows {:.1f}% of units in common support.  \n",
    "        - Extreme propensity scores affect {:.1f}% (logistic) and {:.1f}% (random forest) of units.  \n",
    "\n",
    "        The {} model provides better overlap. Overall, the positivity assumption appears to be {} satisfied, which {} for reliable causal inference using propensity score methods.\n",
    "        \"\"\".format(\n",
    "            in_support_logistic, \n",
    "            in_support_rf,\n",
    "            extreme_ps_logistic,\n",
    "            extreme_ps_rf,\n",
    "            \"logistic regression\" if in_support_logistic > in_support_rf else \"random forest\",\n",
    "            \"reasonably well\" if max(in_support_logistic, in_support_rf) > 80 else \"partially\",\n",
    "            \"is promising\" if max(in_support_logistic, in_support_rf) > 80 else \"raises some concerns\"\n",
    "        )),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    mo.output.replace(mo.vstack([header, explanation, table, assessment]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mWxS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Create a figure for ROC curves\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Calculate ROC curves\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(T_train, propensity_scores)\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(T_train, rf_propensity_scores)\n",
    "\n",
    "    # Calculate AUC scores\n",
    "    auc_lr = roc_auc_score(T_train, propensity_scores)\n",
    "    auc_rf = roc_auc_score(T_train, rf_propensity_scores)\n",
    "\n",
    "    # Plot ROC curves\n",
    "    ax.plot(fpr_lr, tpr_lr, lw=2, label=f'Logistic Regression (AUC = {auc_lr:.3f})')\n",
    "    ax.plot(fpr_rf, tpr_rf, lw=2, label=f'Random Forest (AUC = {auc_rf:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curves for Propensity Score Models')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    # Calculate standardized mean differences for covariates\n",
    "    def calc_smd(var, treatment):\n",
    "        \"\"\"Calculate standardized mean difference for a variable\"\"\"\n",
    "        treated_mean = var[treatment == 1].mean()\n",
    "        control_mean = var[treatment == 0].mean()\n",
    "        treated_var = var[treatment == 1].var()\n",
    "        control_var = var[treatment == 0].var()\n",
    "        pooled_std = np.sqrt((treated_var + control_var) / 2)\n",
    "        # Handle zero standard deviation\n",
    "        if pooled_std == 0:\n",
    "            return 0\n",
    "        return (treated_mean - control_mean) / pooled_std\n",
    "\n",
    "    # Calculate SMD for each variable\n",
    "    smd_values = []\n",
    "    for col in X_train_scaled.columns:\n",
    "        smd = calc_smd(X_train_scaled[col], T_train)\n",
    "        smd_values.append({'Variable': col, 'SMD': smd})\n",
    "\n",
    "    smd_df = pd.DataFrame(smd_values)\n",
    "\n",
    "    # Sort by absolute SMD\n",
    "    smd_df['Abs_SMD'] = smd_df['SMD'].abs()\n",
    "    smd_df = smd_df.sort_values('Abs_SMD', ascending=False)\n",
    "\n",
    "    # Create two-part layout\n",
    "    roc_plot = mo.mpl.interactive(fig)\n",
    "    roc_header = mo.md(\"#### Propensity Score Model Evaluation\")\n",
    "    roc_explanation = mo.md(\"\"\"\n",
    "    The ROC curves and AUC scores show how well our propensity score models discriminate between treated and control units. Higher AUC indicates better discrimination. \n",
    "\n",
    "    The Random Forest model typically achieves higher AUC, but this doesn't necessarily make it better for propensity score estimation. In fact, for propensity score analysis, we often prefer models that achieve good covariate balance rather than maximizing predictive performance.\n",
    "    \"\"\")\n",
    "\n",
    "    # Show balance table for top 10 most imbalanced covariates\n",
    "    balance_header = mo.md(\"#### Covariate Balance Assessment\")\n",
    "    balance_explanation = mo.md(\"\"\"\n",
    "    The table below shows the standardized mean differences (SMD) for the most imbalanced covariates. SMD measures the difference in means between treated and control groups in standard deviation units.\n",
    "\n",
    "    - **SMD > 0.1**: Indicates meaningful imbalance\n",
    "    - **SMD > 0.25**: Indicates substantial imbalance\n",
    "\n",
    "    Effective propensity score methods should reduce these imbalances when we condition on the propensity score.\n",
    "    \"\"\")\n",
    "\n",
    "    balance_table = mo.ui.table(smd_df.head(10).round(4))\n",
    "\n",
    "    # Combine all elements\n",
    "    mo.output.replace(mo.vstack([\n",
    "        roc_header, \n",
    "        roc_plot,\n",
    "        roc_explanation,\n",
    "        balance_header,\n",
    "        balance_explanation,\n",
    "        balance_table\n",
    "    ]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXej",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# 2. Implement Propensity Score Matching\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def ps_matching(T, Y, ps, method='nearest', k=1, caliper=None):\n",
    "    \"\"\"Calculate ATE using propensity score matching\"\"\"\n",
    "    # Create a DataFrame with all necessary info\n",
    "    data = pd.DataFrame({\n",
    "        'treatment': T.values,\n",
    "        'outcome': Y.values,\n",
    "        'ps': ps\n",
    "    })\n",
    "\n",
    "    # Separate treated and control\n",
    "    treated = data[data['treatment'] == 1]\n",
    "    control = data[data['treatment'] == 0]\n",
    "\n",
    "    # Reshape propensity scores for NearestNeighbors\n",
    "    treated_ps = treated['ps'].values.reshape(-1, 1)\n",
    "    control_ps = control['ps'].values.reshape(-1, 1)\n",
    "\n",
    "    # Nearest neighbor matching\n",
    "    nn = NearestNeighbors(n_neighbors=k)\n",
    "    nn.fit(control_ps)\n",
    "    distances, indices = nn.kneighbors(treated_ps)\n",
    "\n",
    "    # For each treated unit, find its matches\n",
    "    matched_pairs = []\n",
    "\n",
    "    for i, treated_idx in enumerate(treated.index):\n",
    "        for j in range(k):\n",
    "            control_idx = control.index[indices[i, j]]\n",
    "            dist = distances[i, j]\n",
    "\n",
    "            # Apply caliper if specified\n",
    "            if caliper is None or dist < caliper * np.std(data['ps']):\n",
    "                matched_pairs.append({\n",
    "                    'treated_ps': treated.loc[treated_idx, 'ps'],\n",
    "                    'control_ps': control.loc[control_idx, 'ps'],\n",
    "                    'treated_outcome': treated.loc[treated_idx, 'outcome'],\n",
    "                    'control_outcome': control.loc[control_idx, 'outcome'],\n",
    "                    'ps_diff': abs(treated.loc[treated_idx, 'ps'] - control.loc[control_idx, 'ps'])\n",
    "                })\n",
    "\n",
    "    # Create dataframe of matched pairs\n",
    "    if len(matched_pairs) > 0:\n",
    "        matched_df = pd.DataFrame(matched_pairs)\n",
    "\n",
    "        # Calculate treatment effect\n",
    "        ate = (matched_df['treated_outcome'] - matched_df['control_outcome']).mean()\n",
    "\n",
    "        return ate, matched_df\n",
    "    else:\n",
    "        print(\"No matches found with current settings\")\n",
    "        return np.nan, None\n",
    "\n",
    "# Apply matching with different settings\n",
    "matching_results = []\n",
    "\n",
    "for _ps_method, _ps_values in [('Logistic', propensity_scores), ('RF', rf_propensity_scores)]:\n",
    "    for k in [1, 5]:\n",
    "        for caliper in [None, 0.2]:\n",
    "            # Skip multiple neighbors with no caliper\n",
    "            if k > 1 and caliper is None:\n",
    "                continue\n",
    "\n",
    "            # Calculate matching estimate\n",
    "            psm_ate, matched_data = ps_matching(T_train, Y_train, _ps_values, \n",
    "                                              method='nearest', k=k, caliper=caliper)\n",
    "\n",
    "            if not np.isnan(psm_ate) and matched_data is not None:\n",
    "                # Save result\n",
    "                matching_results.append({\n",
    "                    'PS Method': _ps_method,\n",
    "                    'k': k,\n",
    "                    'Caliper': caliper,\n",
    "                    'ATE': psm_ate,\n",
    "                    'Bias': psm_ate - true_ate,\n",
    "                    'Abs Bias': abs(psm_ate - true_ate),\n",
    "                    'Matches': len(matched_data),\n",
    "                    'Matched Data': matched_data\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "matching_results_df = pd.DataFrame([\n",
    "    {k: v for k, v in result.items() if k != 'Matched Data'} for result in matching_results\n",
    "])\n",
    "\n",
    "print(\"\\nPropensity Score Matching Results:\")\n",
    "print(matching_results_df.sort_values('Abs Bias').head())\n",
    "\n",
    "# Find best matching method\n",
    "if not matching_results_df.empty:\n",
    "    best_match_idx = matching_results_df['Abs Bias'].idxmin()\n",
    "    best_match = matching_results_df.loc[best_match_idx]\n",
    "    best_match_method = f\"Matching ({best_match['PS Method']}, k={best_match['k']}, caliper={best_match['Caliper']})\"\n",
    "\n",
    "    # Get matched data for visualization\n",
    "    best_matched_data = matching_results[best_match_idx]['Matched Data']\n",
    "else:\n",
    "    best_match = None\n",
    "    best_match_method = None\n",
    "    best_matched_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJmg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# 3. Implement Propensity Score Stratification\n",
    "\n",
    "def ps_stratification(T, Y, ps, n_strata=5):\n",
    "    \"\"\"Calculate ATE using propensity score stratification\"\"\"\n",
    "    # Create a DataFrame with all necessary variables\n",
    "    data = pd.DataFrame({\n",
    "        'treatment': T.values,\n",
    "        'outcome': Y.values,\n",
    "        'ps': ps\n",
    "    })\n",
    "\n",
    "    # Create strata based on propensity scores\n",
    "    data['stratum'] = pd.qcut(data['ps'], n_strata, labels=False)\n",
    "\n",
    "    # Calculate treatment effect within each stratum\n",
    "    stratum_effects = []\n",
    "    stratum_sizes = []\n",
    "    stratum_treated_counts = []\n",
    "    stratum_control_counts = []\n",
    "\n",
    "    for stratum in range(n_strata):\n",
    "        stratum_data = data[data['stratum'] == stratum]\n",
    "\n",
    "        # Check if both treated and control units exist in this stratum\n",
    "        treated_count = (stratum_data['treatment'] == 1).sum()\n",
    "        control_count = (stratum_data['treatment'] == 0).sum()\n",
    "\n",
    "        if treated_count > 0 and control_count > 0:\n",
    "            # Calculate treatment effect\n",
    "            treated_mean = stratum_data.loc[stratum_data['treatment'] == 1, 'outcome'].mean()\n",
    "            control_mean = stratum_data.loc[stratum_data['treatment'] == 0, 'outcome'].mean()\n",
    "            effect = treated_mean - control_mean\n",
    "\n",
    "            # Save effect and size\n",
    "            stratum_effects.append(effect)\n",
    "            stratum_sizes.append(len(stratum_data))\n",
    "            stratum_treated_counts.append(treated_count)\n",
    "            stratum_control_counts.append(control_count)\n",
    "        else:\n",
    "            print(f\"Stratum {stratum} does not have both treated and control units.\")\n",
    "\n",
    "    # Calculate weighted average of stratum-specific effects\n",
    "    if len(stratum_effects) > 0:\n",
    "        weights = np.array(stratum_sizes) / sum(stratum_sizes)\n",
    "        ate = sum(weights * np.array(stratum_effects))\n",
    "        return ate, stratum_effects, stratum_sizes, stratum_treated_counts, stratum_control_counts\n",
    "    else:\n",
    "        return np.nan, [], [], [], []\n",
    "\n",
    "# Apply stratification with different propensity score models and strata numbers\n",
    "strat_results = []\n",
    "\n",
    "for _ps_method, _ps_values in [('Logistic', propensity_scores), ('RF', rf_propensity_scores)]:\n",
    "    for n_strata in [5, 10]:\n",
    "        # Calculate stratification estimate\n",
    "        strat_ate, stratum_effects, stratum_sizes, treated_counts, control_counts = \\\n",
    "            ps_stratification(T_train, Y_train, _ps_values, n_strata)\n",
    "\n",
    "        if not np.isnan(strat_ate):\n",
    "            # Save result\n",
    "            strat_results.append({\n",
    "                'PS Method': _ps_method,\n",
    "                'n_strata': n_strata,\n",
    "                'ATE': strat_ate,\n",
    "                'Bias': strat_ate - true_ate,\n",
    "                'Abs Bias': abs(strat_ate - true_ate),\n",
    "                'Stratum Effects': stratum_effects,\n",
    "                'Stratum Sizes': stratum_sizes,\n",
    "                'Treated Counts': treated_counts,\n",
    "                'Control Counts': control_counts\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "strat_results_df = pd.DataFrame([\n",
    "    {k: v for k, v in result.items() if k not in ['Stratum Effects', 'Stratum Sizes', \n",
    "                                               'Treated Counts', 'Control Counts']} \n",
    "    for result in strat_results\n",
    "])\n",
    "\n",
    "print(\"\\nPropensity Score Stratification Results:\")\n",
    "print(strat_results_df.sort_values('Abs Bias'))\n",
    "\n",
    "# Find best stratification method\n",
    "if not strat_results_df.empty:\n",
    "    best_strat_idx = strat_results_df['Abs Bias'].idxmin()\n",
    "    best_strat = strat_results_df.loc[best_strat_idx]\n",
    "    best_strat_method = f\"Stratification ({best_strat['PS Method']}, n_strata={best_strat['n_strata']})\"\n",
    "\n",
    "    # Extract details for visualization\n",
    "    best_strat_effects = strat_results[best_strat_idx]['Stratum Effects']\n",
    "    best_strat_sizes = strat_results[best_strat_idx]['Stratum Sizes'] \n",
    "else:\n",
    "    best_strat = None\n",
    "    best_strat_method = None\n",
    "    best_strat_effects = None\n",
    "    best_strat_sizes = None\n",
    "\n",
    "# Create strata effects visualization\n",
    "if best_strat_effects is not None:\n",
    "    strat_fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    strata_indices = list(range(len(best_strat_effects)))\n",
    "    ax.bar(strata_indices, best_strat_effects, alpha=0.7)\n",
    "    ax.axhline(y=best_strat['ATE'], color='red', linestyle='--', \n",
    "              label=f'Overall ATE: {best_strat[\"ATE\"]:.4f}')\n",
    "    ax.axhline(y=true_ate, color='green', linestyle=':', \n",
    "              label=f'True ATE: {true_ate:.4f}')\n",
    "    ax.set_title('Treatment Effects by Propensity Score Stratum')\n",
    "    ax.set_xlabel('Propensity Score Stratum (low to high)')\n",
    "    ax.set_ylabel('Stratum-Specific ATE')\n",
    "    ax.set_xticks(strata_indices)\n",
    "    ax.legend()\n",
    "    strat_plot = mo.mpl.interactive(strat_fig)\n",
    "else:\n",
    "    strat_plot = None\n",
    "\n",
    "# Compare all propensity score methods\n",
    "ps_methods = []\n",
    "\n",
    "# Add best methods from each category\n",
    "ps_methods.append({\n",
    "    'Method': best_ipw_method,\n",
    "    'ATE': best_ipw['ATE'],\n",
    "    'Bias': best_ipw['Bias'],\n",
    "    'Abs Bias': best_ipw['Abs Bias'],\n",
    "    'Type': 'IPW'\n",
    "})\n",
    "\n",
    "if best_match is not None:\n",
    "    ps_methods.append({\n",
    "        'Method': best_match_method,\n",
    "        'ATE': best_match['ATE'],\n",
    "        'Bias': best_match['Bias'],\n",
    "        'Abs Bias': best_match['Abs Bias'],\n",
    "        'Type': 'Matching'\n",
    "    })\n",
    "\n",
    "if best_strat is not None:\n",
    "    ps_methods.append({\n",
    "        'Method': best_strat_method,\n",
    "        'ATE': best_strat['ATE'],\n",
    "        'Bias': best_strat['Bias'],\n",
    "        'Abs Bias': best_strat['Abs Bias'],\n",
    "        'Type': 'Stratification'\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and sort by absolute bias\n",
    "ps_methods_df = pd.DataFrame(ps_methods)\n",
    "ps_methods_df = ps_methods_df.sort_values('Abs Bias')\n",
    "\n",
    "print(\"\\nComparison of Best Propensity Score Methods:\")\n",
    "print(ps_methods_df)\n",
    "\n",
    "# Create comparison visualization\n",
    "comp_fig, comp_ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars\n",
    "colors = {'IPW': 'skyblue', 'Matching': 'lightgreen', 'Stratification': 'salmon'}\n",
    "for i, (idx, row) in enumerate(ps_methods_df.iterrows()):\n",
    "    comp_ax.barh(i, row['ATE'], color=colors[row['Type']], label=row['Type'] if i == 0 else \"\")\n",
    "\n",
    "# Add method names and reference line\n",
    "comp_ax.set_yticks(range(len(ps_methods_df)))\n",
    "comp_ax.set_yticklabels(ps_methods_df['Method'])\n",
    "comp_ax.axvline(x=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "\n",
    "# Add legend and labels\n",
    "handles, labels = comp_ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "comp_ax.legend(by_label.values(), by_label.keys(), loc='lower right')\n",
    "\n",
    "comp_ax.set_title('Comparison of Best Propensity Score Methods')\n",
    "comp_ax.set_xlabel('ATE Estimate')\n",
    "comp_ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create interactive plot for marimo\n",
    "comparison_plot = mo.mpl.interactive(comp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UmEG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Display results of Propensity Score Methods\n",
    "def _():\n",
    "    # Create header for the section\n",
    "    header = mo.md(\"#### Comparison of Propensity Score Methods\")\n",
    "\n",
    "    # Create explanation text\n",
    "    explanation = mo.md(\"\"\"\n",
    "    We've implemented and compared three propensity score-based causal inference methods:\n",
    "\n",
    "    1. **Inverse Probability Weighting (IPW)**: Weights observations inversely to their probability of receiving the treatment to create balance.\n",
    "    2. **Propensity Score Matching**: Pairs treated units with similar control units based on propensity scores.\n",
    "    3. **Propensity Score Stratification**: Divides the sample into strata based on propensity scores and calculates treatment effects within each stratum.\n",
    "\n",
    "    Each method has different strengths and weaknesses. The comparison below shows their performance in estimating the Average Treatment Effect (ATE).\n",
    "    \"\"\")\n",
    "\n",
    "    # Create results summary\n",
    "    best_method_idx = ps_methods_df['Abs Bias'].idxmin()\n",
    "    best_method = ps_methods_df.loc[best_method_idx]\n",
    "\n",
    "    summary = mo.callout(\n",
    "        mo.md(f\"\"\"\n",
    "        **Best Method: {best_method['Method']}**\n",
    "\n",
    "        - Estimated ATE: {best_method['ATE']:.4f}\n",
    "        - True ATE: {true_ate:.4f}\n",
    "        - Absolute Bias: {best_method['Abs Bias']:.4f}\n",
    "\n",
    "        This analysis shows that propensity score methods can effectively reduce bias in causal estimates from observational data.\n",
    "        \"\"\"),\n",
    "        kind=\"success\"\n",
    "    )\n",
    "\n",
    "    # Create table with results\n",
    "    results_table = mo.ui.table(ps_methods_df.reset_index(drop=True))\n",
    "\n",
    "    # Display comparison plot\n",
    "    plot_header = mo.md(\"#### Visual Comparison of Methods\")\n",
    "\n",
    "    # Display stratification plot if available\n",
    "    if strat_plot is not None:\n",
    "        strat_header = mo.md(\"#### Treatment Effects by Propensity Score Stratum\")\n",
    "        strat_explanation = mo.md(\"\"\"\n",
    "        This plot shows how treatment effects vary across different propensity score strata. \n",
    "        Heterogeneity in these effects may indicate effect modification by variables related to treatment assignment.\n",
    "        \"\"\")\n",
    "        strat_section = mo.vstack([strat_header, strat_explanation, strat_plot])\n",
    "    else:\n",
    "        strat_section = mo.md(\"\")\n",
    "\n",
    "    # Combine all components\n",
    "    components = [header, explanation, summary, results_table, plot_header, comparison_plot]\n",
    "    if strat_plot is not None:\n",
    "        components.append(strat_section)\n",
    "\n",
    "    # Display all components\n",
    "    mo.output.replace(mo.vstack(components))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vEBW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"### 6.3 Advanced Machine Learning Methods {#ml-methods}\n",
    "\n",
    "    > ðŸš€ **Step 3**: Leverage machine learning techniques for improved causal inference\n",
    "\n",
    "    Finally, we'll explore advanced methods that combine machine learning with causal inference principles to estimate treatment effects more accurately. These methods can capture complex non-linear relationships and interactions between variables without requiring strong parametric assumptions.\n",
    "    \"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kLmu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"#### 6.3.1 Meta-Learners for Causal Inference {#meta-learners}\n",
    "\n",
    "    Meta-learners are a class of methods that use machine learning algorithms to estimate causal effects by combining multiple prediction models in different ways. Unlike traditional methods, meta-learners can capture complex, non-linear relationships between variables without requiring explicit parametric assumptions.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IpqN",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "s_learner_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### S-Learner (Single Model)\n",
    "\n",
    "    The S-Learner (Single model) uses a single machine learning model with the treatment indicator included as a regular feature:\n",
    "\n",
    "    1. **Train a model** to predict outcome using both covariates and treatment: \n",
    "       $$\\hat{\\mu}(x, t) = E[Y | X=x, T=t]$$\n",
    "\n",
    "    2. **Estimate treatment effects** by taking the difference in predictions for treated vs. untreated:\n",
    "       $$\\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)$$\n",
    "\n",
    "    **Advantages**: Simple to implement, requires only one model\n",
    "\n",
    "    **Limitations**: May underestimate treatment effects if treatment assignment is highly imbalanced\n",
    "    \"\"\"),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "t_learner_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### T-Learner (Two Models)\n",
    "\n",
    "    The T-Learner (Two models) fits separate models for the treated and control groups:\n",
    "\n",
    "    1. **Train two separate models**:\n",
    "       - Control model: $$\\hat{\\mu}_0(x) = E[Y | X=x, T=0]$$\n",
    "       - Treatment model: $$\\hat{\\mu}_1(x) = E[Y | X=x, T=1]$$\n",
    "\n",
    "    2. **Estimate treatment effects** by taking the difference in predictions:\n",
    "       $$\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)$$\n",
    "\n",
    "    **Advantages**: Can capture heterogeneous response surfaces, doesn't impose shared structure\n",
    "\n",
    "    **Limitations**: May suffer from high variance in regions with few samples from either group\n",
    "    \"\"\"),\n",
    "    kind=\"warn\"\n",
    ")\n",
    "\n",
    "x_learner_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### X-Learner\n",
    "\n",
    "    The X-Learner extends the T-Learner with a more sophisticated approach:\n",
    "\n",
    "    1. **Train response surface models** (same as T-Learner):\n",
    "       - Control model: $$\\hat{\\mu}_0(x) = E[Y | X=x, T=0]$$\n",
    "       - Treatment model: $$\\hat{\\mu}_1(x) = E[Y | X=x, T=1]$$\n",
    "\n",
    "    2. **Impute individual treatment effects** for each unit:\n",
    "       - For treated units: $$D_i^1 = Y_i(1) - \\hat{\\mu}_0(X_i)$$\n",
    "       - For control units: $$D_i^0 = \\hat{\\mu}_1(X_i) - Y_i(0)$$\n",
    "\n",
    "    3. **Train two treatment effect models**:\n",
    "       - Using treated units: $$\\hat{\\tau}_1(x) = E[D_i^1 | X_i=x]$$\n",
    "       - Using control units: $$\\hat{\\tau}_0(x) = E[D_i^0 | X_i=x]$$\n",
    "\n",
    "    4. **Combine the two estimates** using a weighting function g(x):\n",
    "       $$\\hat{\\tau}(x) = g(x)\\hat{\\tau}_0(x) + (1-g(x))\\hat{\\tau}_1(x)$$\n",
    "       where g(x) can be the propensity score.\n",
    "\n",
    "    **Advantages**: Performs well with heterogeneous treatment effects and imbalanced treatment groups\n",
    "\n",
    "    **Limitations**: More complex, requires estimating propensity scores\n",
    "    \"\"\"),\n",
    "    kind=\"success\"\n",
    ")\n",
    "\n",
    "# Stack all descriptions\n",
    "mo.vstack([\n",
    "    mo.md(\"Meta-learners use machine learning algorithms to estimate causal effects. Here are the three main types:\"),\n",
    "    s_learner_desc,\n",
    "    t_learner_desc,\n",
    "    x_learner_desc\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxZZ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Import required ML libraries\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # S-Learner implementation\n",
    "    def s_learner(X, T, Y, X_test=None, model=None):\n",
    "        \"\"\"\n",
    "        Estimate treatment effects using S-Learner\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame of covariates\n",
    "        T : Series of treatment assignments\n",
    "        Y : Series of outcomes\n",
    "        X_test : DataFrame of test covariates or None\n",
    "        model : Fitted sklearn model or None\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ate : Estimated average treatment effect\n",
    "        cate : Estimated conditional average treatment effects\n",
    "        model : Fitted model\n",
    "        \"\"\"\n",
    "        # Default model\n",
    "        if model is None:\n",
    "            model = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "        # Create combined dataset\n",
    "        X_combined = X.copy()\n",
    "        X_combined['treatment'] = T\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_combined, Y)\n",
    "\n",
    "        # Predict on test set if provided, otherwise on training set\n",
    "        if X_test is not None:\n",
    "            X_pred = X_test\n",
    "        else:\n",
    "            X_pred = X\n",
    "\n",
    "        # Create counterfactual datasets\n",
    "        X_pred_1 = X_pred.copy()\n",
    "        X_pred_1['treatment'] = 1\n",
    "\n",
    "        X_pred_0 = X_pred.copy()\n",
    "        X_pred_0['treatment'] = 0\n",
    "\n",
    "        # Predict potential outcomes\n",
    "        y_pred_1 = model.predict(X_pred_1)\n",
    "        y_pred_0 = model.predict(X_pred_0)\n",
    "\n",
    "        # Calculate treatment effects\n",
    "        cate = y_pred_1 - y_pred_0\n",
    "        ate = cate.mean()\n",
    "\n",
    "        return ate, cate, model\n",
    "\n",
    "    # T-Learner implementation\n",
    "    def t_learner(X, T, Y, X_test=None, model_t=None, model_c=None):\n",
    "        \"\"\"\n",
    "        Estimate treatment effects using T-Learner\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame of covariates\n",
    "        T : Series of treatment assignments\n",
    "        Y : Series of outcomes\n",
    "        X_test : DataFrame of test covariates or None\n",
    "        model_t : Sklearn model for treated group or None\n",
    "        model_c : Sklearn model for control group or None\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ate : Estimated average treatment effect\n",
    "        cate : Estimated conditional average treatment effects\n",
    "        models : Tuple of (treatment_model, control_model)\n",
    "        \"\"\"\n",
    "        # Default models\n",
    "        if model_t is None:\n",
    "            model_t = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        if model_c is None:\n",
    "            model_c = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)\n",
    "\n",
    "        # Split data into treated and control groups\n",
    "        X_t = X[T == 1]\n",
    "        Y_t = Y[T == 1]\n",
    "        X_c = X[T == 0]\n",
    "        Y_c = Y[T == 0]\n",
    "\n",
    "        # Fit models\n",
    "        model_t.fit(X_t, Y_t)\n",
    "        model_c.fit(X_c, Y_c)\n",
    "\n",
    "        # Predict on test set if provided, otherwise on training set\n",
    "        if X_test is not None:\n",
    "            X_pred = X_test\n",
    "        else:\n",
    "            X_pred = X\n",
    "\n",
    "        # Predict potential outcomes\n",
    "        y_pred_1 = model_t.predict(X_pred)\n",
    "        y_pred_0 = model_c.predict(X_pred)\n",
    "\n",
    "        # Calculate treatment effects\n",
    "        cate = y_pred_1 - y_pred_0\n",
    "        ate = cate.mean()\n",
    "\n",
    "        return ate, cate, (model_t, model_c)\n",
    "\n",
    "    # X-Learner implementation\n",
    "    def x_learner(X, T, Y, X_test=None, models_t=None, models_c=None, propensity_model=None):\n",
    "        \"\"\"\n",
    "        Estimate treatment effects using X-Learner\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame of covariates\n",
    "        T : Series of treatment assignments\n",
    "        Y : Series of outcomes\n",
    "        X_test : DataFrame of test covariates or None\n",
    "        models_t : List of two Sklearn models for treated group or None\n",
    "        models_c : List of two Sklearn models for control group or None\n",
    "        propensity_model : Sklearn classifier for propensity scores or None\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ate : Estimated average treatment effect\n",
    "        cate : Estimated conditional average treatment effects\n",
    "        models : Tuple of (models_t, models_c, propensity_model)\n",
    "        \"\"\"\n",
    "        # Default models\n",
    "        if models_t is None:\n",
    "            models_t = [\n",
    "                RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "                RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)\n",
    "            ]\n",
    "        if models_c is None:\n",
    "            models_c = [\n",
    "                RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=44),\n",
    "                RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=45)\n",
    "            ]\n",
    "        if propensity_model is None:\n",
    "            propensity_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        # Split data into treated and control groups\n",
    "        X_t = X[T == 1]\n",
    "        Y_t = Y[T == 1]\n",
    "        X_c = X[T == 0]\n",
    "        Y_c = Y[T == 0]\n",
    "\n",
    "        # Step 1: Estimate the response surfaces\n",
    "        model_t1, model_c1 = models_t[0], models_c[0]\n",
    "        model_t1.fit(X_t, Y_t)\n",
    "        model_c1.fit(X_c, Y_c)\n",
    "\n",
    "        # Predict responses for all units\n",
    "        mu_t = model_t1.predict(X)\n",
    "        mu_c = model_c1.predict(X)\n",
    "\n",
    "        # Step 2: Compute the imputed treatment effects\n",
    "        D_t = Y_t.values - model_c1.predict(X_t)  # Imputed effect for treated units\n",
    "        D_c = model_t1.predict(X_c) - Y_c.values  # Imputed effect for control units\n",
    "\n",
    "        # Step 3: Estimate the CATE functions\n",
    "        model_t2, model_c2 = models_t[1], models_c[1]\n",
    "        model_t2.fit(X_t, D_t)\n",
    "        model_c2.fit(X_c, D_c)\n",
    "\n",
    "        # Step 4: Combine the CATE functions using propensity scores\n",
    "        propensity_model.fit(X, T)\n",
    "        g = propensity_model.predict_proba(X)[:, 1]  # Propensity scores\n",
    "\n",
    "        # Predict on test set if provided, otherwise on training set\n",
    "        if X_test is not None:\n",
    "            X_pred = X_test\n",
    "            g_pred = propensity_model.predict_proba(X_pred)[:, 1]\n",
    "        else:\n",
    "            X_pred = X\n",
    "            g_pred = g\n",
    "\n",
    "        # Predict treatment effects\n",
    "        tau_t = model_t2.predict(X_pred)\n",
    "        tau_c = model_c2.predict(X_pred)\n",
    "\n",
    "        # Weighted average of treatment effects\n",
    "        cate = g_pred * tau_c + (1 - g_pred) * tau_t\n",
    "        ate = cate.mean()\n",
    "\n",
    "        return ate, cate, (models_t, models_c, propensity_model)\n",
    "\n",
    "    # Compare meta-learners\n",
    "    np.random.seed(42)  # Set seed for reproducibility\n",
    "\n",
    "    # Initialize results list\n",
    "    meta_learner_results = []\n",
    "\n",
    "    # True ATE for comparison\n",
    "    true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "    # S-Learner with different base models\n",
    "    for model_name, model in [\n",
    "        ('Random Forest', RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)),\n",
    "        ('Gradient Boosting', GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42))\n",
    "    ]:\n",
    "        # Train S-Learner\n",
    "        s_ate, s_cate, _ = s_learner(X_train_scaled, T_train, Y_train, model=model)\n",
    "\n",
    "        # Save results\n",
    "        meta_learner_results.append({\n",
    "            'Method': f'S-Learner ({model_name})',\n",
    "            'ATE': s_ate,\n",
    "            'Bias': s_ate - true_ate,\n",
    "            'Abs Bias': abs(s_ate - true_ate)\n",
    "        })\n",
    "\n",
    "    # T-Learner with different base models\n",
    "    for model_name, model_t, model_c in [\n",
    "        ('Random Forest', \n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)),\n",
    "        ('Gradient Boosting', \n",
    "         GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "         GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=43))\n",
    "    ]:\n",
    "        # Train T-Learner\n",
    "        t_ate, t_cate, _ = t_learner(X_train_scaled, T_train, Y_train, model_t=model_t, model_c=model_c)\n",
    "\n",
    "        # Save results\n",
    "        meta_learner_results.append({\n",
    "            'Method': f'T-Learner ({model_name})',\n",
    "            'ATE': t_ate,\n",
    "            'Bias': t_ate - true_ate,\n",
    "            'Abs Bias': abs(t_ate - true_ate)\n",
    "        })\n",
    "\n",
    "    # X-Learner with different base models\n",
    "    for model_name, models_t, models_c, prop_model in [\n",
    "        ('Random Forest', \n",
    "         [RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "          RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)],\n",
    "         [RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=44),\n",
    "          RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=45)],\n",
    "         LogisticRegression(max_iter=1000)),\n",
    "        ('Gradient Boosting', \n",
    "         [GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "          GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=43)],\n",
    "         [GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=44),\n",
    "          GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=45)],\n",
    "         LogisticRegression(max_iter=1000))\n",
    "    ]:\n",
    "        # Train X-Learner\n",
    "        x_ate, x_cate, _ = x_learner(X_train_scaled, T_train, Y_train, \n",
    "                                    models_t=models_t, models_c=models_c, \n",
    "                                    propensity_model=prop_model)\n",
    "\n",
    "        # Save results\n",
    "        meta_learner_results.append({\n",
    "            'Method': f'X-Learner ({model_name})',\n",
    "            'ATE': x_ate,\n",
    "            'Bias': x_ate - true_ate,\n",
    "            'Abs Bias': abs(x_ate - true_ate)\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and sort by absolute bias\n",
    "    meta_learner_df = pd.DataFrame(meta_learner_results)\n",
    "    meta_learner_df = meta_learner_df.sort_values('Abs Bias')\n",
    "\n",
    "    # Compare meta-learners - show results\n",
    "    header = mo.md(\"#### Meta-Learner Results\")\n",
    "    table = mo.ui.table(meta_learner_df)\n",
    "\n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.barh(y=meta_learner_df['Method'], width=meta_learner_df['ATE'], color='skyblue')\n",
    "    ax.axvline(x=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "    ax.set_title('Comparison of ATE Estimates from Meta-Learners')\n",
    "    ax.set_xlabel('ATE Estimate')\n",
    "    ax.set_ylabel('Method')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create interactive plot\n",
    "    plot = mo.mpl.interactive(fig)\n",
    "\n",
    "    # Select the best meta-learner method\n",
    "    best_ml_idx = meta_learner_df['Abs Bias'].idxmin()\n",
    "    best_ml = meta_learner_df.loc[best_ml_idx]\n",
    "\n",
    "    # Create summary of best method\n",
    "    best_method_summary = mo.callout(\n",
    "        mo.md(f\"\"\"\n",
    "        **Best Meta-Learner Method:**\n",
    "\n",
    "        - **Method**: {best_ml['Method']}\n",
    "        - **ATE Estimate**: {best_ml['ATE']:.4f}\n",
    "        - **True ATE**: {true_ate:.4f}\n",
    "        - **Bias**: {best_ml['Bias']:.4f}\n",
    "\n",
    "        This analysis shows that meta-learners can provide accurate estimates of causal effects by leveraging machine learning algorithms.\n",
    "        \"\"\"),\n",
    "        kind=\"success\"\n",
    "    )\n",
    "\n",
    "    # Plot treatment effect heterogeneity\n",
    "    # Get CATE estimates from X-Learner with Random Forest\n",
    "    _, x_cate, _ = x_learner(\n",
    "        X_train_scaled, T_train, Y_train,\n",
    "        models_t=[RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "                 RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)],\n",
    "        models_c=[RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=44),\n",
    "                 RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=45)],\n",
    "        propensity_model=LogisticRegression(max_iter=1000)\n",
    "    )\n",
    "\n",
    "    # Plot CATE distribution\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "    ax2.hist(x_cate, bins=30, color='skyblue', alpha=0.7)\n",
    "    ax2.axvline(x=x_cate.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean CATE = {x_cate.mean():.4f}')\n",
    "    ax2.axvline(x=true_ate, color='green', linestyle=':', \n",
    "               label=f'True ATE = {true_ate:.4f}')\n",
    "    ax2.set_title('Distribution of Conditional Average Treatment Effects (CATE)')\n",
    "    ax2.set_xlabel('CATE')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    cate_plot = mo.mpl.interactive(fig2)\n",
    "    cate_explanation = mo.md(\"\"\"**Treatment Effect Heterogeneity**: The distribution above shows how treatment effects vary across different individuals. This variation suggests that the intervention may be more effective for some subgroups than others.\"\"\")\n",
    "\n",
    "    # Combine all elements in the output\n",
    "    mo.output.replace(mo.vstack([\n",
    "        header,\n",
    "        table,\n",
    "        plot,\n",
    "        best_method_summary,\n",
    "        mo.md(\"#### Treatment Effect Heterogeneity\"),\n",
    "        cate_plot,\n",
    "        cate_explanation\n",
    "    ]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlnW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"#### 6.3.2 Doubly Robust Methods {#doubly-robust}\n",
    "\n",
    "    Doubly robust methods combine outcome modeling and propensity score approaches to provide protection against misspecification of either model. This \"double robustness\" property makes these methods particularly attractive for causal inference in complex settings.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TTti",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create descriptions of doubly robust methods\n",
    "aipw_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Augmented Inverse Probability Weighting (AIPW)\n",
    "\n",
    "    AIPW combines outcome regression and IPW by using both models to create a doubly robust estimator:\n",
    "\n",
    "    The AIPW estimator can be written as:\n",
    "\n",
    "    \\[ \\hat{\\tau}_{AIPW} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{T_i(Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}_0(X_i))}{1-\\hat{e}(X_i)} \\right) \\]\n",
    "\n",
    "    where:\n",
    "    - \\(\\hat{\\mu}_1(X_i)\\) and \\(\\hat{\\mu}_0(X_i)\\) are outcome models for treated and control\n",
    "    - \\(\\hat{e}(X_i)\\) is the propensity score model\n",
    "\n",
    "    **Key property**: Consistent if *either* the outcome model *or* the propensity score model is correctly specified (not necessarily both)\n",
    "\n",
    "    **Advantages**: More robust to model misspecification, often lower variance than IPW\n",
    "    \"\"\"),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "dml_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Double Machine Learning (DML)\n",
    "\n",
    "    DML addresses issues of regularization bias and overfitting in high-dimensional settings:\n",
    "\n",
    "    1. **Cross-fitting approach**:\n",
    "       - Split the data into K folds\n",
    "       - For each fold, fit models on the other K-1 folds and predict on the held-out fold\n",
    "       - This reduces the impact of overfitting\n",
    "\n",
    "    2. **Orthogonalization**:\n",
    "       - Remove the dependence between treatment and covariates\n",
    "       - Remove the dependence between outcome and covariates\n",
    "       - Study the residual relationship\n",
    "\n",
    "    DML can be implemented as:\n",
    "\n",
    "    \\[ \\hat{\\tau}_{DML} = \\frac{\\frac{1}{n}\\sum_{i=1}^n (T_i - \\hat{e}(X_i))(Y_i - \\hat{m}(X_i))}{\\frac{1}{n}\\sum_{i=1}^n (T_i - \\hat{e}(X_i))^2} \\]\n",
    "\n",
    "    where \\(\\hat{m}(X_i)\\) is a model for the outcome and \\(\\hat{e}(X_i)\\) is the propensity score model.\n",
    "\n",
    "    **Advantages**: Handles high-dimensional settings well, allows complex ML models, reduces regularization bias\n",
    "    \"\"\"),\n",
    "    kind=\"warn\"\n",
    ")\n",
    "\n",
    "# Stack all descriptions\n",
    "mo.vstack([\n",
    "    mo.md(\"Doubly robust methods offer protection against model misspecification by combining outcome modeling and propensity score approaches:\"),\n",
    "    aipw_desc,\n",
    "    dml_desc\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RKFZ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Import required libraries\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    # Implement Augmented Inverse Probability Weighting (AIPW)\n",
    "    def doubly_robust_aipw(X, T, Y, outcome_model=None, propensity_model=None):\n",
    "        \"\"\"\n",
    "        Estimate ATE using Augmented Inverse Probability Weighting (AIPW)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame of covariates\n",
    "        T : Series of treatment assignments\n",
    "        Y : Series of outcomes\n",
    "        outcome_model : sklearn regressor or None\n",
    "        propensity_model : sklearn classifier or None\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ate : Estimated average treatment effect\n",
    "        \"\"\"\n",
    "        from sklearn.base import clone\n",
    "\n",
    "        # Default models\n",
    "        if outcome_model is None:\n",
    "            outcome_model = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        if propensity_model is None:\n",
    "            propensity_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        # Split data by treatment group\n",
    "        X_t = X[T == 1]\n",
    "        Y_t = Y[T == 1]\n",
    "        X_c = X[T == 0]\n",
    "        Y_c = Y[T == 0]\n",
    "\n",
    "        # Fit outcome models for each treatment group\n",
    "        model_t = clone(outcome_model)\n",
    "        model_c = clone(outcome_model)\n",
    "        model_t.fit(X_t, Y_t)\n",
    "        model_c.fit(X_c, Y_c)\n",
    "\n",
    "        # Predict potential outcomes for all units\n",
    "        mu_1 = model_t.predict(X)\n",
    "        mu_0 = model_c.predict(X)\n",
    "\n",
    "        # Fit propensity score model\n",
    "        ps_model = clone(propensity_model)\n",
    "        ps_model.fit(X, T)\n",
    "        ps = ps_model.predict_proba(X)[:, 1]\n",
    "\n",
    "        # Handle extreme propensity scores\n",
    "        eps = 1e-12\n",
    "        ps = np.maximum(eps, np.minimum(1 - eps, ps))\n",
    "\n",
    "        # Calculate AIPW estimator\n",
    "        aipw_0 = mu_0 + (1 - T) * (Y - mu_0) / (1 - ps)\n",
    "        aipw_1 = mu_1 + T * (Y - mu_1) / ps\n",
    "\n",
    "        # Calculate ATE\n",
    "        ate = (aipw_1 - aipw_0).mean()\n",
    "\n",
    "        return ate\n",
    "\n",
    "    # Implement Double Machine Learning (DML)\n",
    "    def double_machine_learning(X, T, Y, outcome_model=None, propensity_model=None, n_splits=5):\n",
    "        \"\"\"\n",
    "        Estimate ATE using Double Machine Learning with cross-fitting\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame of covariates\n",
    "        T : Series of treatment assignments\n",
    "        Y : Series of outcomes\n",
    "        outcome_model : sklearn regressor or None\n",
    "        propensity_model : sklearn classifier or None\n",
    "        n_splits : int, number of folds for cross-fitting\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ate : Estimated average treatment effect\n",
    "        \"\"\"\n",
    "        from sklearn.base import clone\n",
    "\n",
    "        # Default models\n",
    "        if outcome_model is None:\n",
    "            outcome_model = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        if propensity_model is None:\n",
    "            propensity_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        # Initialize arrays for predictions\n",
    "        n = len(Y)\n",
    "        Y_hat = np.zeros(n)\n",
    "        T_hat = np.zeros(n)\n",
    "\n",
    "        # Set up cross-fitting\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        # Perform cross-fitting\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            # Get train and test data\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_test = X.iloc[test_idx]\n",
    "            T_train_fold = T.iloc[train_idx]\n",
    "            T_test = T.iloc[test_idx]\n",
    "            Y_train_fold = Y.iloc[train_idx]\n",
    "\n",
    "            # Fit and predict with outcome model\n",
    "            m_model = clone(outcome_model)\n",
    "            m_model.fit(X_train, Y_train_fold)\n",
    "            Y_hat[test_idx] = m_model.predict(X_test)\n",
    "\n",
    "            # Fit and predict with propensity model\n",
    "            e_model = clone(propensity_model)\n",
    "            e_model.fit(X_train, T_train_fold)\n",
    "            T_hat[test_idx] = e_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate residuals\n",
    "        Y_resid = Y - Y_hat\n",
    "        T_resid = T - T_hat\n",
    "\n",
    "        # Estimate treatment effect using DML formula\n",
    "        numerator = np.mean(T_resid * Y_resid)\n",
    "        denominator = np.mean(T_resid * T)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Denominator in DML estimator is zero\")\n",
    "\n",
    "        ate = numerator / denominator\n",
    "\n",
    "        return ate\n",
    "\n",
    "    # True ATE for reference\n",
    "    true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "    # Compare doubly robust methods\n",
    "    dr_results = []\n",
    "\n",
    "    # Test different model combinations for AIPW\n",
    "    for method_name, outcome_model, propensity_model in [\n",
    "        ('AIPW (RF, Logistic)', \n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "         LogisticRegression(max_iter=1000)),\n",
    "        ('AIPW (GB, Logistic)', \n",
    "         GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "         LogisticRegression(max_iter=1000)),\n",
    "        ('AIPW (RF, RF)', \n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "         RandomForestClassifier(n_estimators=100, min_samples_leaf=5, random_state=42))\n",
    "    ]:\n",
    "        # Estimate ATE with AIPW\n",
    "        try:\n",
    "            aipw_ate = doubly_robust_aipw(X_train_scaled, T_train, Y_train, \n",
    "                                       outcome_model=outcome_model,\n",
    "                                       propensity_model=propensity_model)\n",
    "\n",
    "            # Store results\n",
    "            dr_results.append({\n",
    "                'Method': method_name,\n",
    "                'ATE': aipw_ate,\n",
    "                'Bias': aipw_ate - true_ate,\n",
    "                'Abs Bias': abs(aipw_ate - true_ate),\n",
    "                'Type': 'AIPW'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name}: {e}\")\n",
    "\n",
    "    # Test different model combinations for DML\n",
    "    for method_name, outcome_model, propensity_model in [\n",
    "        ('DML (RF, Logistic)', \n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "         LogisticRegression(max_iter=1000)),\n",
    "        ('DML (GB, Logistic)', \n",
    "         GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "         LogisticRegression(max_iter=1000)),\n",
    "        ('DML (RF, RF)', \n",
    "         RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "         RandomForestClassifier(n_estimators=100, min_samples_leaf=5, random_state=42))\n",
    "    ]:\n",
    "        # Estimate ATE with DML\n",
    "        try:\n",
    "            dml_ate = double_machine_learning(X_train_scaled, T_train, Y_train, \n",
    "                                           outcome_model=outcome_model,\n",
    "                                           propensity_model=propensity_model)\n",
    "\n",
    "            # Store results\n",
    "            dr_results.append({\n",
    "                'Method': method_name,\n",
    "                'ATE': dml_ate,\n",
    "                'Bias': dml_ate - true_ate,\n",
    "                'Abs Bias': abs(dml_ate - true_ate),\n",
    "                'Type': 'DML'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name}: {e}\")\n",
    "\n",
    "    # Convert to DataFrame and sort by absolute bias\n",
    "    dr_results_df = pd.DataFrame(dr_results)\n",
    "    dr_results_df = dr_results_df.sort_values('Abs Bias')\n",
    "\n",
    "    # Compare doubly robust methods - show results\n",
    "    header = mo.md(\"#### Doubly Robust Method Results\")\n",
    "    table = mo.ui.table(dr_results_df)\n",
    "\n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.barh(y=dr_results_df['Method'], width=dr_results_df['ATE'], color='skyblue')\n",
    "    ax.axvline(x=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "    ax.set_title('Comparison of ATE Estimates from Doubly Robust Methods')\n",
    "    ax.set_xlabel('ATE Estimate')\n",
    "    ax.set_ylabel('Method')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create interactive plot\n",
    "    plot = mo.mpl.interactive(fig)\n",
    "\n",
    "    # Select the best doubly robust method\n",
    "    if not dr_results_df.empty:\n",
    "        best_dr_idx = dr_results_df['Abs Bias'].idxmin()\n",
    "        best_dr = dr_results_df.loc[best_dr_idx]\n",
    "\n",
    "        # Create summary of best method\n",
    "        best_method_summary = mo.callout(\n",
    "            mo.md(f\"\"\"\n",
    "            **Best Doubly Robust Method:**\n",
    "\n",
    "            - **Method**: {best_dr['Method']}\n",
    "            - **ATE Estimate**: {best_dr['ATE']:.4f}\n",
    "            - **True ATE**: {true_ate:.4f}\n",
    "            - **Bias**: {best_dr['Bias']:.4f}\n",
    "\n",
    "            Doubly robust methods provide protection against model misspecification, making them more robust for causal inference in complex settings.\n",
    "            \"\"\"),\n",
    "            kind=\"success\"\n",
    "        )\n",
    "\n",
    "        # Comparison of AIPW vs DML\n",
    "        method_comparison = mo.md(\"\"\"\n",
    "        **AIPW vs DML Comparison**:\n",
    "\n",
    "        - **AIPW** directly augments IPW with an outcome model correction term, providing a straightforward implementation of double robustness\n",
    "        - **DML** uses cross-fitting to address regularization bias, making it particularly well-suited for high-dimensional settings\n",
    "\n",
    "        Both methods leverage the strengths of outcome modeling and propensity score approaches, providing more reliable causal estimates than either approach alone.\n",
    "        \"\"\")\n",
    "\n",
    "        # Combine all elements in the output\n",
    "        mo.output.replace(mo.vstack([\n",
    "            header,\n",
    "            table,\n",
    "            plot,\n",
    "            best_method_summary,\n",
    "            method_comparison\n",
    "        ]))\n",
    "    else:\n",
    "        mo.output.replace(mo.md(\"**Error:** No valid results from doubly robust methods.\"))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaQp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"#### 6.3.3 Causal Forests {#causal-forests}\n",
    "\n",
    "    Causal forests are a powerful extension of random forests specifically designed to estimate heterogeneous treatment effects. They are particularly useful for understanding how treatment effects vary across different subgroups and for identifying important features that modify treatment effects.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IWgg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create description of causal forests\n",
    "causal_forest_desc = mo.callout(\n",
    "    mo.md(r\"\"\"\n",
    "    #### Causal Forests\n",
    "\n",
    "    Causal forests extend random forests to directly estimate heterogeneous treatment effects. The key ideas include:\n",
    "\n",
    "    1. **Honest trees**: Split the sample into a training set (used to determine splits) and an estimation set (used to estimate treatment effects within leaves)\n",
    "\n",
    "    2. **Orthogonalization**: Remove the effect of confounders by residualizing the outcome and treatment\n",
    "\n",
    "    3. **Adaptive sample splitting**: Focus on regions with high treatment effect heterogeneity\n",
    "\n",
    "    The algorithm builds many trees and averages the results to obtain the Conditional Average Treatment Effect (CATE) function:\n",
    "\n",
    "    \\[ \\hat{\\tau}(x) = E[Y(1) - Y(0) | X = x] \\]\n",
    "\n",
    "    **Key advantages**:\n",
    "    - Directly targets heterogeneous treatment effects\n",
    "    - Provides measures of variable importance for effect modification\n",
    "    - Performs well with high-dimensional covariates\n",
    "    - Offers honest confidence intervals\n",
    "\n",
    "    **Implementation**: Causal forests are available in packages like `econml` (Python) and `grf` (R)\n",
    "    \"\"\"),\n",
    "    kind=\"info\"\n",
    ")\n",
    "\n",
    "mo.vstack([\n",
    "    causal_forest_desc\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fCoF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install(\"econml\")\n",
    "def _():\n",
    "\n",
    "    try:\n",
    "        from econml.dml import CausalForestDML\n",
    "        from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "\n",
    "        # Implement Causal Forest\n",
    "        def causal_forest(X, T, Y, n_estimators=100, min_samples_leaf=5):\n",
    "            \"\"\"\n",
    "            Estimate ATE and CATE using Causal Forest\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            X : DataFrame of covariates\n",
    "            T : Series of treatment assignments\n",
    "            Y : Series of outcomes\n",
    "            n_estimators : int, Number of trees\n",
    "            min_samples_leaf : int, Minimum samples in leaf\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            ate : Estimated average treatment effect\n",
    "            cate : Estimated conditional average treatment effects\n",
    "            model : Fitted causal forest model\n",
    "            \"\"\"\n",
    "            # Initialize model\n",
    "            cf = CausalForestDML(\n",
    "                model_y=LassoCV(cv=3),\n",
    "                model_t=LogisticRegression(max_iter=1000),\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                discrete_treatment=True,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Fit model\n",
    "            cf.fit(Y, T, X=X)\n",
    "\n",
    "            # Predict CATE\n",
    "            cate = cf.effect(X)\n",
    "\n",
    "            # Calculate ATE\n",
    "            ate = cate.mean()\n",
    "\n",
    "            return ate, cate, cf\n",
    "\n",
    "        # Apply causal forest to our data\n",
    "        cf_ate, cf_cate, cf_model = causal_forest(X_train_scaled, T_train, Y_train)\n",
    "\n",
    "        # True ATE for reference\n",
    "        true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "        # Results summary\n",
    "        results_summary = mo.callout(\n",
    "            mo.md(f\"\"\"\n",
    "            **Causal Forest Results:**\n",
    "\n",
    "            - **Estimated ATE**: {cf_ate:.4f}\n",
    "            - **True ATE**: {true_ate:.4f}\n",
    "            - **Bias**: {cf_ate - true_ate:.4f}\n",
    "            - **Absolute Bias**: {abs(cf_ate - true_ate):.4f}\n",
    "            \"\"\"),\n",
    "            kind=\"info\"\n",
    "        )\n",
    "\n",
    "        # Visualize CATE distribution\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(cf_cate, bins=30, color='skyblue', alpha=0.7)\n",
    "        ax.axvline(x=cf_cate.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean CATE = {cf_cate.mean():.4f}')\n",
    "        ax.axvline(x=true_ate, color='green', linestyle=':', \n",
    "                  label=f'True ATE = {true_ate:.4f}')\n",
    "        ax.set_title('Distribution of Causal Forest CATE Estimates')\n",
    "        ax.set_xlabel('CATE')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        cate_plot = mo.mpl.interactive(fig)\n",
    "\n",
    "        # Feature importance if available\n",
    "        if hasattr(cf_model, 'feature_importances_'):\n",
    "            # Get feature importance\n",
    "            feature_imp = pd.DataFrame({\n",
    "                'Feature': X_train_scaled.columns,\n",
    "                'Importance': cf_model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "            # Plot feature importance\n",
    "            fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "            ax2.barh(y=feature_imp['Feature'].head(10), width=feature_imp['Importance'].head(10))\n",
    "            ax2.set_title('Top 10 Features for Treatment Effect Heterogeneity')\n",
    "            ax2.set_xlabel('Importance')\n",
    "            ax2.invert_yaxis()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            feature_plot = mo.mpl.interactive(fig2)\n",
    "\n",
    "            # Combine all elements in the output\n",
    "            mo.output.replace(mo.vstack([\n",
    "                mo.md(\"#### Causal Forest Results\"),\n",
    "                results_summary,\n",
    "                mo.md(\"#### Treatment Effect Heterogeneity\"),\n",
    "                cate_plot,\n",
    "                mo.md(\"#### Feature Importance for Effect Modification\"),\n",
    "                feature_plot,\n",
    "                mo.md(\"\"\"\n",
    "                The feature importance plot shows which variables contribute most to treatment effect heterogeneity. \n",
    "                These are the features that most strongly modify the effect of treatment, and may be useful for \n",
    "                targeting interventions to those who would benefit most.\n",
    "                \"\"\")\n",
    "            ]))\n",
    "        else:\n",
    "            # Output without feature importance\n",
    "            mo.output.replace(mo.vstack([\n",
    "                mo.md(\"#### Causal Forest Results\"),\n",
    "                results_summary,\n",
    "                mo.md(\"#### Treatment Effect Heterogeneity\"),\n",
    "                cate_plot,\n",
    "                mo.md(\"\"\"\n",
    "                Causal forests directly estimate treatment effect heterogeneity, allowing us to see how \n",
    "                treatment effects vary across different individuals or subgroups. This variation suggests \n",
    "                that the intervention may be more effective for some subgroups than others.\n",
    "                \"\"\")\n",
    "            ]))\n",
    "\n",
    "    except ImportError:\n",
    "        # If econml is not available, provide instructions\n",
    "        mo.output.replace(mo.vstack([\n",
    "            mo.md(\"#### Causal Forest Implementation\"),\n",
    "            mo.callout(\n",
    "                mo.md(\"\"\"\n",
    "                **EconML package not available**\n",
    "\n",
    "                Causal Forests require the EconML package, which is not currently installed.\n",
    "\n",
    "                To install EconML and implement Causal Forests, you can run:\n",
    "                ```\n",
    "                pip install econml\n",
    "                ```\n",
    "\n",
    "                Causal forests are powerful for estimating heterogeneous treatment effects and \n",
    "                identifying important features that modify treatment effects.\n",
    "                \"\"\"),\n",
    "                kind=\"warn\"\n",
    "            )\n",
    "        ]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LkGn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"#### 6.3.4 Comparing All Advanced Methods {#compare-advanced}\n",
    "\n",
    "    Now let's compare all the advanced machine learning methods we've implemented to see which ones perform best in estimating causal effects in our dataset.\n",
    "    \"\"\")\n",
    "    mo.output.replace(subsection_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zVRe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    # True ATE for reference\n",
    "    true_ate = (Y1_train - Y0_train).mean()\n",
    "\n",
    "    # Collect results from all advanced methods\n",
    "    # These would be calculated by previous cells, but we'll recreate them here for consistency\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # S-Learner (Random Forest)\n",
    "    def s_learner_rf(X, T, Y):\n",
    "        # Create combined dataset\n",
    "        X_combined = X.copy()\n",
    "        X_combined['treatment'] = T\n",
    "\n",
    "        # Fit model\n",
    "        model = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        model.fit(X_combined, Y)\n",
    "\n",
    "        # Create counterfactual datasets\n",
    "        X_pred_1 = X.copy()\n",
    "        X_pred_1['treatment'] = 1\n",
    "\n",
    "        X_pred_0 = X.copy()\n",
    "        X_pred_0['treatment'] = 0\n",
    "\n",
    "        # Predict potential outcomes\n",
    "        y_pred_1 = model.predict(X_pred_1)\n",
    "        y_pred_0 = model.predict(X_pred_0)\n",
    "\n",
    "        # Calculate ATE\n",
    "        ate = (y_pred_1 - y_pred_0).mean()\n",
    "\n",
    "        return ate\n",
    "\n",
    "    # T-Learner (Random Forest)\n",
    "    def t_learner_rf(X, T, Y):\n",
    "        # Split data into treated and control groups\n",
    "        X_t = X[T == 1]\n",
    "        Y_t = Y[T == 1]\n",
    "        X_c = X[T == 0]\n",
    "        Y_c = Y[T == 0]\n",
    "\n",
    "        # Fit models\n",
    "        model_t = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        model_c = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)\n",
    "        model_t.fit(X_t, Y_t)\n",
    "        model_c.fit(X_c, Y_c)\n",
    "\n",
    "        # Predict potential outcomes\n",
    "        y_pred_1 = model_t.predict(X)\n",
    "        y_pred_0 = model_c.predict(X)\n",
    "\n",
    "        # Calculate ATE\n",
    "        ate = (y_pred_1 - y_pred_0).mean()\n",
    "\n",
    "        return ate\n",
    "\n",
    "    # Doubly Robust (AIPW with RF)\n",
    "    def aipw_rf(X, T, Y):\n",
    "        # Split data by treatment group\n",
    "        X_t = X[T == 1]\n",
    "        Y_t = Y[T == 1]\n",
    "        X_c = X[T == 0]\n",
    "        Y_c = Y[T == 0]\n",
    "\n",
    "        # Fit outcome models\n",
    "        model_t = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "        model_c = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=43)\n",
    "        model_t.fit(X_t, Y_t)\n",
    "        model_c.fit(X_c, Y_c)\n",
    "\n",
    "        # Predict potential outcomes\n",
    "        mu_1 = model_t.predict(X)\n",
    "        mu_0 = model_c.predict(X)\n",
    "\n",
    "        # Fit propensity model\n",
    "        ps_model = LogisticRegression(max_iter=1000)\n",
    "        ps_model.fit(X, T)\n",
    "        ps = ps_model.predict_proba(X)[:, 1]\n",
    "\n",
    "        # Handle extreme propensity scores\n",
    "        eps = 1e-12\n",
    "        ps = np.maximum(eps, np.minimum(1 - eps, ps))\n",
    "\n",
    "        # Calculate AIPW estimator\n",
    "        aipw_0 = mu_0 + (1 - T) * (Y - mu_0) / (1 - ps)\n",
    "        aipw_1 = mu_1 + T * (Y - mu_1) / ps\n",
    "\n",
    "        # Calculate ATE\n",
    "        ate = (aipw_1 - aipw_0).mean()\n",
    "\n",
    "        return ate\n",
    "\n",
    "    # Try to calculate Causal Forest ATE if available\n",
    "    try:\n",
    "        from econml.dml import CausalForestDML\n",
    "        from sklearn.linear_model import LassoCV\n",
    "\n",
    "        # Initialize and fit model\n",
    "        cf = CausalForestDML(\n",
    "            model_y=LassoCV(cv=3),\n",
    "            model_t=LogisticRegression(max_iter=1000),\n",
    "            n_estimators=100,\n",
    "            min_samples_leaf=5,\n",
    "            discrete_treatment=True,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        cf.fit(Y_train, T_train, X=X_train_scaled)\n",
    "        cf_ate = cf.effect(X_train_scaled).mean()\n",
    "\n",
    "        include_cf = True\n",
    "    except:\n",
    "        include_cf = False\n",
    "\n",
    "    # Compute ATEs\n",
    "    sl_ate = s_learner_rf(X_train_scaled, T_train, Y_train)\n",
    "    tl_ate = t_learner_rf(X_train_scaled, T_train, Y_train)\n",
    "    aipw_ate = aipw_rf(X_train_scaled, T_train, Y_train)\n",
    "\n",
    "    # Compile results\n",
    "    results = [\n",
    "        {'Method': 'S-Learner (RF)', 'ATE': sl_ate, 'Bias': sl_ate - true_ate, 'Abs Bias': abs(sl_ate - true_ate), 'Type': 'Meta-Learner'},\n",
    "        {'Method': 'T-Learner (RF)', 'ATE': tl_ate, 'Bias': tl_ate - true_ate, 'Abs Bias': abs(tl_ate - true_ate), 'Type': 'Meta-Learner'},\n",
    "        {'Method': 'AIPW (RF, Logistic)', 'ATE': aipw_ate, 'Bias': aipw_ate - true_ate, 'Abs Bias': abs(aipw_ate - true_ate), 'Type': 'Doubly Robust'}\n",
    "    ]\n",
    "\n",
    "    # Add Causal Forest if available\n",
    "    if include_cf:\n",
    "        results.append({'Method': 'Causal Forest', 'ATE': cf_ate, 'Bias': cf_ate - true_ate, 'Abs Bias': abs(cf_ate - true_ate), 'Type': 'Causal Forest'})\n",
    "\n",
    "    # Convert to DataFrame and sort by absolute bias\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Abs Bias')\n",
    "\n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot with colors by method type\n",
    "    colors = {'Meta-Learner': 'skyblue', 'Doubly Robust': 'lightgreen', 'Causal Forest': 'salmon'}\n",
    "\n",
    "    for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "        ax.barh(i, row['ATE'], color=colors[row['Type']], label=row['Type'] if row['Type'] not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    # Add method names and reference line\n",
    "    ax.set_yticks(range(len(results_df)))\n",
    "    ax.set_yticklabels(results_df['Method'])\n",
    "    ax.axvline(x=true_ate, color='red', linestyle='--', label=f'True ATE = {true_ate:.4f}')\n",
    "\n",
    "    # Add legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique_labels = []\n",
    "    unique_handles = []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in unique_labels:\n",
    "            unique_labels.append(label)\n",
    "            unique_handles.append(handle)\n",
    "\n",
    "    ax.legend(unique_handles, unique_labels, loc='lower right')\n",
    "\n",
    "    ax.set_title('Comparison of Advanced Machine Learning Methods for Causal Inference')\n",
    "    ax.set_xlabel('ATE Estimate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create interactive plot\n",
    "    plot = mo.mpl.interactive(fig)\n",
    "\n",
    "    # Best method summary\n",
    "    best_method_idx = results_df['Abs Bias'].idxmin()\n",
    "    best_method = results_df.loc[best_method_idx]\n",
    "\n",
    "    best_method_summary = mo.callout(\n",
    "        mo.md(f\"\"\"\n",
    "        **Best Advanced Method: {best_method['Method']}**\n",
    "\n",
    "        - **ATE Estimate**: {best_method['ATE']:.4f}\n",
    "        - **True ATE**: {true_ate:.4f}\n",
    "        - **Bias**: {best_method['Bias']:.4f}\n",
    "\n",
    "        Advanced machine learning methods can significantly improve causal estimates by capturing complex relationships \n",
    "        between variables without requiring strong parametric assumptions.\n",
    "        \"\"\"),\n",
    "        kind=\"success\"\n",
    "    )\n",
    "\n",
    "    # Method comparison and analysis\n",
    "    method_analysis = mo.md(\"\"\"\n",
    "    **Analysis of Advanced Methods**:\n",
    "\n",
    "    1. **Meta-Learners** leverage flexible machine learning algorithms to model outcomes or treatment effects. \n",
    "       They work well when relationships are complex but depend heavily on the choice of base learner.\n",
    "\n",
    "    2. **Doubly Robust Methods** combine outcome modeling and propensity score approaches, providing protection \n",
    "       against model misspecification. They tend to have lower bias and are more robust to model choice.\n",
    "\n",
    "    3. **Causal Forests** excel at capturing treatment effect heterogeneity and provide valuable insights through \n",
    "       feature importance. They're particularly useful for understanding which subgroups benefit most from treatment.\n",
    "\n",
    "    The best method depends on the specific context, data structure, and research question. In practice, \n",
    "    it's valuable to implement multiple methods and compare their results, as we've done here.\n",
    "    \"\"\")\n",
    "\n",
    "    # Combine all elements in the output\n",
    "    mo.output.replace(mo.vstack([\n",
    "        mo.md(\"#### Comparison of Advanced Machine Learning Methods\"),\n",
    "        plot,\n",
    "        best_method_summary,\n",
    "        method_analysis,\n",
    "        mo.ui.table(results_df)\n",
    "    ]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woaO",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    section_header = mo.md(\"\"\"## 8. Conclusion and Best Practices {#conclusion}\"\"\")\n",
    "    mo.output.replace(section_header)\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HnMC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 8.1 Summary of Findings {#summary-findings}\"\"\")\n",
    "\n",
    "    # Create a summary of key findings from the causal analysis\n",
    "    key_findings = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Key Insights from Causal Analysis\n",
    "\n",
    "        1. **Treatment Effect Size**: The IHDP intervention has a substantial positive effect on cognitive scores with an average treatment effect (ATE) of approximately 4.0.\n",
    "\n",
    "        2. **Selection Bias**: The semi-synthetic IHDP dataset exhibits moderate selection bias, as evidenced by the small difference between naive estimators and true effects.\n",
    "\n",
    "        3. **Method Performance**: Advanced ML methods, particularly the S-Learner with Random Forest, achieved the lowest bias (0.004) among all approaches, demonstrating the value of flexible machine learning in causal inference.\n",
    "\n",
    "        4. **Treatment Effect Heterogeneity**: All methods revealed variation in treatment effects across subgroups, suggesting that the intervention effectiveness differs based on individual characteristics.\n",
    "\n",
    "        5. **Important Modifiers**: Causal forest analysis identified first-born status (x_5), birth weight (x_0), and mother's education level (x_4) as the most important variables influencing treatment effect heterogeneity.\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Create a summary of methodological takeaways\n",
    "    method_takeaways = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Methodological Takeaways\n",
    "\n",
    "        1. **Method Progression**: Starting with simple methods provided valuable baselines, while propensity score methods offered intuitive adjustments for confounding, and advanced ML methods captured complex relationships.\n",
    "\n",
    "        2. **Propensity Score Considerations**: The logistic regression model provided better common support (89% vs. 61%) than the random forest model, despite the latter's superior AUC (0.91 vs. 0.75), highlighting that discriminative performance isn't the primary goal in propensity score estimation.\n",
    "\n",
    "        3. **Doubly Robust Advantage**: AIPW methods demonstrated strong performance with very low bias, confirming the theoretical advantage of protection against model misspecification.\n",
    "\n",
    "        4. **Treatment Effect Heterogeneity**: Methods capable of estimating CATE (Conditional Average Treatment Effect) provided richer insights than those focused solely on ATE, revealing important patterns in effect variation across subgroups.\n",
    "\n",
    "        5. **Model Selection Importance**: The choice of base learner in meta-learners and specification choices in propensity score methods substantively affected estimation quality, emphasizing the importance of comparing multiple approaches.\n",
    "        \"\"\"),\n",
    "        kind=\"success\"\n",
    "    )\n",
    "\n",
    "    mo.output.replace(mo.vstack([subsection_header, key_findings, method_takeaways]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wadT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 8.2 Recommendations {#recommendations}\"\"\")\n",
    "\n",
    "    # Create guidelines for method selection\n",
    "    method_guidelines = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Guidelines for Method Selection\n",
    "\n",
    "        **Simple Methods**\n",
    "        - Use **Regression Adjustment** when relationships appear linear and interpretability is a priority\n",
    "        - Apply **Stratification** to examine treatment effect heterogeneity across specific covariates\n",
    "        - Employ **Naive Mean Difference** only as a baseline for comparison\n",
    "\n",
    "        **Propensity Score Methods**\n",
    "        - Choose **Matching** when intuitive explanation is important and preserving original outcome scale is desired\n",
    "        - Use **IPW** when using all available data is critical and propensity scores are well-estimated\n",
    "        - Apply **Stratification** to examine effect modification across propensity score strata\n",
    "\n",
    "        **Advanced ML Methods**\n",
    "        - Implement **S-Learner** when simplicity and a single model are preferred\n",
    "        - Choose **T-Learner** when treatment and control groups may have very different outcome relationships\n",
    "        - Use **Doubly Robust Methods** when robustness to model misspecification is critical\n",
    "        - Apply **Causal Forests** when treatment effect heterogeneity is the primary focus\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    # Create practical considerations\n",
    "    practical_considerations = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Practical Considerations\n",
    "\n",
    "        1. **Data Quality Assessment**: Before applying causal methods, carefully examine data for missing values, outliers, and covariate balance between treatment groups.\n",
    "\n",
    "        2. **Positivity/Overlap Check**: Verify sufficient overlap in propensity scores between treated and control units to ensure reliable causal inference.\n",
    "\n",
    "        3. **Multiple Method Comparison**: Implement several causal methods and compare results; consistent estimates across methods increase confidence in findings.\n",
    "\n",
    "        4. **Sensitivity Analysis**: When working with real observational data (unlike our semi-synthetic case), conduct sensitivity analysis to assess robustness to unmeasured confounding.\n",
    "\n",
    "        5. **Focused Heterogeneity Analysis**: Based on domain knowledge, identify key variables likely to modify treatment effects and deliberately examine heterogeneity across these dimensions.\n",
    "\n",
    "        6. **Interpretability Needs**: Consider the audience and purpose when selecting methods; simpler methods may be preferred when communication to non-technical stakeholders is important.\n",
    "        \"\"\"),\n",
    "        kind=\"warn\"\n",
    "    )\n",
    "\n",
    "    # Create limitations and caveats\n",
    "    limitations = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        #### Limitations and Caveats\n",
    "\n",
    "        1. **Untestable Assumptions**: Remember that unconfoundedness/ignorability cannot be directly verified from observed data; domain knowledge is essential for justifying this assumption.\n",
    "\n",
    "        2. **Semi-Synthetic Nature**: Our IHDP dataset is semi-synthetic, allowing us to know true effects; in real applications, ground truth is unknown and evaluation is more challenging.\n",
    "\n",
    "        3. **External Validity**: Causal effects estimated from a specific population may not generalize to different populations with different covariate distributions.\n",
    "\n",
    "        4. **Extreme Propensity Scores**: Units with very high or low propensity scores can lead to unstable estimates in methods like IPW; trimming or stabilization should be considered.\n",
    "\n",
    "        5. **Computational Requirements**: Advanced ML methods offer improved performance but at the cost of greater computational complexity and reduced interpretability.\n",
    "\n",
    "        6. **Target Estimand Clarity**: Different methods may target different estimands (ATE, ATT, CATE); ensure the chosen methods align with the specific causal question of interest.\n",
    "        \"\"\"),\n",
    "        kind=\"danger\"\n",
    "    )\n",
    "\n",
    "    mo.output.replace(mo.vstack([subsection_header, method_guidelines, practical_considerations, limitations]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VCRE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a final summary visualization showing method performance across categories\n",
    "def _():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create data for the visualization\n",
    "    methods = ['Naive Mean Difference', 'Regression Adjustment', 'Stratification',\n",
    "              'IPW', 'Matching', 'S-Learner (RF)', 'AIPW', 'Causal Forest']\n",
    "\n",
    "    # These are example values - you would replace with your actual results\n",
    "    bias = [0.067, 0.010, 0.085, 0.036, 0.026, 0.004, 0.017, 0.129]\n",
    "\n",
    "    categories = ['Simple', 'Simple', 'Simple', \n",
    "                  'Propensity Score', 'Propensity Score',\n",
    "                  'Advanced ML', 'Advanced ML', 'Advanced ML']\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Method': methods,\n",
    "        'Absolute Bias': bias,\n",
    "        'Category': categories\n",
    "    })\n",
    "\n",
    "    # Sort by absolute bias\n",
    "    plot_data = plot_data.sort_values('Absolute Bias')\n",
    "\n",
    "    # Create color mapping for categories\n",
    "    colors = {'Simple': 'skyblue', 'Propensity Score': 'lightgreen', 'Advanced ML': 'salmon'}\n",
    "    bar_colors = [colors[cat] for cat in plot_data['Category']]\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot horizontal bars\n",
    "    bars = ax.barh(plot_data['Method'], plot_data['Absolute Bias'], color=bar_colors)\n",
    "\n",
    "    # Add a vertical line for reference\n",
    "    ax.axvline(x=0.05, color='gray', linestyle='--', alpha=0.7, label='5% Bias Threshold')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_title('Comparison of All Causal Inference Methods by Absolute Bias')\n",
    "    ax.set_xlabel('Absolute Bias')\n",
    "    ax.set_ylabel('Method')\n",
    "\n",
    "    # Add text labels to the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "               f'{width:.4f}', ha='left', va='center')\n",
    "\n",
    "    # Add a legend for categories\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[cat], label=cat) for cat in colors.keys()]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    # Add grid lines for better readability\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Create conclusions based on the visual\n",
    "    conclusions = mo.md(\"\"\"\n",
    "    ### Final Insights\n",
    "\n",
    "    This comparative analysis across all implemented methods reveals several key patterns:\n",
    "\n",
    "    1. **Method Sophistication vs. Performance**: While advanced ML methods generally perform well, certain simpler methods (like Regression Adjustment) can achieve comparable results when relationships are approximately linear.\n",
    "\n",
    "    2. **Low Selection Bias Impact**: The relatively small bias of even the naive method suggests moderate selection bias in this dataset, which explains why even simple methods can perform adequately.\n",
    "\n",
    "    3. **Heterogeneity Insights Matter**: Beyond just bias reduction, methods like Causal Forests provide valuable insights into effect heterogeneity that can guide targeted interventions.\n",
    "\n",
    "    4. **Method Selection Considerations**: The \"best\" method depends on the specific context, goals, and constraints. Factors like interpretability, computational complexity, and the specific causal questions should guide method selection.\n",
    "\n",
    "    5. **Multiple Methods Approach**: Using multiple methods and comparing results provides more robust and trustworthy causal insights than relying on any single method.\n",
    "    \"\"\")\n",
    "\n",
    "    # Create the final layout\n",
    "    final_header = mo.md(\"### Comprehensive Method Performance\")\n",
    "    plot = mo.mpl.interactive(fig)\n",
    "\n",
    "    mo.output.replace(mo.vstack([final_header, plot, conclusions]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hgqU",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PSUk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    subsection_header = mo.md(\"\"\"### 8.4 Resources and References {#resources}\"\"\")\n",
    "\n",
    "    references = mo.md(\"\"\"\n",
    "    #### Key Books\n",
    "\n",
    "    1. Pearl, J. (2009). **Causality: Models, Reasoning, and Inference** (2nd ed.). Cambridge University Press. \n",
    "       [https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B](https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B)\n",
    "\n",
    "    2. HernÃ¡n, M. A., & Robins, J. M. (2020). **Causal Inference: What If**. Chapman & Hall/CRC.\n",
    "       [https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n",
    "\n",
    "    3. Peters, J., Janzing, D., & SchÃ¶lkopf, B. (2017). **Elements of Causal Inference: Foundations and Learning Algorithms**. MIT Press.\n",
    "       [https://library.oapen.org/bitstream/id/056a11be-ce3a-44b9-8987-a6c68fce8d9b/11283.pdf](https://library.oapen.org/bitstream/id/056a11be-ce3a-44b9-8987-a6c68fce8d9b/11283.pdf)\n",
    "\n",
    "    4. Imbens, G. W., & Rubin, D. B. (2015). **Causal Inference for Statistics, Social, and Biomedical Sciences**. Cambridge University Press.\n",
    "       [https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB](https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB)\n",
    "\n",
    "    5. Cunningham, S. (2021). **Causal Inference: The Mixtape**. Yale University Press.\n",
    "       [https://mixtape.scunning.com/](https://mixtape.scunning.com/)\n",
    "\n",
    "    6. Chernozhukov, V., et al. (2023). **Applied Causal Inference Powered by ML and AI**. \n",
    "       [https://www.artsci.com/acipma](https://www.artsci.com/acipma)\n",
    "\n",
    "    #### Research Articles and Papers\n",
    "\n",
    "    7. Zanga, A., Ozkirimli, E., & Stella, F. (2022). **A Survey on Causal Discovery: Theory and Practice**. International Journal of Approximate Reasoning.\n",
    "       [https://www.sciencedirect.com/science/article/abs/pii/S0888613X22001402](https://www.sciencedirect.com/science/article/abs/pii/S0888613X22001402)\n",
    "\n",
    "    8. Pearl, J. (2010). **An Introduction to Causal Inference**. The International Journal of Biostatistics, 6(2).\n",
    "       [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836213/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836213/)\n",
    "\n",
    "    9. Bongers, S., ForrÃ©, P., Peters, J., & Mooij, J. M. (2021). **Foundations of structural causal models with cycles and latent variables**. The Annals of Statistics, 49(5), 2885-2915.\n",
    "\n",
    "    10. Athey, S., & Imbens, G. (2019). **Machine learning methods that economists should know about**. Annual Review of Economics.\n",
    "        [https://web.stanford.edu/~athey/papers/MLECTA.pdf](https://web.stanford.edu/~athey/papers/MLECTA.pdf)\n",
    "\n",
    "    #### Online Resources\n",
    "\n",
    "    11. Neal, B. (2023). **Which causal inference book you should read**.\n",
    "        [https://www.bradyneal.com/which-causal-inference-book](https://www.bradyneal.com/which-causal-inference-book)\n",
    "\n",
    "    12. Pearl, J. **Causal Inference in Statistics: A Primer (Course Materials)**.\n",
    "        [http://bayes.cs.ucla.edu/PRIMER/](http://bayes.cs.ucla.edu/PRIMER/)\n",
    "\n",
    "    13. **Introduction to Causal Inference (Course)** by Brady Neal.\n",
    "        [https://www.bradyneal.com/causal-inference-course](https://www.bradyneal.com/causal-inference-course)\n",
    "\n",
    "    14. **DoWhy: A Python library for causal inference**.\n",
    "        [https://microsoft.github.io/dowhy/](https://microsoft.github.io/dowhy/)\n",
    "\n",
    "    15. **EconML: A Python library for ML-based causal inference**.\n",
    "        [https://github.com/microsoft/EconML](https://github.com/microsoft/EconML)\n",
    "    \"\"\")\n",
    "\n",
    "    mo.output.replace(mo.vstack([subsection_header, references]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfOT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    license_header = mo.md(\"\"\"### License {#license}\"\"\")\n",
    "\n",
    "    license_text = mo.callout(\n",
    "        mo.md(\"\"\"\n",
    "        ## MIT License\n",
    "\n",
    "        **Understanding Causal Inference with IHDP: From Theory to Practice**\n",
    "\n",
    "        Copyright (c) 2025\n",
    "\n",
    "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "        of this software and associated documentation files (the \"Notebook\"), to deal\n",
    "        in the Notebook without restriction, including without limitation the rights\n",
    "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "        copies of the Notebook, and to permit persons to whom the Notebook is\n",
    "        furnished to do so, subject to the following conditions:\n",
    "\n",
    "        The above copyright notice and this permission notice shall be included in all\n",
    "        copies or substantial portions of the Notebook.\n",
    "\n",
    "        THE NOTEBOOK IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "        OUT OF OR IN CONNECTION WITH THE NOTEBOOK OR THE USE OR OTHER DEALINGS IN THE\n",
    "        NOTEBOOK.\n",
    "\n",
    "        ---\n",
    "\n",
    "        **Citation Information:**\n",
    "\n",
    "        If you use this notebook in your research or teaching, please cite it as:\n",
    "\n",
    "        ```\n",
    "        @misc{causal_inference_ihdp,\n",
    "          author = {Sai Surya Madhav Rebbapragada},\n",
    "          title = {Understanding Causal Inference with IHDP: From Theory to Practice},\n",
    "          year = {2025},\n",
    "          url = {https://github.com/suryaMadhav16/AdvDataScienceCasuality}\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        **Data Attribution:**\n",
    "\n",
    "        The IHDP data used in this notebook is based on the Infant Health and Development Program and was modified by Hill (2011) for causal inference research.\n",
    "\n",
    "        Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.\n",
    "        \"\"\"),\n",
    "        kind=\"info\"\n",
    "    )\n",
    "\n",
    "    mo.output.replace(mo.vstack([license_header, license_text]))\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGiW",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
