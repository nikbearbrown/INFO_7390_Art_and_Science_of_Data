id,title,abstract,authors,categories,date,source
2504.16929v1,I-Con: A Unifying Framework for Representation Learning,"As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.",['Unknown Author'],"['cs.LG', 'cs.AI', 'cs.CV', 'cs.IT', 'math.IT']",2025-04-23T17:59:01Z,arXiv
2504.16925v1,Latent Diffusion Planning for Imitation Learning,"Recent progress in imitation learning has been enabled by policy
architectures that scale to complex visuomotor tasks, multimodal distributions,
and large datasets. However, these methods often rely on learning from large
amount of expert demonstrations. To address these shortcomings, we propose
Latent Diffusion Planning (LDP), a modular approach consisting of a planner
which can leverage action-free demonstrations, and an inverse dynamics model
which can leverage suboptimal data, that both operate over a learned latent
space. First, we learn a compact latent space through a variational
autoencoder, enabling effective forecasting of future states in image-based
domains. Then, we train a planner and an inverse dynamics model with diffusion
objectives. By separating planning from action prediction, LDP can benefit from
the denser supervision signals of suboptimal and action-free data. On simulated
visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation
learning approaches, as they cannot leverage such additional data.",['Unknown Author'],"['cs.RO', 'cs.AI']",2025-04-23T17:53:34Z,arXiv
2504.16923v1,"Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous
  Driving","High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA",['Unknown Author'],"['cs.RO', 'cs.LG', 'cs.SY', 'eess.SY']",2025-04-23T17:51:36Z,arXiv
2504.16922v1,"Generalized Neighborhood Attention: Multi-dimensional Sparse Attention
  at the Speed of Light","Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.",['Unknown Author'],"['cs.CV', 'cs.AI', 'cs.LG']",2025-04-23T17:49:53Z,arXiv
2504.16918v1,OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents,"Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T17:45:05Z,arXiv
2504.16917v1,"Application of an attention-based CNN-BiLSTM framework for in vivo
  two-photon calcium imaging of neuronal ensembles: decoding complex bilateral
  forelimb movements from unilateral M1","Decoding behavior, such as movement, from multiscale brain networks remains a
central objective in neuroscience. Over the past decades, artificial
intelligence and machine learning have played an increasingly significant role
in elucidating the neural mechanisms underlying motor function. The advancement
of brain-monitoring technologies, capable of capturing complex neuronal signals
with high spatial and temporal resolution, necessitates the development and
application of more sophisticated machine learning models for behavioral
decoding. In this study, we employ a hybrid deep learning framework, an
attention-based CNN-BiLSTM model, to decode skilled and complex forelimb
movements using signals obtained from in vivo two-photon calcium imaging. Our
findings demonstrate that the intricate movements of both ipsilateral and
contralateral forelimbs can be accurately decoded from unilateral M1 neuronal
ensembles. These results highlight the efficacy of advanced hybrid deep
learning models in capturing the spatiotemporal dependencies of neuronal
networks activity linked to complex movement execution.",['Unknown Author'],"['q-bio.NC', 'cs.LG']",2025-04-23T17:43:00Z,arXiv
2504.16913v1,"Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM
  Behind AI-Generated Text","In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T17:39:49Z,arXiv
2504.16907v1,BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation,"Text-to-video (T2V) generative models have rapidly advanced and found
widespread applications across fields like entertainment, education, and
marketing. However, the adversarial vulnerabilities of these models remain
rarely explored. We observe that in T2V generation tasks, the generated videos
often contain substantial redundant information not explicitly specified in the
text prompts, such as environmental elements, secondary objects, and additional
details, providing opportunities for malicious attackers to embed hidden
harmful content. Exploiting this inherent redundancy, we introduce BadVideo,
the first backdoor attack framework tailored for T2V generation. Our attack
focuses on designing target adversarial outputs through two key strategies: (1)
Spatio-Temporal Composition, which combines different spatiotemporal features
to encode malicious information; (2) Dynamic Element Transformation, which
introduces transformations in redundant elements over time to convey malicious
information. Based on these strategies, the attacker's malicious target
seamlessly integrates with the user's textual instructions, providing high
stealthiness. Moreover, by exploiting the temporal dimension of videos, our
attack successfully evades traditional content moderation systems that
primarily analyze spatial information within individual frames. Extensive
experiments demonstrate that BadVideo achieves high attack success rates while
preserving original semantics and maintaining excellent performance on clean
inputs. Overall, our work reveals the adversarial vulnerability of T2V models,
calling attention to potential risks and misuse. Our project page is at
https://wrt2000.github.io/BadVideo2025/.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T17:34:48Z,arXiv
2504.16902v1,Building A Secure Agentic AI Application Leveraging A2A Protocol,"As Agentic AI systems evolve from basic workflows to complex multi agent
collaboration, robust protocols such as Google's Agent2Agent (A2A) become
essential enablers. To foster secure adoption and ensure the reliability of
these complex interactions, understanding the secure implementation of A2A is
essential. This paper addresses this goal by providing a comprehensive security
analysis centered on the A2A protocol. We examine its fundamental elements and
operational dynamics, situating it within the framework of agent communication
development. Utilizing the MAESTRO framework, specifically designed for AI
risks, we apply proactive threat modeling to assess potential security issues
in A2A deployments, focusing on aspects such as Agent Card management, task
execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development
methodologies and architectural best practices designed to build resilient and
effective A2A systems. Our analysis also explores how the synergy between A2A
and the Model Context Protocol (MCP) can further enhance secure
interoperability. This paper equips developers and architects with the
knowledge and practical guidance needed to confidently leverage the A2A
protocol for building robust and secure next generation agentic applications.",['Unknown Author'],"['cs.CR', 'cs.AI']",2025-04-23T17:27:49Z,arXiv
2504.16891v1,"AIMO-2 Winning Solution: Building State-of-the-Art Mathematical
  Reasoning Models with OpenMathReasoning dataset","This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.",['Unknown Author'],"['cs.AI', 'cs.CL', 'cs.LG']",2025-04-23T17:13:04Z,arXiv
2504.16886v1,Exploring zero-shot structure-based protein fitness prediction,"The ability to make zero-shot predictions about the fitness consequences of
protein sequence changes with pre-trained machine learning models enables many
practical applications. Such models can be applied for downstream tasks like
genetic variant interpretation and protein engineering without additional
labeled data. The advent of capable protein structure prediction tools has led
to the availability of orders of magnitude more precomputed predicted
structures, giving rise to powerful structure-based fitness prediction models.
Through our experiments, we assess several modeling choices for structure-based
models and their effects on downstream fitness prediction. Zero-shot fitness
prediction models can struggle to assess the fitness landscape within
disordered regions of proteins, those that lack a fixed 3D structure. We
confirm the importance of matching protein structures to fitness assays and
find that predicted structures for disordered regions can be misleading and
affect predictive performance. Lastly, we evaluate an additional
structure-based model on the ProteinGym substitution benchmark and show that
simple multi-modal ensembles are strong baselines.",['Unknown Author'],"['q-bio.QM', 'cs.LG', 'q-bio.BM']",2025-04-23T17:01:09Z,arXiv
2504.16879v1,Learning Verifiable Control Policies Using Relaxed Verification,"To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.",['Unknown Author'],"['eess.SY', 'cs.LG', 'cs.SY']",2025-04-23T16:54:35Z,arXiv
2504.16875v1,"Hybrid Reinforcement Learning and Model Predictive Control for Adaptive
  Control of Hydrogen-Diesel Dual-Fuel Combustion","Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.",['Unknown Author'],['cs.LG'],2025-04-23T16:51:49Z,arXiv
2504.16871v1,Exploring How LLMs Capture and Represent Domain-Specific Knowledge,"We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks",['Unknown Author'],['cs.LG'],2025-04-23T16:46:06Z,arXiv
2504.16866v1,"An Adaptive ML Framework for Power Converter Monitoring via Federated
  Transfer Learning","This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.",['Unknown Author'],['cs.LG'],2025-04-23T16:39:54Z,arXiv
2504.16864v1,"Common Functional Decompositions Can Mis-attribute Differences in
  Outcomes Between Populations","In science and social science, we often wish to explain why an outcome is
different in two populations. For instance, if a jobs program benefits members
of one city more than another, is that due to differences in program
participants (particular covariates) or the local labor markets (outcomes given
covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool
in econometrics that explains the difference in the mean outcome across two
populations. However, the KOB decomposition assumes a linear relationship
between covariates and outcomes, while the true relationship may be
meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear
functional decompositions for the relationship between outcomes and covariates
in one population. It seems natural to extend the KOB decomposition using these
functional decompositions. We observe that a successful extension should not
attribute the differences to covariates -- or, respectively, to outcomes given
covariates -- if those are the same in the two populations. Unfortunately, we
demonstrate that, even in simple examples, two common decompositions --
functional ANOVA and Accumulated Local Effects -- can attribute differences to
outcomes given covariates, even when they are identical in two populations. We
provide a characterization of when functional ANOVA misattributes, as well as a
general property that any discrete decomposition must satisfy to avoid
misattribution. We show that if the decomposition is independent of its input
distribution, it does not misattribute. We further conjecture that
misattribution arises in any reasonable additive decomposition that depends on
the distribution of the covariates.",['Unknown Author'],"['stat.ME', 'cs.LG', 'econ.EM', 'stat.ML']",2025-04-23T16:36:55Z,arXiv
2504.16837v1,Approximating Optimal Labelings for Temporal Connectivity,"In a temporal graph the edge set dynamically changes over time according to a
set of time-labels associated with each edge that indicates at which time-steps
the edge is available. Two vertices are connected if there is a path connecting
them in which the edges are traversed in increasing order of their labels. We
study the problem of scheduling the availability time of the edges of a
temporal graph in such a way that all pairs of vertices are connected within a
given maximum allowed time $a$ and the overall number of labels is minimized.
  The problem, known as \emph{Minimum Aged Labeling} (MAL), has several
applications in logistics, distribution scheduling, and information spreading
in social networks, where carefully choosing the time-labels can significantly
reduce infrastructure costs, fuel consumption, or greenhouse gases.
  The problem MAL has previously been proved to be NP-complete on undirected
graphs and \APX-hard on directed graphs. In this paper, we extend our knowledge
on the complexity and approximability of MAL in several directions. We first
show that the problem cannot be approximated within a factor better than
$O(\log n)$ when $a\geq 2$, unless $\text{P} = \text{NP}$, and a factor better
than $2^{\log ^{1-\epsilon} n}$ when $a\geq 3$, unless $\text{NP}\subseteq
\text{DTIME}(2^{\text{polylog}(n)})$, where $n$ is the number of vertices in
the graph. Then we give a set of approximation algorithms that, under some
conditions, almost match these lower bounds. In particular, we show that the
approximation depends on a relation between $a$ and the diameter of the input
graph.
  We further establish a connection with a foundational optimization problem on
static graphs called \emph{Diameter Constrained Spanning Subgraph} (DCSS) and
show that our hardness results also apply to DCSS.",['Unknown Author'],"['cs.DS', 'cs.AI']",2025-04-23T16:00:33Z,arXiv
2504.16834v1,Improving Significant Wave Height Prediction Using Chronos Models,"Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.",['Unknown Author'],"['cs.LG', 'cs.AI', 'physics.ao-ph']",2025-04-23T15:56:28Z,arXiv
2504.16831v1,"Evaluating Autoencoders for Parametric and Invertible Multidimensional
  Projections","Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.",['Unknown Author'],['cs.LG'],2025-04-23T15:47:20Z,arXiv
2504.16828v1,Process Reward Models That Think,"Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.",['Unknown Author'],"['cs.LG', 'cs.AI', 'cs.CL']",2025-04-23T15:44:54Z,arXiv
2504.16798v1,"4D Multimodal Co-attention Fusion Network with Latent Contrastive
  Alignment for Alzheimer's Diagnosis","Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.",['Unknown Author'],"['cs.MM', 'cs.CV', 'cs.LG']",2025-04-23T15:18:55Z,arXiv
2504.16795v1,"Random Long-Context Access for Mamba via Hardware-aligned Hierarchical
  Sparse Attention","A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T15:15:06Z,arXiv
2504.16791v1,Radiometer Calibration using Machine Learning,"Radiometers are crucial instruments in radio astronomy, forming the primary
component of nearly all radio telescopes. They measure the intensity of
electromagnetic radiation, converting this radiation into electrical signals. A
radiometer's primary components are an antenna and a Low Noise Amplifier (LNA),
which is the core of the ``receiver'' chain. Instrumental effects introduced by
the receiver are typically corrected or removed during calibration. However,
impedance mismatches between the antenna and receiver can introduce unwanted
signal reflections and distortions. Traditional calibration methods, such as
Dicke switching, alternate the receiver input between the antenna and a
well-characterised reference source to mitigate errors by comparison. Recent
advances in Machine Learning (ML) offer promising alternatives. Neural
networks, which are trained using known signal sources, provide a powerful
means to model and calibrate complex systems where traditional analytical
approaches struggle. These methods are especially relevant for detecting the
faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is
one of the main challenges in observational Cosmology today. Here, for the
first time, we introduce and test a machine learning-based calibration
framework capable of achieving the precision required for radiometric
experiments aiming to detect the 21-cm line.",['Unknown Author'],"['astro-ph.IM', 'astro-ph.CO', 'cs.AI']",2025-04-23T15:10:25Z,arXiv
2504.16788v1,"Towards Explainable AI: Multi-Modal Transformer for Video-based Image
  Description Generation","Understanding and analyzing video actions are essential for producing
insightful and contextualized descriptions, especially for video-based
applications like intelligent monitoring and autonomous systems. The proposed
work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested
architecture makes use of ResNet50 to extract visual features from video frames
that are taken from the Microsoft Research Video Description Corpus (MSVD), and
Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual
characteristics are converted into patch embeddings and then run through an
encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In
order to align textual and visual representations and guarantee high-quality
description production, the system uses multi-head self-attention and
cross-attention techniques. The model's efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested
framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)
and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores
of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and
0.795 (MSVD). By producing human-like, contextually relevant descriptions,
strengthening interpretability, and improving real-world applications, this
research advances explainable AI.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T15:03:37Z,arXiv
2504.16787v1,Credible plan-driven RAG method for Multi-hop Question Answering,"Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.",['Unknown Author'],"['cs.CL', 'cs.AI', 'I.2.0']",2025-04-23T15:03:17Z,arXiv
2504.16786v1,"MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating
  Over-Smoothing and Incorporating Outlier Scores","Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.",['Unknown Author'],"['cs.CL', 'cs.LG']",2025-04-23T15:02:53Z,arXiv
2504.16778v1,"Evaluation Framework for AI Systems in ""the Wild""","Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.",['Unknown Author'],"['cs.CL', 'cs.AI', 'cs.CY']",2025-04-23T14:52:39Z,arXiv
2504.16768v1,"How Effective are Generative Large Language Models in Performing
  Requirements Classification?","In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.",['Unknown Author'],"['cs.CL', 'cs.AI', 'cs.SE']",2025-04-23T14:41:11Z,arXiv
2504.16767v1,Online model learning with data-assimilated reservoir computers,"We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\""ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\""ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.",['Unknown Author'],"['cs.LG', 'physics.flu-dyn', 'stat.AP']",2025-04-23T14:35:54Z,arXiv
2504.16763v1,Noise-Tolerant Coreset-Based Class Incremental Continual Learning,"Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.",['Unknown Author'],"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE']",2025-04-23T14:34:20Z,arXiv
2504.16760v1,Lightweight Latent Verifiers for Efficient Meta-Generation Strategies,"Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.",['Unknown Author'],['cs.AI'],2025-04-23T14:33:20Z,arXiv
2504.16755v1,"QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization
  Algorithm via Principal Component Analysis","The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.",['Unknown Author'],"['cs.LG', 'cs.ET']",2025-04-23T14:27:31Z,arXiv
2504.16754v1,"HEMA : A Hippocampus-Inspired Extended Memory Architecture for
  Long-Context AI Conversations","Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 &gt;= 0.80 and R@50 &gt;= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T14:27:12Z,arXiv
2504.16748v1,"Simple Graph Contrastive Learning via Fractional-order Neural Diffusion
  Networks","Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.",['Unknown Author'],['cs.LG'],2025-04-23T14:17:28Z,arXiv
2504.16738v1,"MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon
  Manipulation Planning","Planning long-horizon motions using a set of predefined skills is a key
challenge in robotics and AI. Addressing this challenge requires methods that
systematically explore skill combinations to uncover task-solving sequences,
harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize
across unseen tasks, and bypass reliance on symbolic world representations that
demand extensive domain and task-specific knowledge. Despite significant
progress, these elements remain largely disjoint in existing approaches,
leaving a critical gap in achieving robust, scalable solutions for complex,
long-horizon problems. In this work, we present MOSAIC, a skill-centric
framework that unifies these elements by using the skills themselves to guide
the planning process. MOSAIC uses two families of skills: Generators compute
executable trajectories and world configurations, and Connectors link these
independently generated skill trajectories by solving boundary value problems,
enabling progress toward completing the overall task. By breaking away from the
conventional paradigm of incrementally discovering skills from predefined start
or goal states--a limitation that significantly restricts exploration--MOSAIC
focuses planning efforts on regions where skills are inherently effective. We
demonstrate the efficacy of MOSAIC in both simulated and real-world robotic
manipulation tasks, showcasing its ability to solve complex long-horizon
planning problems using a diverse set of skills incorporating generative
diffusion models, motion planning algorithms, and manipulation-specific models.
Visit https://skill-mosaic.github.io for demonstrations and examples.",['Unknown Author'],"['cs.RO', 'cs.AI']",2025-04-23T14:09:42Z,arXiv
2504.16736v1,A Survey of AI Agent Protocols,"The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.",['Unknown Author'],['cs.AI'],2025-04-23T14:07:26Z,arXiv
2504.16732v1,"Simplified Swarm Learning Framework for Robust and Scalable Diagnostic
  Services in Cancer Histopathology","The complexities of healthcare data, including privacy concerns, imbalanced
datasets, and interoperability issues, necessitate innovative machine learning
solutions. Swarm Learning (SL), a decentralized alternative to Federated
Learning, offers privacy-preserving distributed training, but its reliance on
blockchain technology hinders accessibility and scalability. This paper
introduces a \textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}
tailored for resource-constrained environments. By eliminating blockchain
dependencies and adopting lightweight peer-to-peer communication, the proposed
framework ensures robust model synchronization while maintaining data privacy.
Applied to cancer histopathology, the framework integrates optimized
pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,
to improve diagnostic accuracy. Extensive experiments demonstrate the
framework's efficacy in handling imbalanced and biased datasets, achieving
comparable performance to centralized models while preserving privacy. This
study paves the way for democratizing advanced machine learning in healthcare,
offering a scalable, accessible, and efficient solution for privacy-sensitive
diagnostic applications.",['Unknown Author'],"['cs.DC', 'cs.LG']",2025-04-23T14:04:15Z,arXiv
2504.16728v1,"IRIS: Interactive Research Ideation System for Accelerating Scientific
  Discovery","The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System",['Unknown Author'],"['cs.AI', 'cs.CL']",2025-04-23T14:01:36Z,arXiv
2504.16727v1,"V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental
  Visual Variations","Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T14:01:32Z,arXiv
2504.16723v1,"Detecting and Understanding Hateful Contents in Memes Through Captioning
  and Visual Question-Answering","Memes are widely used for humor and cultural commentary, but they are
increasingly exploited to spread hateful content. Due to their multimodal
nature, hateful memes often evade traditional text-only or image-only detection
systems, particularly when they employ subtle or coded references. To address
these challenges, we propose a multimodal hate detection framework that
integrates key components: OCR to extract embedded text, captioning to describe
visual content neutrally, sub-label classification for granular categorization
of hateful content, RAG for contextually relevant retrieval, and VQA for
iterative analysis of symbolic and contextual cues. This enables the framework
to uncover latent signals that simpler pipelines fail to detect. Experimental
results on the Facebook Hateful Memes dataset reveal that the proposed
framework exceeds the performance of unimodal and conventional multimodal
models in both accuracy and AUC-ROC.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T13:52:14Z,arXiv
2504.16722v1,"PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum
  Learning","In computer animation, game design, and human-computer interaction,
synthesizing human motion that aligns with user intent remains a significant
challenge. Existing methods have notable limitations: textual approaches offer
high-level semantic guidance but struggle to describe complex actions
accurately; trajectory-based techniques provide intuitive global motion
direction yet often fall short in generating precise or customized character
movements; and anchor poses-guided methods are typically confined to synthesize
only simple motion patterns. To generate more controllable and precise human
motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel
framework that integrates trajectory guidance with sparse anchor motion
control. Global trajectories ensure consistency in spatial direction and
displacement, while sparse anchor motions only deliver precise action guidance
without displacement. This decoupling enables independent refinement of both
aspects, resulting in a more controllable, high-fidelity, and sophisticated
motion synthesis. ProMoGen supports both dual and single control paradigms
within a unified training process. Moreover, we recognize that direct learning
from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse
Anchor Posture Curriculum Learning)}, a curriculum learning strategy that
progressively adjusts the number of anchors used for guidance, thereby enabling
more precise and stable convergence. Extensive experiments demonstrate that
ProMoGen excels in synthesizing vivid and diverse motions guided by predefined
trajectory and arbitrary anchor frames. Our approach seamlessly integrates
personalized motion with structured guidance, significantly outperforming
state-of-the-art methods across multiple control scenarios.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T13:51:42Z,arXiv
2504.16711v1,"A Unified Retrieval Framework with Document Ranking and EDU Filtering
  for Multi-document Summarization","In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.",['Unknown Author'],"['cs.LG', 'cs.IR']",2025-04-23T13:41:10Z,arXiv
2504.16693v1,"PIN-WM: Learning Physics-INformed World Models for Non-Prehensile
  Manipulation","While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.",['Unknown Author'],"['cs.LG', 'cs.RO']",2025-04-23T13:27:07Z,arXiv
2504.16688v1,"A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation
  for 6G: MLR, ANOVA, and Residual Distribution Analysis","Modeling path loss in indoor LoRaWAN technology deployments is inherently
challenging due to structural obstructions, occupant density and activities,
and fluctuating environmental conditions. This study proposes a two-stage
approach to capture and analyze these complexities using an extensive dataset
of 1,328,334 field measurements collected over six months in a single-floor
office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,
we implement a multiple linear regression model that includes traditional
propagation metrics (distance, structural walls) and an extension with proposed
environmental variables (relative humidity, temperature, carbon dioxide,
particulate matter, and barometric pressure). Using analysis of variance, we
demonstrate that adding these environmental factors can reduce unexplained
variance by 42.32 percent. Secondly, we examine residual distributions by
fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,
Student's t, and Gaussian Mixture Models with one to five components. Our
results show that a four-component Gaussian Mixture Model captures the residual
heterogeneity of indoor signal propagation most accurately, significantly
outperforming single-distribution approaches. Given the push toward
ultra-reliable, context-aware communications in 6G networks, our analysis shows
that environment-aware modeling can substantially improve LoRaWAN network
design in dynamic indoor IoT deployments.",['Unknown Author'],"['cs.NI', 'cs.LG', 'eess.SP']",2025-04-23T13:19:35Z,arXiv
2504.16684v1,"SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting
  Harvest and Storage Characteristics of Sugar Beets","While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.",['Unknown Author'],"['cs.CV', 'cs.LG']",2025-04-23T13:14:03Z,arXiv
2504.16683v1,"MCMC for Bayesian estimation of Differential Privacy from Membership
  Inference Attacks","We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.",['Unknown Author'],"['cs.LG', 'stat.ML']",2025-04-23T13:10:37Z,arXiv
2504.16682v1,Provable wavelet-based neural approximation,"In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.",['Unknown Author'],"['cs.LG', 'math.CA', 'stat.ML']",2025-04-23T13:02:37Z,arXiv
2504.16680v1,"Offline Robotic World Model: Learning Robotic Policies without a Physics
  Simulator","Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.",['Unknown Author'],"['cs.RO', 'cs.AI', 'cs.LG']",2025-04-23T12:58:15Z,arXiv
2504.16677v1,"A Post-trainer's Guide to Multilingual Training Data: Uncovering
  Cross-lingual Transfer Dynamics","In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T12:52:49Z,arXiv
2504.16668v1,"Efficient Data Valuation Approximation in Federated Learning: A
  Sampling-based Approach","Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.",['Unknown Author'],"['cs.LG', 'cs.DB']",2025-04-23T12:36:20Z,arXiv
2504.16667v1,Representation Learning via Non-Contrastive Mutual Information,"Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.",['Unknown Author'],"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML', 'I.2.6; I.2.10']",2025-04-23T12:35:27Z,arXiv
2504.16651v1,"MAYA: Addressing Inconsistencies in Generative Password Guessing through
  a Unified Benchmark","The rapid evolution of generative models has led to their integration across
various fields, including password guessing, aiming to generate passwords that
resemble human-created ones in complexity, structure, and patterns. Despite
generative model's promise, inconsistencies in prior research and a lack of
rigorous evaluation have hindered a comprehensive understanding of their true
potential. In this paper, we introduce MAYA, a unified, customizable,
plug-and-play password benchmarking framework. MAYA provides a standardized
approach for evaluating generative password-guessing models through a rigorous
set of advanced testing scenarios and a collection of eight real-life password
datasets. Using MAYA, we comprehensively evaluate six state-of-the-art
approaches, which have been re-implemented and adapted to ensure
standardization, for a total of over 15,000 hours of computation. Our findings
indicate that these models effectively capture different aspects of human
password distribution and exhibit strong generalization capabilities. However,
their effectiveness varies significantly with long and complex passwords.
Through our evaluation, sequential models consistently outperform other
generative architectures and traditional password-guessing tools, demonstrating
unique capabilities in generating accurate and complex guesses. Moreover,
models learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By releasing
MAYA, we aim to foster further research, providing the community with a new
tool to consistently and reliably benchmark password-generation techniques. Our
framework is publicly available at
https://github.com/williamcorrias/MAYA-Password-Benchmarking",['Unknown Author'],"['cs.CR', 'cs.AI', 'cs.LG']",2025-04-23T12:16:59Z,arXiv
2504.16640v1,"SSLR: A Semi-Supervised Learning Method for Isolated Sign Language
  Recognition","Sign language is the primary communication language for people with disabling
hearing loss. Sign language recognition (SLR) systems aim to recognize sign
gestures and translate them into spoken language. One of the main challenges in
SLR is the scarcity of annotated datasets. To address this issue, we propose a
semi-supervised learning (SSL) approach for SLR (SSLR), employing a
pseudo-label method to annotate unlabeled samples. The sign gestures are
represented using pose information that encodes the signer's skeletal joint
points. This information is used as input for the Transformer backbone model
utilized in the proposed approach. To demonstrate the learning capabilities of
SSL across various labeled data sizes, several experiments were conducted using
different percentages of labeled data with varying numbers of classes. The
performance of the SSL approach was compared with a fully supervised
learning-based model on the WLASL-100 dataset. The obtained results of the SSL
model outperformed the supervised learning-based model with less labeled data
in many cases.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T11:59:52Z,arXiv
2504.16639v1,"DAPLSR: Data Augmentation Partial Least Squares Regression Model via
  Manifold Optimization","Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.",['Unknown Author'],"['cs.LG', 'cs.IR']",2025-04-23T11:58:28Z,arXiv
2504.16635v1,"Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning
  and GARCH Models","In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.",['Unknown Author'],"['cs.AI', 'q-fin.CP', 'q-fin.RM', 'q-fin.ST']",2025-04-23T11:54:22Z,arXiv
2504.16628v1,"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language
  Models using Pareto High-quality Data","Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.",['Unknown Author'],"['cs.LG', 'cs.CL']",2025-04-23T11:35:57Z,arXiv
2504.16624v1,"Compositional Active Learning of Synchronous Systems through Automated
  Alphabet Refinement","Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.",['Unknown Author'],"['cs.LG', 'cs.FL']",2025-04-23T11:30:01Z,arXiv
2504.16622v1,"Cognitive Silicon: An Architectural Blueprint for Post-Industrial
  Computing Systems","Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.",['Unknown Author'],"['cs.AI', 'cs.CY']",2025-04-23T11:24:30Z,arXiv
2504.16612v1,"Federated EndoViT: Pretraining Vision Transformers via Federated
  Learning on Endoscopic Image Collections","Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.",['Unknown Author'],"['cs.CV', 'cs.LG']",2025-04-23T10:54:32Z,arXiv
2504.16604v1,"Debunking with Dialogue? Exploring AI-Generated Counterspeech to
  Challenge Conspiracy Theories","Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.",['Unknown Author'],"['cs.CL', 'cs.AI', 'cs.SI', 'I.2.7']",2025-04-23T10:32:45Z,arXiv
2504.16601v1,"Comparing Large Language Models and Traditional Machine Translation
  Tools for Translating Medical Consultation Summaries: A Pilot Study","This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T10:31:33Z,arXiv
2504.16595v1,HERB: Human-augmented Efficient Reinforcement learning for Bin-packing,"Packing objects efficiently is a fundamental problem in logistics, warehouse
automation, and robotics. While traditional packing solutions focus on
geometric optimization, packing irregular, 3D objects presents significant
challenges due to variations in shape and stability. Reinforcement
Learning~(RL) has gained popularity in robotic packing tasks, but training
purely from simulation can be inefficient and computationally expensive. In
this work, we propose HERB, a human-augmented RL framework for packing
irregular objects. We first leverage human demonstrations to learn the best
sequence of objects to pack, incorporating latent factors such as space
optimization, stability, and object relationships that are difficult to model
explicitly. Next, we train a placement algorithm that uses visual information
to determine the optimal object positioning inside a packing container. Our
approach is validated through extensive performance evaluations, analyzing both
packing efficiency and latency. Finally, we demonstrate the real-world
feasibility of our method on a robotic system. Experimental results show that
our method outperforms geometric and purely RL-based approaches by leveraging
human intuition, improving both packing robustness and adaptability. This work
highlights the potential of combining human expertise-driven RL to tackle
complex real-world packing challenges in robotic systems.",['Unknown Author'],"['cs.RO', 'cs.LG']",2025-04-23T10:24:36Z,arXiv
2504.16588v1,"Data-Assimilated Model-Based Reinforcement Learning for Partially
  Observed Chaotic Flows","The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.",['Unknown Author'],"['eess.SY', 'cs.LG', 'cs.SY', 'physics.flu-dyn']",2025-04-23T10:12:53Z,arXiv
2504.16585v1,"Enhancing Variable Selection in Large-scale Logistic Regression:
  Leveraging Manual Labeling with Beneficial Noise","In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.",['Unknown Author'],"['cs.LG', 'stat.CO', 'stat.ML']",2025-04-23T10:05:54Z,arXiv
2504.16584v1,"Case Study: Fine-tuning Small Language Models for Accurate and Private
  CWE Detection in Python Code","Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and analyzing code for security vulnerabilities, such as Common
Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure
and substantial computational requirements pose challenges for analyzing
sensitive or proprietary codebases due to privacy concerns and inference costs.
This work explores the potential of Small Language Models (SLMs) as a viable
alternative for accurate, on-premise vulnerability detection. We investigated
whether a 350-million parameter pre-trained code model (codegen-mono) could be
effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within
Python code. To facilitate this, we developed a targeted dataset of 500
examples using a semi-supervised approach involving LLM-driven synthetic data
generation coupled with meticulous human review. Initial tests confirmed that
the base codegen-mono model completely failed to identify CWEs in our samples.
However, after applying instruction-following fine-tuning, the specialized SLM
achieved remarkable performance on our test set, yielding approximately 99%
accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results
strongly suggest that fine-tuned SLMs can serve as highly accurate and
efficient tools for CWE detection, offering a practical and privacy-preserving
solution for integrating advanced security analysis directly into development
workflows.",['Unknown Author'],"['cs.CR', 'cs.AI']",2025-04-23T10:05:27Z,arXiv
2504.16580v1,Hyper-Transforming Latent Diffusion Models,"We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.",['Unknown Author'],"['cs.LG', 'stat.ML']",2025-04-23T10:01:18Z,arXiv
2504.16576v1,MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation,"The burgeoning presence of multimodal content-sharing platforms propels the
development of personalized recommender systems. Previous works usually suffer
from data sparsity and cold-start problems, and may fail to adequately explore
semantic user-product associations from multimodal data. To address these
issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)
framework for user recommendation. For a comprehensive information exploration
from user-product relations, we construct two hypergraphs, i.e. a user-to-user
(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared
preferences among users and intricate multimodal semantic resemblance among
items, respectively. This process yields denser second-order semantics that are
fused with first-order user-item interaction as complementary to alleviate the
data sparsity issue. Then, we design a contrastive feature enhancement paradigm
by applying synergistic contrastive learning. By maximizing/minimizing the
mutual information between second-order (e.g. shared preference pattern for
users) and first-order (information of selected items for users) embeddings of
the same/different users and items, the feature distinguishability can be
effectively enhanced. Compared with using sparse primary user-item interaction
only, our MMHCL obtains denser second-order hypergraphs and excavates more
abundant shared attributes to explore the user-product associations, which to a
certain extent alleviates the problems of data sparsity and cold-start.
Extensive experiments have comprehensively demonstrated the effectiveness of
our method. Our code is publicly available at: https://github.com/Xu107/MMHCL.",['Unknown Author'],"['cs.IR', 'cs.AI']",2025-04-23T09:58:54Z,arXiv
2504.16574v1,"PIS: Linking Importance Sampling and Attention Mechanisms for Efficient
  Prompt Compression","Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T09:53:01Z,arXiv
2504.16573v1,"PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling
  Assistant System","Psychological counseling is a highly personalized and dynamic process that
requires therapists to continuously monitor emotional changes, document session
insights, and maintain therapeutic continuity. In this paper, we introduce
PsyCounAssist, a comprehensive AI-powered counseling assistant system
specifically designed to augment psychological counseling practices.
PsyCounAssist integrates multimodal emotion recognition combining speech and
photoplethysmography (PPG) signals for accurate real-time affective analysis,
automated structured session reporting using large language models (LLMs), and
personalized AI-generated follow-up support. Deployed on Android-based tablet
devices, the system demonstrates practical applicability and flexibility in
real-world counseling scenarios. Experimental evaluation confirms the
reliability of PPG-based emotional classification and highlights the system's
potential for non-intrusive, privacy-aware emotional support. PsyCounAssist
represents a novel approach to ethically and effectively integrating AI into
psychological counseling workflows.",['Unknown Author'],"['cs.HC', 'cs.AI']",2025-04-23T09:49:05Z,arXiv
2504.16562v1,"A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and
  Environments","Augmented Reality (AR) is transforming the way we interact with virtual
information in the physical world. By overlaying digital content in real-world
environments, AR enables new forms of immersive and engaging experiences.
However, existing AR systems often struggle to effectively manage the many
interactive possibilities that AR presents. This vision paper speculates on
AI-driven approaches for adaptive AR content placement, dynamically adjusting
to user movement and environmental changes. By leveraging machine learning
methods, such a system would intelligently manage content distribution between
AR projections integrated into the external environment and fixed static
content, enabling seamless UI layout and potentially reducing users' cognitive
load. By exploring the possibilities of AI-driven dynamic AR content placement,
we aim to envision new opportunities for innovation and improvement in various
industries, from urban navigation and workplace productivity to immersive
learning and beyond. This paper outlines a vision for the development of more
intuitive, engaging, and effective AI-powered AR experiences.",['Unknown Author'],"['cs.HC', 'cs.AI']",2025-04-23T09:42:38Z,arXiv
2504.16559v1,Unified Molecule Generation and Property Prediction,"Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.",['Unknown Author'],"['cs.LG', 'q-bio.QM', '68T01', 'I.2.1']",2025-04-23T09:36:46Z,arXiv
2504.16555v1,Confidence Sequences for Generalized Linear Models via Regret Analysis,"We develop a methodology for constructing confidence sets for parameters of
statistical models via a reduction to sequential prediction. Our key
observation is that for any generalized linear model (GLM), one can construct
an associated game of sequential probability assignment such that achieving low
regret in the game implies a high-probability upper bound on the excess
likelihood of the true parameter of the GLM. This allows us to develop a scheme
that we call online-to-confidence-set conversions, which effectively reduces
the problem of proving the desired statistical claim to an algorithmic
question. We study two varieties of this conversion scheme: 1) analytical
conversions that only require proving the existence of algorithms with low
regret and provide confidence sets centered at the maximum-likelihood estimator
2) algorithmic conversions that actively leverage the output of the online
algorithm to construct confidence sets (and may be centered at other,
adaptively constructed point estimators). The resulting methodology recovers
all state-of-the-art confidence set constructions within a single framework,
and also provides several new types of confidence sets that were previously
unknown in the literature.",['Unknown Author'],"['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2025-04-23T09:32:40Z,arXiv
2504.16553v1,"Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs
  in Acoustic Wavefield Simulations","Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.",['Unknown Author'],"['cs.LG', 'physics.comp-ph', 'physics.geo-ph']",2025-04-23T09:32:14Z,arXiv
2504.16548v1,"Exploring human-SAV interaction using large language models: The impact
  of psychological ownership and anthropomorphism on user experience","There has been extensive prior work exploring how psychological factors such
as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).
However, limited research has been conducted on how prompt strategies in large
language model (LLM)-powered SAV User Interfaces (UIs) affect users'
perceptions, experiences, and intentions to adopt such technology. In this
work, we investigate how conversational UIs powered by LLMs drive these
psychological factors and psychological ownership, the sense of possession a
user may come to feel towards an entity or object they may not legally own. We
designed four SAV UIs with varying levels of anthropomorphic characteristics
and psychological ownership triggers. Quantitative measures of psychological
ownership, anthropomorphism, quality of service, disclosure tendency, sentiment
of SAV responses, and overall acceptance were collected after participants
interacted with each SAV. Qualitative feedback was also gathered regarding the
experience of psychological ownership during the interactions. The results
indicate that an SAV conversational UI designed to be more anthropomorphic and
to induce psychological ownership improved users' perceptions of the SAV's
human-like qualities and improved the sentiment of responses compared to a
control condition. These findings provide practical guidance for designing
LLM-based conversational UIs that enhance user experience and adoption of SAVs.",['Unknown Author'],"['cs.HC', 'cs.AI', 'cs.ET']",2025-04-23T09:25:22Z,arXiv
2504.16538v1,"Streetscape Analysis with Generative AI (SAGAI): Vision-Language
  Assessment and Mapping of Urban Scenes","Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.",['Unknown Author'],"['cs.CV', 'cs.LG', 'I.2; I.4; J.4']",2025-04-23T09:08:06Z,arXiv
2504.16537v1,Transformers for Complex Query Answering over Knowledge Hypergraphs,"Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T09:07:21Z,arXiv
2504.16516v1,"Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion
  and Reasoning for Vision-and-Language Navigation","Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T08:41:27Z,arXiv
2504.16515v1,"Federated Learning of Low-Rank One-Shot Image Detection Models in Edge
  Devices with Scalable Accuracy and Compute Complexity","This paper introduces a novel federated learning framework termed LoRa-FL
designed for training low-rank one-shot image detection models deployed on edge
devices. By incorporating low-rank adaptation techniques into one-shot
detection architectures, our method significantly reduces both computational
and communication overhead while maintaining scalable accuracy. The proposed
framework leverages federated learning to collaboratively train lightweight
image recognition models, enabling rapid adaptation and efficient deployment
across heterogeneous, resource-constrained devices. Experimental evaluations on
the MNIST and CIFAR10 benchmark datasets, both in an
independent-and-identically-distributed (IID) and non-IID setting, demonstrate
that our approach achieves competitive detection performance while
significantly reducing communication bandwidth and compute complexity. This
makes it a promising solution for adaptively reducing the communication and
compute power overheads, while not sacrificing model accuracy.",['Unknown Author'],"['cs.CV', 'cs.AI']",2025-04-23T08:40:44Z,arXiv
2504.16506v1,A Comprehensive Survey of Synthetic Tabular Data Generation,"Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.",['Unknown Author'],['cs.LG'],2025-04-23T08:33:34Z,arXiv
2504.16503v1,Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression,"Symbolic regression is a technique that can automatically derive analytic
models from data. Traditionally, symbolic regression has been implemented
primarily through genetic programming that evolves populations of candidate
solutions sampled by genetic operators, crossover and mutation. More recently,
neural networks have been employed to learn the entire analytical model, i.e.,
its structure and coefficients, using regularized gradient-based optimization.
Although this approach tunes the model's coefficients better, it is prone to
premature convergence to suboptimal model structures. Here, we propose a
neuro-evolutionary symbolic regression method that combines the strengths of
evolutionary-based search for optimal neural network (NN) topologies with
gradient-based tuning of the network's parameters. Due to the inherent high
computational demand of evolutionary algorithms, it is not feasible to learn
the parameters of every candidate NN topology to full convergence. Thus, our
method employs a memory-based strategy and population perturbations to enhance
exploitation and reduce the risk of being trapped in suboptimal NNs. In this
way, each NN topology can be trained using only a short sequence of
backpropagation iterations. The proposed method was experimentally evaluated on
three real-world test problems and has been shown to outperform other NN-based
approaches regarding the quality of the models obtained.",['Unknown Author'],"['cs.NE', 'cs.LG']",2025-04-23T08:29:53Z,arXiv
2504.16501v1,Dynamic Time-aware Continual User Representation Learning,"Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.",['Unknown Author'],['cs.LG'],2025-04-23T08:23:59Z,arXiv
2504.16493v1,"Breaking scaling relations with inverse catalysts: a machine learning
  exploration of trends in $\mathrm{CO_2}$ hydrogenation energy barriers","The conversion of $\mathrm{CO_2}$ into useful products such as methanol is a
key strategy for abating climate change and our dependence on fossil fuels.
Developing new catalysts for this process is costly and time-consuming and can
thus benefit from computational exploration of possible active sites. However,
this is complicated by the complexity of the materials and reaction networks.
Here, we present a workflow for exploring transition states of elementary
reaction steps at inverse catalysts, which is based on the training of a neural
network-based machine learning interatomic potential. We focus on the crucial
formate intermediate and its formation over nanoclusters of indium oxide
supported on Cu(111). The speedup compared to an approach purely based on
density functional theory allows us to probe a wide variety of active sites
found at nanoclusters of different sizes and stoichiometries. Analysis of the
obtained set of transition state geometries reveals different
structure--activity trends at the edge or interior of the nanoclusters.
Furthermore, the identified geometries allow for the breaking of linear scaling
relations, which could be a key underlying reason for the excellent catalytic
performance of inverse catalysts observed in experiments.",['Unknown Author'],"['cond-mat.mtrl-sci', 'cs.LG', 'physics.chem-ph']",2025-04-23T08:12:47Z,arXiv
2504.16489v1,"Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based
  Multi-Agent Debate","Multi-Agent Debate (MAD), leveraging collaborative interactions among Large
Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks.
However, the security implications of their iterative dialogues and
role-playing characteristics, particularly susceptibility to jailbreak attacks
eliciting harmful content, remain critically underexplored. This paper
systematically investigates the jailbreak vulnerabilities of four prominent MAD
frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,
and DeepSeek) without compromising internal agents. We introduce a novel
structured prompt-rewriting framework specifically designed to exploit MAD
dynamics via narrative encapsulation, role-driven escalation, iterative
refinement, and rhetorical obfuscation. Our extensive experiments demonstrate
that MAD systems are inherently more vulnerable than single-agent setups.
Crucially, our proposed attack methodology significantly amplifies this
fragility, increasing average harmfulness from 28.14% to 80.34% and achieving
attack success rates as high as 80% in certain scenarios. These findings reveal
intrinsic vulnerabilities in MAD architectures and underscore the urgent need
for robust, specialized defenses prior to real-world deployment.",['Unknown Author'],"['cs.CR', 'cs.AI']",2025-04-23T08:01:50Z,arXiv
2504.16485v1,"On Developers' Self-Declaration of AI-Generated Code: An Analysis of
  Practices","AI code generation tools have gained significant popularity among developers,
who use them to assist in software development due to their capability to
generate code. Existing studies mainly explored the quality, e.g., correctness
and security, of AI-generated code, while in real-world software development,
the prerequisite is to distinguish AI-generated code from human-written code,
which emphasizes the need to explicitly declare AI-generated code by
developers. To this end, this study intends to understand the ways developers
use to self-declare AI-generated code and explore the reasons why developers
choose to self-declare or not. We conducted a mixed-methods study consisting of
two phases. In the first phase, we mined GitHub repositories and collected 613
instances of AI-generated code snippets. In the second phase, we conducted a
follow-up industrial survey, which received 111 valid responses. Our research
revealed the practices followed by developers to self-declare AI-generated
code. Most practitioners (76.6%) always or sometimes self-declare AI-generated
code. In contrast, other practitioners (23.4%) noted that they never
self-declare AI-generated code. The reasons for self-declaring AI-generated
code include the need to track and monitor the code for future review and
debugging, and ethical considerations. The reasons for not self-declaring
AI-generated code include extensive modifications to AI-generated code and the
developers' perception that self-declaration is an unnecessary activity. We
finally provided guidelines for practitioners to self-declare AI-generated
code, addressing ethical and code quality concerns.",['Unknown Author'],"['cs.SE', 'cs.AI']",2025-04-23T07:52:39Z,arXiv
2504.16479v1,The Dance of Atoms-De Novo Protein Design with Diffusion Model,"The de novo design of proteins refers to creating proteins with specific
structures and functions that do not naturally exist. In recent years, the
accumulation of high-quality protein structure and sequence data and
technological advancements have paved the way for the successful application of
generative artificial intelligence (AI) models in protein design. These models
have surpassed traditional approaches that rely on fragments and
bioinformatics. They have significantly enhanced the success rate of de novo
protein design, and reduced experimental costs, leading to breakthroughs in the
field. Among various generative AI models, diffusion models have yielded the
most promising results in protein design. In the past two to three years, more
than ten protein design models based on diffusion models have emerged. Among
them, the representative model, RFDiffusion, has demonstrated success rates in
25 protein design tasks that far exceed those of traditional methods, and other
AI-based approaches like RFjoint and hallucination. This review will
systematically examine the application of diffusion models in generating
protein backbones and sequences. We will explore the strengths and limitations
of different models, summarize successful cases of protein design using
diffusion models, and discuss future development directions.",['Unknown Author'],"['q-bio.BM', 'cs.AI']",2025-04-23T07:45:00Z,arXiv
2504.16474v1,"Seeking Flat Minima over Diverse Surrogates for Improved Adversarial
  Transferability: A Theoretical Framework and Algorithmic Instantiation","The transfer-based black-box adversarial attack setting poses the challenge
of crafting an adversarial example (AE) on known surrogate models that remain
effective against unseen target models. Due to the practical importance of this
task, numerous methods have been proposed to address this challenge. However,
most previous methods are heuristically designed and intuitively justified,
lacking a theoretical foundation. To bridge this gap, we derive a novel
transferability bound that offers provable guarantees for adversarial
transferability. Our theoretical analysis has the advantages of \textit{(i)}
deepening our understanding of previous methods by building a general attack
framework and \textit{(ii)} providing guidance for designing an effective
attack algorithm. Our theoretical results demonstrate that optimizing AEs
toward flat minima over the surrogate model set, while controlling the
surrogate-target model shift measured by the adversarial model discrepancy,
yields a comprehensive guarantee for AE transferability. The results further
lead to a general transfer-based attack framework, within which we observe that
previous methods consider only partial factors contributing to the
transferability. Algorithmically, inspired by our theoretical results, we first
elaborately construct the surrogate model set in which models exhibit diverse
adversarial vulnerabilities with respect to AEs to narrow an instantiated
adversarial model discrepancy. Then, a \textit{model-Diversity-compatible
Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote
the flatness of AEs over diverse surrogate models to improve transferability.
Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target
models demonstrate the effectiveness of our proposed attack.",['Unknown Author'],"['cs.CR', 'cs.LG']",2025-04-23T07:33:45Z,arXiv
2504.16472v1,"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:
  Open Research Challenges","Despite decades of research and practice in automated software testing,
several fundamental concepts remain ill-defined and under-explored, yet offer
enormous potential real-world impact. We show that these concepts raise
exciting new challenges in the context of Large Language Models for software
test generation. More specifically, we formally define and investigate the
properties of hardening and catching tests. A hardening test is one that seeks
to protect against future regressions, while a catching test is one that
catches such a regression or a fault in new functionality introduced by a code
change. Hardening tests can be generated at any time and may become catching
tests when a future regression is caught. We also define and motivate the
Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated
`just-in-time' to catch new faults before they land into production. We show
that any solution to Catching JiTTest generation can also be repurposed to
catch latent faults in legacy code. We enumerate possible outcomes for
hardening and catching tests and JiTTests, and discuss open research problems,
deployment options, and initial results from our work on automated LLM-based
hardening at Meta. This paper\footnote{Author order is alphabetical. The
corresponding author is Mark Harman.} was written to accompany the keynote by
the authors at the ACM International Conference on the Foundations of Software
Engineering (FSE) 2025.",['Unknown Author'],"['cs.SE', 'cs.AI']",2025-04-23T07:32:43Z,arXiv
2504.16464v1,"ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree
  and Visual Guidance","While recent advancements in robotic manipulation video synthesis have shown
promise, significant challenges persist in ensuring effective
instruction-following and achieving high visual quality. Recent methods, like
RoboDreamer, utilize linguistic decomposition to divide instructions into
separate lower-level primitives, conditioning the world model on these
primitives to achieve compositional instruction-following. However, these
separate primitives do not consider the relationships that exist between them.
Furthermore, recent methods neglect valuable visual guidance, including depth
and semantic guidance, both crucial for enhancing visual quality. This paper
introduces ManipDreamer, an advanced world model based on the action tree and
visual guidance. To better learn the relationships between instruction
primitives, we represent the instruction as the action tree and assign
embeddings to tree nodes, each instruction can acquire its embeddings by
navigating through the action tree. The instruction embeddings can be used to
guide the world model. To enhance visual quality, we combine depth and semantic
guidance by introducing a visual guidance adapter compatible with the world
model. This visual adapter enhances both the temporal and physical consistency
of video generation. Based on the action tree and visual guidance, ManipDreamer
significantly boosts the instruction-following ability and visual quality.
Comprehensive evaluations on robotic manipulation benchmarks reveal that
ManipDreamer achieves large improvements in video quality metrics in both seen
and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from
0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,
compared to the recent RoboDreamer model. Additionally, our method increases
the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on
average.",['Unknown Author'],"['cs.RO', 'cs.AI']",2025-04-23T07:23:41Z,arXiv
2504.16460v1,"T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic
  Understanding via Deep Triplet Loss Fine-Tuning","The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.",['Unknown Author'],"['cs.CL', 'cs.AI', '68T50']",2025-04-23T07:10:37Z,arXiv
2504.16450v1,An Effective Gram Matrix Characterizes Generalization in Deep Networks,"We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.",['Unknown Author'],"['cs.LG', 'stat.ML']",2025-04-23T06:24:42Z,arXiv
2504.16449v1,"From Past to Present: A Survey of Malicious URL Detection Techniques,
  Datasets and Code Repositories","Malicious URLs persistently threaten the cybersecurity ecosystem, by either
deceiving users into divulging private data or distributing harmful payloads to
infiltrate host systems. Gaining timely insights into the current state of this
ongoing battle holds significant importance. However, existing reviews exhibit
4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures
understanding of how detection approaches exploit specific modal information
channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;
3) No open-source implementations are collected to facilitate benchmarking; 4)
Insufficient dataset coverage.This paper presents a comprehensive review of
malicious URL detection technologies, systematically analyzing methods from
traditional blacklisting to advanced deep learning approaches (e.g.
Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
modality-based taxonomy that categorizes existing works according to their
primary data modalities (URL, HTML, Visual, etc.). This hierarchical
classification enables both rigorous technical analysis and clear understanding
of multimodal information utilization. Furthermore, to establish a profile of
accessible datasets and address the lack of standardized benchmarking (where
current studies often lack proper baseline comparisons), we curate and analyze:
1) publicly available datasets (2016-2024), and 2) open-source implementations
from published works(2013-2025). Then, we outline essential design principles
and architectural frameworks for product-level implementations. The review
concludes by examining emerging challenges and proposing actionable directions
for future research. We maintain a GitHub repository for ongoing curating
datasets and open-source implementations:
https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.",['Unknown Author'],"['cs.CR', 'cs.LG']",2025-04-23T06:23:18Z,arXiv
2504.16448v1,"EMRModel: A Large Language Model for Extracting Medical Consultation
  Dialogues into Structured Medical Records","Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.",['Unknown Author'],"['cs.CL', 'cs.AI']",2025-04-23T06:17:55Z,arXiv
2504.16447v1,"Node Assigned physics-informed neural networks for thermal-hydraulic
  system simulation: CVH/FL module","Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.",['Unknown Author'],['cs.LG'],2025-04-23T06:17:04Z,arXiv
2504.16438v1,Private Federated Learning using Preference-Optimized Synthetic Data,"In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.",['Unknown Author'],"['cs.LG', 'cs.AI', 'cs.CR', 'cs.DC']",2025-04-23T05:57:20Z,arXiv
2504.16432v1,"iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold
  Network","As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.",['Unknown Author'],"['cs.LG', 'cs.AI']",2025-04-23T05:34:49Z,arXiv
2504.16431v1,"Target Concrete Score Matching: A Holistic Framework for Discrete
  Diffusion","Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.",['Unknown Author'],['cs.LG'],2025-04-23T05:32:58Z,arXiv
2504.16430v1,MAGIC: Near-Optimal Data Attribution for Deep Learning,"The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.",['Unknown Author'],"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']",2025-04-23T05:32:37Z,arXiv
2504.16427v1,"Can Large Language Models Help Multimodal Language Analysis? MMLA: A
  Comprehensive Benchmark","Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.",['Unknown Author'],"['cs.CL', 'cs.AI', 'cs.MM']",2025-04-23T05:25:13Z,arXiv
2504.16420v1,"A Survey of Foundation Model-Powered Recommender Systems: From
  Feature-Based, Generative to Agentic Paradigms","Recommender systems (RS) have become essential in filtering information and
personalizing content for users. RS techniques have traditionally relied on
modeling interactions between users and items as well as the features of
content using models specific to each task. The emergence of foundation models
(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA
and CLIP, is reshaping the recommendation paradigm. This survey provides a
comprehensive overview of the Foundation Models for Recommender Systems
(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based
augmentation of representations, (2) Generative recommendation approaches, and
(3) Agentic interactive systems. We first review the data foundations of RS,
from traditional explicit or implicit feedback to multimodal content sources.
We then introduce FMs and their capabilities for representation learning,
natural language understanding, and multi-modal reasoning in RS contexts. The
core of the survey discusses how FMs enhance RS under different paradigms.
Afterward, we examine FM applications in various recommendation tasks. Through
an analysis of recent research, we highlight key opportunities that have been
realized as well as challenges encountered. Finally, we outline open research
directions and technical challenges for next-generation FM4RecSys. This survey
not only reviews the state-of-the-art methods but also provides a critical
analysis of the trade-offs among the feature-based, the generative, and the
agentic paradigms, outlining key open issues and future research directions.",['Unknown Author'],"['cs.IR', 'cs.AI']",2025-04-23T05:02:51Z,arXiv
2504.16419v1,PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels,"Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.",['Unknown Author'],"['cs.CV', 'cs.AI', 'cs.HC']",2025-04-23T05:01:25Z,arXiv
