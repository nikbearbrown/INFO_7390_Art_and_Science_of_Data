{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Generative AI for Data Generation\n",
    "\n",
    "Author: Yamini Manral (manral.y@northeastern.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we delve into the development of a PDF bot powered by the Flan-T5 language model. This innovative bot seamlessly consumes PDF documents and provides insightful responses to user queries related to their content. Harnessing the Flan-T5 model's advanced natural language processing capabilities, we construct a bot capable of effectively comprehending and interacting with PDF-based information, enabling users to pose natural language questions about PDF documents and receive insightful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Document Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document analysis or PDF bot refers to the use of automated systems or bots to analyze and extract information from documents, particularly those in PDF format. These bots are designed to understand and process the content within documents, extracting relevant data, and performing various tasks such as text extraction, summarization, question-answering, and more. The primary goal is to make sense of the information within documents in a structured and efficient manner.\n",
    "\n",
    "Key components of a document analysis or PDF bot may include:\n",
    "\n",
    "1. **Document Loading:** The bot can load documents from various sources, such as local files, online repositories, or document-sharing platforms.\n",
    "\n",
    "2. **Text Extraction:** Extracting text from documents is a fundamental step. Bots use techniques to convert the content within documents, especially PDFs, into machine-readable text.\n",
    "\n",
    "3. **Text Splitting:** For large documents, bots may employ strategies to split the text into manageable chunks, facilitating more effective analysis.\n",
    "\n",
    "4. **Text Processing:** Bots can perform natural language processing tasks on the extracted text, such as sentiment analysis, entity recognition, or language translation.\n",
    "\n",
    "5. **Generative AI Techniques:** Some document bots may utilize generative AI techniques to generate relevant and context-aware responses based on queries or prompts.\n",
    "\n",
    "6. **Vector Embeddings:** Embedding techniques, like vector embeddings, can be employed to represent the semantic meaning of text and facilitate similarity searches.\n",
    "\n",
    "7. **Retrieval Mechanisms:** Bots may include retrieval mechanisms to find and present relevant documents or sections based on user queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications and Use Cases:** Document analysis and PDF bots find applications in various domains:\n",
    "\n",
    "1. **Information Retrieval:** Used to quickly search, retrieve, and summarize information from large document collections.\n",
    "\n",
    "2. **Legal and Compliance:** Bots can assist in legal research, contract analysis, and compliance monitoring by extracting and summarizing relevant legal information.\n",
    "\n",
    "3. **Knowledge Management:** Helps organize and manage knowledge by extracting key insights from documents.\n",
    "\n",
    "4. **Customer Support:** Bots can assist in responding to customer queries by extracting relevant information from documents or manuals.\n",
    "\n",
    "5. **Data Augmentation:** Used to generate synthetic data for training machine learning models, enhancing datasets for improved model performance.\n",
    "\n",
    "6. **Research and Development:** Supports researchers in reviewing and summarizing research papers, articles, and technical documents.\n",
    "\n",
    "7. **Human Resources:** Assists in processing resumes, extracting relevant information, and automating aspects of recruitment processes.\n",
    "\n",
    "**Theoretical Underpinnings:**\n",
    "- **GPT-3.5 Turbo:** This model is based on the Transformer architecture, which uses attention mechanisms to capture contextual relationships in data.\n",
    "- **T5 Models:** These models utilize the \"Text-to-Text\" framework, treating all NLP tasks as converting input text to target text, providing a unified approach to various language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Concise overview of the data generation process using generative AI. Detail the context, significance, and principles behind data generation\n",
    "We will use our PDF bot to read the input PDF and then we will try to query the bot about the PDF to get answers about it's contents.\n",
    "\n",
    "**Context:**\n",
    "- Leveraging generative AI for data synthesis.\n",
    "- Creating synthetic data mimicking real-world scenarios.\n",
    "\n",
    "**Significance:**\n",
    "- Augmenting existing datasets for machine learning.\n",
    "- Providing diverse and extensive training data.\n",
    "  \n",
    "**Principles:**\n",
    "- Ensuring data diversity.\n",
    "- Upholding ethical considerations.\n",
    "- Enhancing algorithm robustness through varied data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the chosen generative AI technique and its purpose in data generation\n",
    "\n",
    "**Generative AI Technique:**\n",
    "- Utilizing the Flan T5 Base model.\n",
    "\n",
    "**Purpose:**\n",
    "- Text-to-text generation for synthetic data creation.\n",
    "- Reproducing linguistic patterns from input data.\n",
    "  \n",
    "**Model Characteristics:**\n",
    "- Coherent and contextually relevant responses.\n",
    "- Contribution to dataset augmentation in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and Engaging with the Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the document \"DANNY SMITH.pdf\" as an input file and tried to query the PDF bot.\n",
    "\n",
    "**Data Characteristics:**\n",
    "- The generated data consists of textual responses to user queries about PDF documents.\n",
    "- It captures semantic understanding, context, and relevant information from the PDF content.\n",
    "\n",
    "![img1](./SS/6.png)\n",
    "\n",
    "![img1](./SS/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application Areas:**\n",
    "- Answering user queries about PDF content.\n",
    "- Enhancing the performance of data-driven applications that require natural language understanding.\n",
    "\n",
    "**Analytical Insights:**\n",
    "- The generated data provides insights into the language understanding capabilities of the models.\n",
    "- It enables the extraction of meaningful information from unstructured text.\n",
    "\n",
    "**Queries and Exploration:**\n",
    "- Engage with the generative AI by querying it about the document content.\n",
    "- Explore different scenarios to understand the model's responses under various contexts.\n",
    "- Validate the quality and diversity of generated responses.\n",
    "\n",
    "Another query made to the same PDF:\n",
    "\n",
    "![img1](./SS/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how FLAN T5 model generates data: A high-level overview of how the Flan T5 Base model generates data:\n",
    "\n",
    "1. **Text-to-Text Framework:** T5 is designed with a unified framework for various natural language processing tasks. Both input and output are treated as text, allowing the model to handle a wide range of tasks, including text generation.\n",
    "\n",
    "2. **Prompt-Based Generation:** The model is typically prompted with a specific instruction or question in the form of input text. This prompt guides the model on what type of data to generate.\n",
    "   \n",
    "3. **Context Understanding:** The Flan T5 Base model utilizes its pre-trained knowledge to understand the context provided in the prompt. This involves capturing the relationships between words, phrases, and the overall structure of the input.\n",
    "\n",
    "4. **Conditional Generation:** Based on the input prompt, the model conditions its internal parameters to generate a relevant and coherent output. The output is generated in a text format.\n",
    "\n",
    "5. **Variable Outputs:** The Flan T5 Base model is capable of generating variable-length outputs. The length and complexity of the generated text depend on the nature of the input prompt and the task it is instructed to perform.\n",
    "\n",
    "6. **Task Adaptability:** One of the strengths of the T5 architecture, including Flan T5 Base, is its adaptability to various tasks. It can handle tasks such as summarization, translation, question answering, and text generation, making it versatile for different applications.\n",
    "\n",
    "7. **Fine-Tuning for Specific Tasks:** While pre-trained on a diverse range of tasks, the model can be fine-tuned on specific datasets to specialize in certain domains or applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crafting the Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generation Task:** Document analysis and PDF bots contribute to increased efficiency, accuracy, and automation in handling large volumes of textual information across diverse industries and sectors.\n",
    "\n",
    "The PDF bot is supposed to answer objective questions about the PDF as seen below in the example prompt:\n",
    "\n",
    "![img1](./SS/4.png)\n",
    "\n",
    "**Generated Data Format:**\n",
    "The generated data is in texual format, concise, and to the point. The model is not able to give long, descriptive answers but it is able to provide with a relatively useful response. Here is another example:\n",
    "\n",
    "![img1](./SS/2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Data Generation Process\n",
    "\n",
    "**Implementation:** We implement these libraries (also see requirements.txt for complete range of libraries required). We import various libraries and modules necessary for working with language models, document processing, and embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from constants import *\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define constants for model names and sets up a configuration dictionary with various parameters like the embedding model, language model, and persistence directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Python dictionary named `config` with four key-value pairs. Each key corresponds to a configuration option, and the associated value determines the setting for that option.\n",
    "\n",
    "1. `\"load_in_8bit\": False`: This option is a boolean flag (True/False) that indicates whether to load something in 8-bit mode. In this case, it is set to `False`, meaning that whatever is being loaded should not use 8-bit mode.\n",
    "\n",
    "2. `\"embedding\": EMB_SBERT_MPNET_BASE`: This option is related to embedding and is set to the value of `EMB_SBERT_MPNET_BASE`. \n",
    "\n",
    "3. `\"llm\": LLM_FLAN_T5_BASE`: This option is related to a language model (LLM), and it is set to the value of `LLM_FLAN_T5_BASE`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following functions define how to create different language models using the Hugging Face library. Models include Sentence-BERT MPNet embeddings and Flan T5 Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to create an instance of the HuggingFaceEmbeddings class Sentence-BERT (SBERT) with a model based on the MPNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yamini Manral\\Downloads\\chatbot_app\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n",
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a text-to-text generation pipeline using the Hugging Face Transformers library. It uses the T5 model from the Flan model collection. The function takes an optional argument `load_in_8bit`, which is set to `False` by default. The T5 model is loaded with the specified model name (`\"google/flan-t5-base\"`) and a tokenizer is created accordingly. The function then returns a pipeline for text-to-text generation with specific keyword arguments such as `device_map`, `load_in_8bit`, `max_length`, and `temperature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf\n",
    "pdf_path = \"DANNY SMITH.pdf\"\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents and create text snippets\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n",
    "\n",
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the PDF:**\n",
    "   Code uses the `PDFPlumberLoader` class to load the contents of the PDF file into the `documents` variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Text Splitting:**\n",
    "   - It creates a `CharacterTextSplitter` object with a specified `chunk_size` of 100 and `chunk_overlap` of 0.\n",
    "   - It splits the loaded documents into text chunks using the character-based text splitter.\n",
    "   - It creates a `TokenTextSplitter` object with a `chunk_size` of 1000, `chunk_overlap` of 10, and a specified encoding (\"cl100k_base\").\n",
    "   - It splits the previously obtained text chunks (from character-based splitting) into token-based text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Vectorization (Embedding):**\n",
    "   - It retrieves the \"persist_directory\" from the `config` dictionary.\n",
    "   - It uses the `Chroma.from_documents` method to create a vector database (`vectordb`) from the processed text documents (`texts`).\n",
    "   - The vectorization is performed using an embedding method specified by the `embedding` variable. The vectorized representations are stored in the specified \"persist_directory.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":1})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "\n",
    "# Defining a default prompt for flan models\n",
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "    question_t5_template = \"\"\"\n",
    "    Scan the document with context as {context}\n",
    "    and answer the questions asked on it as {question}\n",
    "     \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Hugging Face Pipeline and Vector Retrieval Setup:**\n",
    "   - It creates an instance of `HuggingFacePipeline` (`hf_llm`) using the specified `llm`.\n",
    "   - It creates a retriever from the vector database (`vectordb`) with search parameters specified by `search_kwargs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Question-Answering Setup:**\n",
    "   - It creates a question-answering system (`qa`) using the `RetrievalQA.from_chain_type` method.\n",
    "   - The language model (`llm`) is provided as the Hugging Face pipeline (`hf_llm`).\n",
    "   - The `chain_type` is set to \"stuff\" (the meaning of this depends on the specific implementation).\n",
    "   - The retriever is set to the previously created retriever (`retriever`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Prompt Definition for Flan Models:**\n",
    "   - It checks if the language model specified in the `config` dictionary is one of the Flan T5 models (`LLM_FLAN_T5_SMALL`, `LLM_FLAN_T5_BASE`, or `LLM_FLAN_T5_LARGE`).\n",
    "   - If true, it defines a template (`question_t5_template`) for question-answering with a context and a question. The answer field is left empty.\n",
    "   - It creates a `PromptTemplate` using the defined template and input variables.\n",
    "   - It sets this prompt template as the prompt for the language model chain in the question-answering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\Yamini Manral\\Downloads\\chatbot_app\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yamini Manral\\Downloads\\chatbot_app\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yamini Manral\\Downloads\\chatbot_app\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What is candidate's phone number?\", 'result': '(617)-431-5067'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is candidate's phone number?\"\n",
    "\n",
    "qa.combine_documents_chain.verbose = False\n",
    "qa.return_source_documents = False\n",
    "qa({\"query\":question,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](./SS/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Evaluation Criteria:** Accessing the quality of generated results from the provided code involves evaluating the output based on various criteria.\n",
    "\n",
    "**1. Ground Truth Comparison:** Since the input PDF is not very huge, I can directly compare the output generated to the facts in the PDF. IN this case, answers generated are correct and concise.\n",
    "\n",
    "**2. Semantic Coherence:** The generated text aligns with the context of the query and is logically structured.\n",
    "\n",
    "**3. Human Evaluation:** The output does not feel natural but it ranks high on being relevant and informative.\n",
    "\n",
    "**4. Consistency and Reproducibility:** Upon re-running the code with the same input multiple times, the model reproduces the same output, maintianing consistency.\n",
    "\n",
    "**5. Testing with new data:** Upon inserting new PDF, the model is still able to correctly answer the queries.\n",
    "\n",
    "**6. Contextual Understanding:** The model is able to comprehend and incorporate context, it generates meaningful responses based on the input context i.e. the PDF.\n",
    "\n",
    "**7. Time and Resource Efficiency:** The computational resources and time required for generating results is not high at all, infact if you have an understanding of what kind of questions to ask, the model can generate impressive results efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Building a PDF Knowledge Bot With Open-Source LLMs - A Step-by-Step Guide](https://www.shakudo.io/blog/build-pdf-bot-open-source-llms)\n",
    "- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1316s)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)\n",
    "- [google/flant5-base](https://huggingface.co/google/flan-t5-base)\n",
    "- [PDFPlumber Documentation](https://github.com/jsvine/pdfplumber)\n",
    "- [OpenAI - Text generation models](https://platform.openai.com/docs/guides/text-generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
