# OPT Project Summary Report
**Student:** Aayush Patel  
**Start Date:** August 6, 2025  
**Project Duration:** 3 weeks (as of August 26, 2025)

Abstract

This project focuses on exploring the Black Box Problem in Artificial Intelligence and developing comprehensive understanding of Explainable AI (XAI) techniques. The work addresses the fundamental question: "Is understanding necessary for trust in AI systems?" The project encompasses philosophical perspectives on AI transparency, comparative analysis of state-of-the-art XAI methods, and practical implications for building trustworthy AI systems.

The research covers both theoretical foundations and practical applications, examining how opacity in AI systems affects trust across high-stakes domains like healthcare, finance, and criminal justice. Through structured analysis, the project evaluates various explainability techniques including model-agnostic methods (SHAP, LIME, permutation importance, counterfactuals) and model-specific approaches (gradient-based methods, attention mechanisms, rule-based models).

The work emphasizes the critical balance between AI performance and interpretability, providing guidance for practitioners on when and how to implement different XAI techniques based on stakeholder needs and application contexts.

Key Accomplishments

### Week 1 (August 8, 2025)
- Successfully onboarded to the botspeak project
- Participated in project orientation meetings
- Conducted supervisor consultation to define research scope and objectives
- Selected the Black Box Problem in AI as the primary research focus

### Week 2 (August 15, 2025)
- **Created comprehensive research notebook** on the Black Box Problem in AI
- Developed structured analysis covering:
  - Philosophical foundations of trust and understanding in AI
  - Real-world case studies demonstrating opacity-related failures
  - Technical sources of AI system opacity
  - Practical implications for AI governance and design
- **Produced educational video** explaining the Black Box Problem concepts
- Established theoretical framework for subsequent technical analysis

### Week 3 (August 22, 2025)
- **Completed comparative review** of Explainable AI (XAI) techniques
- Conducted in-depth analysis of:
  - Model-agnostic methods (SHAP, LIME, permutation importance, counterfactuals)
  - Model-specific approaches (gradient-based, attention mechanisms, rule-based models)
  - Evaluation metrics for explanation quality (fidelity, stability, robustness, human factors)
- **Developed stakeholder-aligned guidance** for XAI implementation
- Created comprehensive technical documentation with real-world deployment examples

## Resources and Links

### Google Colab Notebooks
1. **Black Box Problem in AI (Week 2)**
   - [https://colab.research.google.com/drive/1NHB5qAwfU0fITIdkN17MdC13LXkewmzt#scrollTo=130e06db](https://colab.research.google.com/drive/1NHB5qAwfU0fITIdkN17MdC13LXkewmzt#scrollTo=130e06db)

2. **Explainable AI Techniques Comparative Review (Week 3)**
   - [https://colab.research.google.com/drive/1UhlMI1AHhgJIJUk_antS4Kho4B1P4MRG](https://colab.research.google.com/drive/1UhlMI1AHhgJIJUk_antS4Kho4B1P4MRG)

### Video Content
- **Black Box Problem Educational Video**
  - [Google Drive: https://drive.google.com/drive/folders/15RVAtVj4JHLX74FTOggm89Kic7z1VS38](https://drive.google.com/drive/folders/15RVAtVj4JHLX74FTOggm89Kic7z1VS38)

### Research References and Tools
- **Academic Sources:** Comprehensive bibliography including foundational papers by Ribeiro et al. (LIME), Lundberg & Lee (SHAP), Rudin (interpretable models), and others
- **Industry Case Studies:** Real-world implementations from NVIDIA, PayPal, Microsoft Research, FICO, and banking institutions
- **Evaluation Frameworks:** NIST AI principles, Quantus metrics, TruLens evaluation tools
- **Practical Tools:** InterpretML, Captum, What-If Tool, and other production-ready XAI libraries

## Project Impact and Future Directions

This project contributes to the growing field of responsible AI by providing:
- Clear frameworks for understanding the trust-interpretability relationship
- Practical guidance for implementing XAI techniques in different contexts
- Evidence-based recommendations for stakeholder-specific explanation needs
- Educational resources for broader community understanding

The work establishes a solid foundation for continued exploration of AI transparency, ethics, and practical implementation of explainable systems in high-stakes applications.

## Module Coverage
**INFO 7375 — Computational Skepticism and AI Fluency**  
**Module 4: Explainability and Interpretability of AI Models**  
- Section 1: Philosophical Foundations - The Black Box Problem ✓
- Section 2: Explainable AI (XAI) Techniques - Comparative Review ✓
