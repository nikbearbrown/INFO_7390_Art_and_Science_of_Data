{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8891b98",
   "metadata": {},
   "source": [
    "# Assignment 2: Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a56ce",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200f806",
   "metadata": {},
   "source": [
    "**Generative AI: Use Case**\n",
    "\n",
    "In our assignment, the primary goal is to provide personalized solutions based on individuals' moods using generative AI and prompt engineering. The dataset at our disposal contains a comprehensive set of issues paired with corresponding solutions. For instance, if an individual expresses feelings of anxiety, our generative AI model suggests potential remedies such as engaging in a conversation with a therapist or taking a peaceful walk outdoors. These examples showcase the versatility of our approach in tailoring responses to diverse emotional states. We will be fine tuning the model for our dataset to offer meaningful and contextually relevant recommendations, promoting mental well-being and a positive user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3bd84",
   "metadata": {},
   "source": [
    "**Installing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "\n",
    "!pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3453bc",
   "metadata": {},
   "source": [
    "**Importing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be6f5f",
   "metadata": {},
   "source": [
    "**About the model**\n",
    "\n",
    "GPT-2 XL is a big and powerful language model created by OpenAI. It's like a smart computer program that's really good at understanding and generating human-like text. The \"XL\" means it's extra large, with 1.5 billion parts making it one of the biggest models. It learns by reading a lot of different texts, and then it can be used to write or complete sentences, answer questions, or do other language-related tasks. People can also adjust or fine-tune it for specific jobs. While it's powerful, using it requires a lot of computer resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1489cd",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "We picked a dataset that has two parts: \"Condition\" and \"Solution.\" In the first part, we list the problems or challenges people might face, and in the second part, we suggest ways to overcome those issues. For instance, if someone is feeling really down (that's the condition), the solution could be talking to friends, spending time with family, being around people, or speaking to a therapist. It's like a guide offering ideas to help deal with different situations people might find themselves in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5dc6c",
   "metadata": {},
   "source": [
    "**Loading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Save the dataset in a text file\n",
    "df.to_csv('dataset.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "sentence_prefix = dataset[\"Condition\"][0]\n",
    "solution = dataset[\"Solution\"][0]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the following question.\n",
    "\n",
    "{sentence_prefix}\n",
    "\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SOLUTION:\\n{solution}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e113b9b",
   "metadata": {},
   "source": [
    "# Performing Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f83d6",
   "metadata": {},
   "source": [
    "**Preprocessing the Dataset**\n",
    "\n",
    "Imagine you're trying to cook a delicious meal, but the ingredients you have are messy, incomplete, and inconsistent. It would be quite challenging to create a flavorful dish in that situation. Similarly, in machine learning, data preprocessing is like cleaning and preparing the ingredients before cooking. It involves handling missing values, removing noise, and ensuring consistency in the data. This crucial step ensures the data is of high quality, suitable for analysis, and leads to more accurate and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aade7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_dataset(file_path):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=128,  # Adjust the block size according to your dataset and available memory\n",
    "    )\n",
    "\n",
    "train_dataset = tokenize_dataset('dataset.txt')\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We are not doing masked language modeling here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc95fb",
   "metadata": {},
   "source": [
    "**Setting up training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chatbot_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e15da1",
   "metadata": {},
   "source": [
    "**Initialize Trainer and fine-tune the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a8507",
   "metadata": {},
   "source": [
    "**What is fine tuning?**\n",
    "\n",
    "In Machine Learning, fine tuning is a way to make a pre-trained model work better for a specific task. It's like giving the model a little extra training to help it understand the new job better. This is especially useful when you don't have a lot of data to train the model from scratch and want your model to work efficiently for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./chatbot_model\")\n",
    "tokenizer.save_pretrained(\"./chatbot_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6249a",
   "metadata": {},
   "source": [
    "**Loading the pre-trained model for getting the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0afac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot using the fine-tuned model\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./chatbot_model\")\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./chatbot_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f35c2",
   "metadata": {},
   "source": [
    "**Giving the input text for tokenizing it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"Stress\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9af85",
   "metadata": {},
   "source": [
    "**Generating the output of the given input text from the pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64ba84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate output from the model\n",
    "output = fine_tuned_model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "# Decode and print the generated text\n",
    "response = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3854ab",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have used GPT2-xl model for our use case and have also implemented zero shot inference for the same dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
