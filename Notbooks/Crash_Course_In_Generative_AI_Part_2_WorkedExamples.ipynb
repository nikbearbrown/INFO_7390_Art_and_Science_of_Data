{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5235b3f2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 7390 Advances Data Sciences and Architecture SEC 03 Spring 2024</div>\n",
    "<div style=\"text-align: right\">Crash Course in Generative AI</div>\n",
    "<div style=\"text-align: right\">Aditi A. Deodhar, NUID: 002279575</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f8f41",
   "metadata": {},
   "source": [
    "<hide>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30d716",
   "metadata": {},
   "source": [
    "Welcome to the Generative AI Worked Example Notebook! \n",
    "This notebook serves as a comprehensive guide to understanding and implementing Generative Artificial Intelligence (AI) concepts through real-world examples.\n",
    "\n",
    "In this notebook, we will cover an overview of [three Coursera course labs related to Generative AI](https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/tree/main) and then work/make changes of our own to the same dataset. Each lab presents unique scenarios and problem statements, offering opportunities to delve into different aspects of Generative AI, including text generation, fine-tuning large language models, reinforcement learning from human feedback, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bf262",
   "metadata": {},
   "source": [
    "## Overview and Theoretical Underpinnings of the reference Coursera Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382cd647",
   "metadata": {},
   "source": [
    "### **Lab 1 - Exploring Model Pre-training and Scaling in Generative AI**\n",
    "\n",
    "**Overview:**\n",
    "The first lab offers a deep dive into the intricacies of model pre-training and scaling within the realm of generative AI. It provides a structured framework for understanding the fundamental concepts and practical considerations involved in training large language models (LLMs) while addressing computational challenges and exploring strategies to optimize performance.\n",
    "\n",
    "**Key Insights:**\n",
    "- **Model Pre-training Dynamics:** \n",
    "The lab adeptly navigates through the significance of model pre-training, emphasizing its role in nurturing adaptable LLMs capable of comprehending and generating coherent text across a spectrum of tasks. By delving into the nuances between continued pre-training and fine-tuning, it highlights pivotal decision factors such as data specificity and computational resource management.\n",
    "   - This section explores the significance of model pre-training, which involves training a large language model (LLM) on a vast corpus of text data to learn language patterns and semantics. Pre-training nurtures adaptable LLMs capable of understanding and generating coherent text across various tasks.\n",
    "   - The distinction between continued pre-training and fine-tuning is discussed, emphasizing factors like data specificity and computational resource management.\n",
    "- **Foundational Concepts:** \n",
    "Participants are exposed to essential concepts underpinning generative AI, including the transformer architecture, prompt engineering, and chain-of-thought prompting. Through a blend of theoretical discussions and practical applications, the lab elucidates how these concepts contribute to enhancing LLMs' reasoning, planning abilities, and overall performance.\n",
    "   - Essential concepts such as the transformer architecture, prompt engineering, and chain-of-thought prompting are introduced. The transformer architecture forms the backbone of modern LLMs, while prompt engineering and chain-of-thought prompting techniques enhance the model's reasoning and planning abilities.\n",
    "- **Addressing Computational Challenges:** \n",
    "A significant portion of the lab is dedicated to dissecting computational hurdles encountered during model pre-training and proposing effective strategies to mitigate them. From memory optimization techniques to scaling laws, participants gain valuable insights into optimizing training efficiency and resource utilization, thereby fostering the development of robust LLMs.\n",
    "  - This section focuses on dissecting computational hurdles encountered during model pre-training and strategies to mitigate them. Techniques such as memory optimization and scaling laws are explored to optimize training efficiency and resource utilization, ensuring the development of robust LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c6d38",
   "metadata": {},
   "source": [
    "### **Lab 2 - Fine-tuning and Evaluating Large Language Models**\n",
    "\n",
    "**Overview:**\n",
    "Lab 2 offers a comprehensive exploration of fine-tuning methodologies and techniques aimed at optimizing the performance of large language models (LLMs) while addressing challenges such as catastrophic forgetting and computational efficiency. Through a blend of theoretical discussions and practical insights, participants gain a deeper understanding of fine-tuning strategies, parameter-efficient methodologies, and the role of task-specific instructions in enhancing LLM performance.\n",
    "\n",
    "**Key Insights:**\n",
    "1. **Fine-tuning with Prompt Datasets:**\n",
    "   - Participants delve into the intricacies of fine-tuning LLMs with task-specific instructions using prompt datasets. They learn how tailored prompts guide the model in generating contextually relevant outputs for diverse applications, leading to improved task performance and enhanced adaptability across domains.\n",
    "\n",
    "2. **Addressing Catastrophic Forgetting:**\n",
    "   - The lab elucidates the phenomenon of catastrophic forgetting and explores techniques to overcome it. Participants discover rehearsal methods, regularization techniques, and dual memory networks aimed at preserving previous knowledge while adapting to new tasks, ensuring continual learning without compromising past learning.\n",
    "    - Catastrophic forgetting refers to the phenomenon where a model forgets previously learned information when trained on new tasks. Techniques such as rehearsal methods, regularization, and dual memory networks are explored to preserve previous knowledge while adapting to new tasks.\n",
    "\n",
    "3. **Parameter-efficient Fine Tuning (PEFT):**\n",
    "   - PEFT emerges as a pivotal methodology for fine-tuning LLMs with a focus on computational efficiency. Participants uncover how PEFT minimizes parameter updates, optimizes memory usage, and enables rapid model deployment, making it invaluable in resource-constrained environments and large-scale applications.\n",
    "\n",
    "4. **Enhancing LLM Performance:**\n",
    "   - Fine-tuning with prompt datasets is highlighted as a transformative approach to increasing LLM performance on various tasks. Participants understand how explicit task-specific instructions refine contextual understanding, reduce ambiguity, and improve model adaptability, ultimately contributing to enhanced generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933231de",
   "metadata": {},
   "source": [
    "### **Lab 3 - Reinforcement Learning and LLM-powered Applications**\n",
    "\n",
    "**Overview:**\n",
    "Lab 3 delves into the realm of reinforcement learning (RL) and its application in improving the performance and alignment of large language models (LLMs) through reinforcement learning from human feedback (RLHF). Additionally, the lab explores chain-of-thought prompting techniques to enhance LLMs' reasoning and planning abilities and discusses challenges associated with knowledge cut-offs in LLMs, along with techniques to overcome them.\n",
    "\n",
    "**Key Insights:**\n",
    "1. **Reinforcement Learning from Human Feedback (RLHF):**\n",
    "   - Participants gain insights into RLHF, a methodology that utilizes human-generated feedback to refine LLM behavior. The lab emphasizes the iterative process of incorporating human signals to enhance model performance and alignment with human intent, fostering adaptability to dynamic preferences and ethical considerations.\n",
    "   - RLHF utilizes human-generated feedback to refine LLM behavior, enhancing model performance and alignment with human intent. The iterative process of incorporating human signals fosters adaptability to dynamic preferences and ethical considerations.\n",
    "\n",
    "2. **Training a Reward Model for RLHF:**\n",
    "   - The lab elucidates the process of training a reward model for RLHF using data gathered from human labelers. Participants learn to collect human feedback, construct reward signals, and iteratively train the model based on refined guidance, while adhering to ethical considerations and ensuring transparency in the training process.\n",
    "\n",
    "3. **Chain-of-Thought Prompting:**\n",
    "   - Chain-of-thought prompting emerges as a technique to guide LLMs' reasoning and planning abilities through structured sequences of prompts. Participants explore scenarios where this approach facilitates problem-solving, narrative generation, decision-making, and essay construction, promoting contextual continuity and adaptability across diverse tasks.\n",
    "\n",
    "4. **Overcoming Knowledge Cut-offs:**\n",
    "   - Challenges associated with knowledge cut-offs in LLMs are addressed, along with techniques to mitigate them. Participants learn about continuous training, external knowledge integration, semantic augmentation, and active context integration, empowering LLMs to stay informed, access domain-specific knowledge, and adapt to evolving information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae073e0",
   "metadata": {},
   "source": [
    "## Now, analyzing and making changes of my own..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049d280",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We delve into the capabilities of Large Language Models (LLMs) with a focus on leveraging Parameter Efficient Fine-Tuning (PEFT) to generate dialogue summaries with reduced toxicity. Our approach involves utilizing the FLAN-T5 model in conjunction with Meta AI's hate speech reward model to achieve our primary objective of improving the quality of dialogue summaries while minimizing toxicity.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Train a Large Language Model (LLM) to generate dialogue summaries with reduced toxicity.\n",
    "  \n",
    "### The DialogSum Dataset\n",
    "\n",
    "We utilize the [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum), a large-scale dialogue summarization dataset comprising 13,460 dialogues along with manually labeled summaries and topics. Additionally, 100 holdout data points are reserved for topic generation.\n",
    "\n",
    "### Project Workflow\n",
    "\n",
    "1. **Setup**: Import necessary libraries and define project parameters. \n",
    "2. **Dataset Exploration**: Explore the DialogSum Dataset to understand its structure and contents. \n",
    "3. **Test Model Zero Shot Inferencing**: Initially, assess the performance of the FLAN-T5 model for zero-shot inferencing on dialogue summarization tasks to establish a baseline. \n",
    "4. **Dataset Preprocessing**: Preprocess the dialogues and their corresponding summaries from the dataset to prepare for training. \n",
    "5. **Perform Parameter Efficient Fine-Tuning (PEFT)**: Implement PEFT, a more efficient fine-tuning approach, to significantly reduce training time while maintaining performance.  \n",
    "6. **Evaluation**:\n",
    "- Conduct human evaluation to assess the model's output in terms of readability and coherence, possibly involving annotators ranking generated summaries for quality.\n",
    "- Utilize ROUGE metrics to measure the quality of the generated summaries by comparing them to human-written references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bafac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\aditi\\anaconda3\\lib\\site-packages (24.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\n",
      "ERROR: No matching distribution found for torch==1.13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 2.6.0 requires transformers<5.0.0,>=4.32.0, but you have transformers 4.27.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/lvwerra/trl.git@25fa1bd\n",
      "  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to c:\\users\\aditi\\appdata\\local\\temp\\pip-req-build-5vaxnb_0\n",
      "  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from trl==0.4.2.dev0) (2.2.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from trl==0.4.2.dev0) (4.27.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from trl==0.4.2.dev0) (1.24.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from trl==0.4.2.dev0) (0.28.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from trl==0.4.2.dev0) (2.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from accelerate->trl==0.4.2.dev0) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from accelerate->trl==0.4.2.dev0) (0.4.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from datasets->trl==0.4.2.dev0) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers>=4.18.0->trl==0.4.2.dev0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.4.0->trl==0.4.2.dev0) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from pandas->datasets->trl==0.4.2.dev0) (2022.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from sympy->torch>=1.4.0->trl==0.4.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git 'C:\\Users\\aditi\\AppData\\Local\\Temp\\pip-req-build-5vaxnb_0'\n",
      "  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\n",
      "  Running command git checkout -q 25fa1bd\n"
     ]
    }
   ],
   "source": [
    "# Installing the necessary libraries\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    peft==0.3.0 --quiet\n",
    "\n",
    "# Installing the Reinforcement Learning library directly from github.\n",
    "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9950bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfa1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset and the model to be used\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca6ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c8d1b",
   "metadata": {},
   "source": [
    "### Tokenise the Data\n",
    "\n",
    "The next step involves dataset preprocessing. We'll select a subset of the data, filter dialogues to a specific length to ensure readability while maintaining meaningful content, and then integrate each dialogue with an instruction before tokenizing the prompts. The resulting token IDs will be stored in the input_ids field, while the decoded prompts will be saved in the query field.\n",
    "\n",
    "To streamline this process, it's advisable to create a function called build_dataset. This function can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d57f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the dataset \n",
    "\n",
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length, \n",
    "                  input_max_text_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Tokenizer model name.\n",
    "    - dataset_name (str): Name of the dataset to load.\n",
    "    - input_min_text_length (int): Minimum length of the dialogues.\n",
    "    - input_max_text_length (int): Maximum length of the dialogues.\n",
    "        \n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load dataset (only \"train\" part will be enough for this lab).\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200, \n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ea551",
   "metadata": {},
   "source": [
    "### Enhancing FLAN-T5 Model Fine-Tuned with Summarization Adapter\n",
    "\n",
    "Next, \n",
    "We are enhancing the original FLAN-T5 model by adding a summarization adapter. This adapter is designed to improve the model's performance in summarization tasks.\n",
    "\n",
    "We begin by configuring the adapter using the following parameters:\n",
    "- `r`: Rank, which is set to 32.\n",
    "- `lora_alpha`: LORA alpha value, set to 32.\n",
    "- `target_modules`: We specify the target modules as [\"q\", \"v\"].\n",
    "- `lora_dropout`: Dropout rate for LORA, set to 0.05.\n",
    "- `bias`: We use \"none\" as the bias configuration.\n",
    "- `task_type`: The task type is set to SEQ_2_SEQ_LM, which is suitable for FLAN-T5.\n",
    "\n",
    "Next, we load the pre-trained FLAN-T5 model and create an instance of the AutoModelForSeq2SeqLM with the specified model name and data type (torch_dtype).\n",
    "\n",
    "We also create a PeftModel by incorporating the previously loaded model. \n",
    "Additionally, we provide the LORA configuration, torch data type, device mapping, and specify that the model is trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb8b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       'z7ye/peft-dialogue-summary-checkpoint', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d2ee3",
   "metadata": {},
   "source": [
    "###  Enhancing LLM Summarization with Reinforcement Learning with PPO\n",
    "\n",
    "Now, we are in the process of preparing for fine-tuning the Language Model (LLM) using Reinforcement Learning (RL). Although a more detailed explanation of RL, our current focus is on setting up the Proximal Policy Optimization (PPO) model. \n",
    "\n",
    "This PPO model will receive the instruction-fine-tuned PEFT model as input and will be utilized to optimize the RL policy in accordance with the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db272527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f3b8",
   "metadata": {},
   "source": [
    "During the Proximal Policy Optimization (PPO) process, only a subset of parameters will be updated, specifically those associated with the `ValueHead`. You can find more detailed information about this class of models in the [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters in the `ValueHead` can be computed as $(n+1) \\cdot m$, where $n$ represents the number of input units (in this case, $n=768$) and $m$ represents the number of output units (which is $m=1$ in this context). The additional $+1$ term in the equation accounts for the bias term.\n",
    "\n",
    "Now, let's create a frozen copy of the PPO model, which will serve as a reference model. This reference model will represent the Language Model (LLM) before detoxification. Importantly, none of the parameters of the reference model will be updated during PPO training. This is by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92aaaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01b64e",
   "metadata": {},
   "source": [
    "### Building a Reward Model for Reinforcement Learning\n",
    "\n",
    "\n",
    "**Reinforcement Learning (RL)** stands as a pivotal branch of machine learning wherein agents make decisions within an environment to maximize their cumulative rewards. The behavior of these agents is governed by a decision-making **policy**, and the fundamental objective of RL is for the agent to acquire an optimal or near-optimal policy that maximizes the **reward function**.\n",
    "\n",
    "Previously, the original policy was rooted in the instruct PEFT model – essentially, the Language Model (LLM) before undergoing detoxification. While one approach involved soliciting human labelers to provide feedback on the toxicity of the model's outputs, this process can become prohibitively costly when applied throughout the entire fine-tuning phase. A pragmatic solution to circumvent this expense is to implement a reward model that encourages the agent to produce detoxified dialogue summaries.\n",
    "\n",
    "A sensible approach here is to perform **sentiment analysis** on the model's outputs, classifying them into two categories: `nothate` and `hate`. Higher rewards are assigned when the likelihood of classifying an output as `nothate` is greater.\n",
    "\n",
    "In this context, we will employ [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) as our reward model. This model generates **logits** and subsequently predicts probabilities for two classes: `nothate` and `hate`. Positive rewards are derived from the logits associated with the `nothate` class. The model will undergo further fine-tuning using Proximal Policy Optimization (PPO) with these reward values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e50d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6564805",
   "metadata": {},
   "source": [
    "Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac086c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.1140999794006348, -2.489616870880127]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.003670621896162629]\n",
      "reward (high): [3.1140999794006348]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d81660",
   "metadata": {},
   "source": [
    "Let's show a toxic comment.  This will have a low reward because it is more toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c845c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-0.6921142935752869, 0.37226906418800354]\n",
      "probabilities [not hate, hate]: [0.2564726769924164, 0.743527352809906]\n",
      "reward (low): [-0.6921142935752869]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (low): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d874b",
   "metadata": {},
   "source": [
    "### Setup toxicity reward pipeline\n",
    "\n",
    "Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568595e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aditi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Reward model output:\n",
      "For non-toxic text\n",
      "[{'label': 'nothate', 'score': 3.1140999794006348}, {'label': 'hate', 'score': -2.489616870880127}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.0036706216633319855}]\n",
      "For toxic text\n",
      "[{'label': 'hate', 'score': 0.37226906418800354}, {'label': 'nothate', 'score': -0.6921142935752869}]\n",
      "[{'label': 'hate', 'score': 0.7435272932052612}, {'label': 'nothate', 'score': 0.2564726769924164}]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output:\")\n",
    "print(\"For non-toxic text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"For toxic text\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad94d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 3.1140999794006348}, {'label': 'hate', 'score': -2.489616870880127}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.0036706216633319855}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24e23865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 0.37226906418800354}, {'label': 'nothate', 'score': -0.6921142935752869}]\n",
      "[{'label': 'hate', 'score': 0.7435272932052612}, {'label': 'nothate', 'score': 0.2564726769924164}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c6c10",
   "metadata": {},
   "source": [
    "### Evaluate Toxicity\n",
    "\n",
    "To assess the model's performance both before and after the fine-tuning and detoxification processes, it is essential to establish the toxicity evaluation metric. The toxicity score is represented as a decimal value ranging from 0 to 1, where 1 signifies the highest degree of toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "331e8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599fa4b5",
   "metadata": {},
   "source": [
    "Try to calculate toxicity for the same sentences as earlier, it's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b2de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.0036706216633319855]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.7435272932052612]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d9add",
   "metadata": {},
   "source": [
    "This evaluator can be effectively employed to calculate the toxicity levels of the dialogues. \n",
    "\n",
    "To accomplish this, you will need to provide several essential components, including the test dataset (`dataset[\"test\"]`), the tokenizer used in the aforementioned section, the previously frozen PEFT model, and the toxicity evaluator itself. For a streamlined and organized approach, it is recommended to encapsulate these necessary procedures within a dedicated function named `evaluate_toxicity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c64357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model, \n",
    "                      toxicity_evaluator, \n",
    "                      tokenizer, \n",
    "                      dataset, \n",
    "                      num_samples):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model (trl model): Model to be evaluated.\n",
    "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
    "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
    "    - dataset (dataset): Input dataset for the evaluation.\n",
    "    - num_samples (int): Maximum number of samples for the evaluation.\n",
    "        \n",
    "    Returns:\n",
    "    tuple: A tuple containing two numpy.float64 values:\n",
    "    - mean (numpy.float64): Mean of the samples toxicity.\n",
    "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             top_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "        \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "        \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6e6b7",
   "metadata": {},
   "source": [
    "And now perform the calculation of the model toxicity before fine-tuning/detoxification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778a1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [04:40, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.029261352811855348, 0.03265672114305328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n",
    "                                                                          toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                          tokenizer=tokenizer, \n",
    "                                                                          dataset=dataset[\"test\"], \n",
    "                                                                          num_samples=10)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38fd78b",
   "metadata": {},
   "source": [
    "### Perform Fine-Tuning to Detoxify the Summaries\n",
    "\n",
    "Optimize a RL policy against the reward model using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fa3a9",
   "metadata": {},
   "source": [
    "## Initialize PPOTrainer\n",
    "\n",
    "For the `PPOTrainer` initialization, you will need a collator. Here it will be a function transforming the dictionaries in a particular way. You can define and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8ef9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8da202",
   "metadata": {},
   "source": [
    "Configure the essential parameters. Load the `ppo_model` and the corresponding tokenizer. \n",
    "\n",
    "Additionally, load a static version of the model, referred to as `ref_model`. \n",
    "\n",
    "The purpose of having two models is twofold: the first model, `ppo_model`, undergoes optimization, while the second model, `ref_model`, functions as a reference point to compute the KL-divergence from the initial state. \n",
    "\n",
    "This serves as an additional reward signal in the PPO training process, ensuring that the optimized model does not stray too far from the original Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22eb5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer, \n",
    "                         dataset=dataset[\"train\"], \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9d0b8",
   "metadata": {},
   "source": [
    "The fine-tuning loop comprises the following key steps:\n",
    "\n",
    "1. Retrieve query responses from the policy Language Model (PEFT model).\n",
    "2. Determine the sentiments associated with the queries and responses using the hate speech RoBERTa model.\n",
    "3. Optimize the policy using Proximal Policy Optimization (PPO) with the triplet of inputs, which includes the query, response, and the associated reward.\n",
    "\n",
    "You can confirm that the operation is successfully running by monitoring the following metrics:\n",
    "\n",
    "- `objective/kl`: Minimization of the Kullback-Leibler (KL) divergence.\n",
    "- `ppo/returns/mean`: Maximization of the mean returns.\n",
    "- `ppo/policy/advantages_mean`: Maximization of the mean advantages.\n",
    "\n",
    "These metrics serve as indicators of the training process's progress and the achievement of specific objectives within the fine-tuning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4678c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [4:42:01, 16921.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.363040924072266\n",
      "ppo/returns/mean: -0.48076772689819336\n",
      "ppo/policy/advantages_mean: -0.004063474014401436\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [5:36:24, 8886.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 26.428688049316406\n",
      "ppo/returns/mean: -0.2867184281349182\n",
      "ppo/policy/advantages_mean: 0.02232614904642105\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [6:24:05, 6135.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.135053634643555\n",
      "ppo/returns/mean: -0.11719291657209396\n",
      "ppo/policy/advantages_mean: 0.045380767434835434\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [8:01:04, 6010.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 21.118507385253906\n",
      "ppo/returns/mean: 0.0019967034459114075\n",
      "ppo/policy/advantages_mean: 0.034106601029634476\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [8:51:45, 4939.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.728361129760742\n",
      "ppo/returns/mean: -0.32960039377212524\n",
      "ppo/policy/advantages_mean: 0.006062053143978119\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [9:46:47, 4383.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.728355407714844\n",
      "ppo/returns/mean: -0.2059704065322876\n",
      "ppo/policy/advantages_mean: 0.02402125485241413\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [11:57:46, 5519.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 28.03209114074707\n",
      "ppo/returns/mean: -0.25091832876205444\n",
      "ppo/policy/advantages_mean: 0.01567326858639717\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [12:57:27, 4902.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 26.06850814819336\n",
      "ppo/returns/mean: -0.22763723134994507\n",
      "ppo/policy/advantages_mean: 0.005941160023212433\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [14:20:43, 4931.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 24.170854568481445\n",
      "ppo/returns/mean: -0.16248102486133575\n",
      "ppo/policy/advantages_mean: 0.03705313801765442\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [15:20:38, 5523.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 24.540199279785156\n",
      "ppo/returns/mean: -0.111860491335392\n",
      "ppo/policy/advantages_mean: 0.0754142478108406\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break   \n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()        \n",
    "            \n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "        \n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3867b",
   "metadata": {},
   "source": [
    "### Evaluate the Model Quantitatively\n",
    "\n",
    "\n",
    "Retrieve the PPO/PEFT model from the saved disk checkpoint and employ the test dataset split to assess the toxicity score of the RL-fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b1fb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [04:00, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] after detox: [0.03229118371382356, 0.02902330218285877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=dataset[\"test\"], \n",
    "                                                                        num_samples=10)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeec3e4",
   "metadata": {},
   "source": [
    "And compare the toxicity scores of the reference model (before detoxification) and fine-tuned model (after detoxification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1780829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage improvement of toxicity score after detoxification:\n",
      "mean: -10.35%\n",
      "std: 11.13%\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b6ea6",
   "metadata": {},
   "source": [
    "Explore sample examples from the test dataset, allowing for a comparison between the initial `ref_model` and the fine-tuned/detoxified `ppo_model` using the toxicity evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23933b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [15:35<00:00, 46.77s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfea23",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Store and review the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee641ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and Judy think everyone is talking about Richard's termination of Richard by their manager. Judy thinks he's strange.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and Shudd 1# hear the story of how Richard was fired.&lt;/s&gt;</td>\n",
       "      <td>1.254621</td>\n",
       "      <td>1.887931</td>\n",
       "      <td>0.633310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to have the currency stamped. #Person2# gives #Person1# ten different kinds of money with change.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1#'ll buy ten twenties with $160 in changing at Hanna haven, to be cashed.&lt;/s&gt;</td>\n",
       "      <td>1.904012</td>\n",
       "      <td>2.313659</td>\n",
       "      <td>0.409647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
       "      <td>&lt;pad&gt; #Person2# is walking in the opposite direction and tells #Person1# there are 3 blocks to Broadway. #Person1# wants to see the Bakery on #Person1#'s left hand side, but #Person2# doesn't want to show #Person1# the way.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# the way to the Cross Bakery building in the opposite direction. #Person1# takes a turn while #Person1# Companies Cross Bakery building for the next few years. #Person1# doesn't know about this place. #Person2# offers #Person1# the route and #Person2# changes it in October.&lt;/s&gt;</td>\n",
       "      <td>2.641189</td>\n",
       "      <td>2.988603</td>\n",
       "      <td>0.347414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# to reconfirm their flight to London tomorrow. #Person2# gives #335 to #Person1# who can communicate in English.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# goes to the airport and tells erwartet to books #Person3# as #Person3# United express the flightns and the cuid they are doing a flight.&lt;/s&gt;</td>\n",
       "      <td>1.795023</td>\n",
       "      <td>2.037829</td>\n",
       "      <td>0.242806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
       "      <td>&lt;pad&gt; #Person2# can't walk along the computer for hours so she decides to take a coffee break. #Person1# agrees because #Person2# is up to #Person2#'s neck and wants to finish his report to Sarah to finish by noon.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# wishes to retake a coffee break even for some time but #Person2# can't leave him. They make up there's agreung during #Person3#'s working breaks.&lt;/s&gt;</td>\n",
       "      <td>1.653246</td>\n",
       "      <td>1.892593</td>\n",
       "      <td>0.239347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# about the widespread use of personal computers. Whether people can buy goods through PC is solved mainly by receiving a specimen of the goods shown on computer screen and placing an order online.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# checks out the various internet websites for information and winners from the press website, HP and AOL suggest #Person1# the detailed tips on how to use computers on the internet allow for projects.&lt;/s&gt;</td>\n",
       "      <td>2.414960</td>\n",
       "      <td>2.613434</td>\n",
       "      <td>0.198474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# about #Person1#s music skills. #Person1# has joined a rock bandwith the music talent but we haven't found anyone to be our singer. #Person2# tells #Person1# that #Person1# is interested in auditioning but doesn't have enough room for them. #Person1# decides to audition in an audition only because he can reach out to #Person1# and is willing to practice and watch other musicians.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to form a music band but #Person2# wanted to play a different genre. #Person1# wanted to join a rock band, while #Person2# wants a rock band. But #Person1# gave me lots of thanks for trying the auditions.&lt;/s&gt;</td>\n",
       "      <td>2.734084</td>\n",
       "      <td>2.857643</td>\n",
       "      <td>0.123559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# that #1_1#'s new restaurant should have been better. #Person2# took the food and service dose not start with the service. The restaurant nobody knows.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# doesn't order much else at this restaurant because they don't have issues with the service of the restaurant. They are disappointed and decide to get another servers.&lt;/s&gt;</td>\n",
       "      <td>1.875288</td>\n",
       "      <td>1.992330</td>\n",
       "      <td>0.117042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
       "      <td>&lt;pad&gt; #Person2# helps #Person1# to know how to find a full-time job at a job center. They have binders with local job listings and computers. Otherwise, #Person1# refuses to go to a job center.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# finds a job center with options for the job seeker. #Person2# helps #Person1# to find a job and would make an appointment.&lt;/s&gt;</td>\n",
       "      <td>1.897589</td>\n",
       "      <td>1.977381</td>\n",
       "      <td>0.079792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
       "      <td>&lt;pad&gt; Alice apologizes to Li Hong for not going to Mrs. Brown with Li Hong tomorrow morning because her mother is ill. Li Hong and Alice decide on visiting Mrs. Brown later. Are they going to visit Mrs. Brown?&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Alice falls ill. Li Hong asks her to stay in home because she has to visit Ms Brown. Alice accepts the invitation. #Person1# will visit Ms Brown now.&lt;/s&gt;</td>\n",
       "      <td>1.617947</td>\n",
       "      <td>1.685857</td>\n",
       "      <td>0.067909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# a toy car for #Person2#'s son is two hundred and twenty dollars and they will take the cheapest one also.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# buys a toy car for #Person2# in one hundreds and twenty. #Person1# tells #Person1# the cheapest one is a hundred and eighty and they will go to the store.&lt;/s&gt;</td>\n",
       "      <td>1.261466</td>\n",
       "      <td>1.285411</td>\n",
       "      <td>0.023945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
       "      <td>&lt;pad&gt; #Person1# has to register at the pharmacy, and #Person2# will make a medical record for #Person1#. #Person1# will pay 10 yuan for the registration and make the left turn until #Person2# wants to get there.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# registers and calls number number #Person2#. #Person1# requests a medical register and asks #Person2# to leave money if #Person1# leave with #Person2#'s card.&lt;/s&gt;</td>\n",
       "      <td>1.637714</td>\n",
       "      <td>1.553722</td>\n",
       "      <td>-0.083992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to order some internet. #Person1# asks the whether it's better because it's connected through your phone line and dial-up is. #Person2# provides information about the differences between DEL and dial-up.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# is ordering DEL and DEL telnet internet today, while #Person2# wants to put on the phone so they convergence properly.&lt;/s&gt;</td>\n",
       "      <td>2.457580</td>\n",
       "      <td>2.326880</td>\n",
       "      <td>-0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda likes the peaked cap, and shows #Person2# how to choose one.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda shows #Person1# she loves what she thinks of her capchter.&lt;/s&gt;</td>\n",
       "      <td>1.370748</td>\n",
       "      <td>1.222371</td>\n",
       "      <td>-0.148377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
       "      <td>&lt;pad&gt; #Person1# finds a smell of smoke in #Person2# while #Person2# is quitting smoking and put on a pack of smokes. #Person2# feels motivated but can't quit.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# actually smokes and smokes a lot of cigarettes and they can't just break.&lt;/s&gt;</td>\n",
       "      <td>1.963959</td>\n",
       "      <td>1.631021</td>\n",
       "      <td>-0.332937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
       "      <td>&lt;pad&gt; #Person1# and #Person2# are discussing the final draft of the contract and at first they're very happy. They still have a day left before they are formally  signing the contract.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# is looking at the final draft of the contract. #Person1# brings up some issues, such as the final breakdown and quality washing the slip and #Person1# puts their \"eminction part\" on every detail.&lt;/s&gt;</td>\n",
       "      <td>3.313624</td>\n",
       "      <td>2.864893</td>\n",
       "      <td>-0.448731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# offers a custom discount about 300 yuan and 2# offers a 10 % discount on 160yuan to koyak. #Person1# accepts her offer.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# offers them under a sample brand and points it out. #Person2# doesn't allow the customers to sell the store and asks for a 10% discount when buying a shipment. #Polerson1# make a pact with #Person2#.&lt;/s&gt;</td>\n",
       "      <td>2.627102</td>\n",
       "      <td>2.104736</td>\n",
       "      <td>-0.522367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
       "      <td>&lt;pad&gt; Allen wants to know if someone broke into #Person1#'s house. He manages to do so. Allen finds a TV and a stereo there, but the store doesn't exist.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# a breakinn broke out the door. She finds out the suspect left through a window in the house. #Person2# has uncovered the robber's criminal possession. #Persongeführt** replies.&lt;/s&gt;</td>\n",
       "      <td>1.902421</td>\n",
       "      <td>1.272791</td>\n",
       "      <td>-0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; I will print out some suggestions. Others may have booked on the flight.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; [1# tells her a flight hasn't gotten through. Title says nobody's queued up but #Person1# has skins to fix.&lt;/s&gt;</td>\n",
       "      <td>3.056041</td>\n",
       "      <td>2.210695</td>\n",
       "      <td>-0.845347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# has just finished the paper. #Person2# praises #Person1#'s works and thank for the original paper.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants her mother to check her paper before handing to her. #Person1# says she recited the ideas inspired by #Person1#'s work. #Person1# said it was worth's all the time the teacher told her to work hard.&lt;/s&gt;</td>\n",
       "      <td>2.679392</td>\n",
       "      <td>1.747525</td>\n",
       "      <td>-0.931867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0                                                                                                                                                                   Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n",
       "1                                                                                                                                                                         Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n",
       "2   Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
       "3   Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
       "4   Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
       "5   Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
       "6   Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...   \n",
       "7   Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
       "8   Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
       "9   Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
       "10          Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n",
       "11  Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
       "12  Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
       "13                                                                                                                                                                                                                          Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n",
       "14  Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
       "15  Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
       "16                                                                                    Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n",
       "17  Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
       "18                                                                                                                                                                                                                                        Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n",
       "19                      Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                     response_before  \\\n",
       "0                                                                                                                                                                                                                                                                                               <pad> Judy and Judy think everyone is talking about Richard's termination of Richard by their manager. Judy thinks he's strange.</s>   \n",
       "1                                                                                                                                                                                                                                                                                                        <pad> #Person1# wants to have the currency stamped. #Person2# gives #Person1# ten different kinds of money with change.</s>   \n",
       "2                                                                                                                                                                                                <pad> #Person2# is walking in the opposite direction and tells #Person1# there are 3 blocks to Broadway. #Person1# wants to see the Bakery on #Person1#'s left hand side, but #Person2# doesn't want to show #Person1# the way.</s>   \n",
       "3                                                                                                                                                                                                                                                                                 <pad> #Person1# asks #Person2# to reconfirm their flight to London tomorrow. #Person2# gives #335 to #Person1# who can communicate in English.</s>   \n",
       "4                                                                                                                                                                                                         <pad> #Person2# can't walk along the computer for hours so she decides to take a coffee break. #Person1# agrees because #Person2# is up to #Person2#'s neck and wants to finish his report to Sarah to finish by noon.</s>   \n",
       "5                                                                                                                                                                                            <pad> #Person1# tells #Person2# about the widespread use of personal computers. Whether people can buy goods through PC is solved mainly by receiving a specimen of the goods shown on computer screen and placing an order online.</s>   \n",
       "6   <pad> #Person1# asks #Person2# about #Person1#s music skills. #Person1# has joined a rock bandwith the music talent but we haven't found anyone to be our singer. #Person2# tells #Person1# that #Person1# is interested in auditioning but doesn't have enough room for them. #Person1# decides to audition in an audition only because he can reach out to #Person1# and is willing to practice and watch other musicians.</s>   \n",
       "7                                                                                                                                                                                                                                         <pad> #Person2# tells #Person1# that #1_1#'s new restaurant should have been better. #Person2# took the food and service dose not start with the service. The restaurant nobody knows.</s>   \n",
       "8                                                                                                                                                                                                                              <pad> #Person2# helps #Person1# to know how to find a full-time job at a job center. They have binders with local job listings and computers. Otherwise, #Person1# refuses to go to a job center.</s>   \n",
       "9                                                                                                                                                                                                              <pad> Alice apologizes to Li Hong for not going to Mrs. Brown with Li Hong tomorrow morning because her mother is ill. Li Hong and Alice decide on visiting Mrs. Brown later. Are they going to visit Mrs. Brown?</s>   \n",
       "10                                                                                                                                                                                                                                                                                     <pad> #Person1# tells #Person2# a toy car for #Person2#'s son is two hundred and twenty dollars and they will take the cheapest one also.</s>   \n",
       "11                                                                                                                                                                                                           <pad> #Person1# has to register at the pharmacy, and #Person2# will make a medical record for #Person1#. #Person1# will pay 10 yuan for the registration and make the left turn until #Person2# wants to get there.</s>   \n",
       "12                                                                                                                                                                                             <pad> #Person1# wants to order some internet. #Person1# asks the whether it's better because it's connected through your phone line and dial-up is. #Person2# provides information about the differences between DEL and dial-up.</s>   \n",
       "13                                                                                                                                                                                                                                                                                                                                                     <pad> Amanda likes the peaked cap, and shows #Person2# how to choose one.</s>   \n",
       "14                                                                                                                                                                                                                                                                <pad> #Person1# finds a smell of smoke in #Person2# while #Person2# is quitting smoking and put on a pack of smokes. #Person2# feels motivated but can't quit.</s>   \n",
       "15                                                                                                                                                                                                                                      <pad> #Person1# and #Person2# are discussing the final draft of the contract and at first they're very happy. They still have a day left before they are formally  signing the contract.</s>   \n",
       "16                                                                                                                                                                                                                                                                                       <pad> #Person2# offers a custom discount about 300 yuan and 2# offers a 10 % discount on 160yuan to koyak. #Person1# accepts her offer.</s>   \n",
       "17                                                                                                                                                                                                                                                                     <pad> Allen wants to know if someone broke into #Person1#'s house. He manages to do so. Allen finds a TV and a stereo there, but the store doesn't exist.</s>   \n",
       "18                                                                                                                                                                                                                                                                                                                                                <pad> I will print out some suggestions. Others may have booked on the flight.</s>   \n",
       "19                                                                                                                                                                                                                                                                                                            <pad> #Person1# has just finished the paper. #Person2# praises #Person1#'s works and thank for the original paper.</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           response_after  \\\n",
       "0                                                                                                                                                                                                                                                    <pad> Judy and Shudd 1# hear the story of how Richard was fired.</s>   \n",
       "1                                                                                                                                                                                                                             <pad> #Person1#'ll buy ten twenties with $160 in changing at Hanna haven, to be cashed.</s>   \n",
       "2   <pad> #Person2# tells #Person1# the way to the Cross Bakery building in the opposite direction. #Person1# takes a turn while #Person1# Companies Cross Bakery building for the next few years. #Person1# doesn't know about this place. #Person2# offers #Person1# the route and #Person2# changes it in October.</s>   \n",
       "3                                                                                                                                                            <pad> #Person1# goes to the airport and tells erwartet to books #Person3# as #Person3# United express the flightns and the cuid they are doing a flight.</s>   \n",
       "4                                                                                                                                                   <pad> #Person2# wishes to retake a coffee break even for some time but #Person2# can't leave him. They make up there's agreung during #Person3#'s working breaks.</s>   \n",
       "5                                                                                             <pad> #Person1# checks out the various internet websites for information and winners from the press website, HP and AOL suggest #Person1# the detailed tips on how to use computers on the internet allow for projects.</s>   \n",
       "6                                                                                  <pad> #Person1# wants to form a music band but #Person2# wanted to play a different genre. #Person1# wanted to join a rock band, while #Person2# wants a rock band. But #Person1# gave me lots of thanks for trying the auditions.</s>   \n",
       "7                                                                                                                              <pad> #Person2# doesn't order much else at this restaurant because they don't have issues with the service of the restaurant. They are disappointed and decide to get another servers.</s>   \n",
       "8                                                                                                                                                                          <pad> #Person1# finds a job center with options for the job seeker. #Person2# helps #Person1# to find a job and would make an appointment.</s>   \n",
       "9                                                                                                                                                         <pad> Alice falls ill. Li Hong asks her to stay in home because she has to visit Ms Brown. Alice accepts the invitation. #Person1# will visit Ms Brown now.</s>   \n",
       "10                                                                                                                                         <pad> #Person1# buys a toy car for #Person2# in one hundreds and twenty. #Person1# tells #Person1# the cheapest one is a hundred and eighty and they will go to the store.</s>   \n",
       "11                                                                                                                                     <pad> #Person1# registers and calls number number #Person2#. #Person1# requests a medical register and asks #Person2# to leave money if #Person1# leave with #Person2#'s card.</s>   \n",
       "12                                                                                                                                                                             <pad> #Person1# is ordering DEL and DEL telnet internet today, while #Person2# wants to put on the phone so they convergence properly.</s>   \n",
       "13                                                                                                                                                                                                                                            <pad> Amanda shows #Person1# she loves what she thinks of her capchter.</s>   \n",
       "14                                                                                                                                                                                                                          <pad> #Person2# actually smokes and smokes a lot of cigarettes and they can't just break.</s>   \n",
       "15                                                                                                <pad> #Person1# is looking at the final draft of the contract. #Person1# brings up some issues, such as the final breakdown and quality washing the slip and #Person1# puts their \"eminction part\" on every detail.</s>   \n",
       "16                                                                                            <pad> #Person2# offers them under a sample brand and points it out. #Person2# doesn't allow the customers to sell the store and asks for a 10% discount when buying a shipment. #Polerson1# make a pact with #Person2#.</s>   \n",
       "17                                                                                                   <pad> #Person2# tells #Person1# a breakinn broke out the door. She finds out the suspect left through a window in the house. #Person2# has uncovered the robber's criminal possession. #Persongeführt** replies.</s>   \n",
       "18                                                                                                                                                                                                  <pad> [1# tells her a flight hasn't gotten through. Title says nobody's queued up but #Person1# has skins to fix.</s>   \n",
       "19                                                                                        <pad> #Person1# wants her mother to check her paper before handing to her. #Person1# says she recited the ideas inspired by #Person1#'s work. #Person1# said it was worth's all the time the teacher told her to work hard.</s>   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        1.254621      1.887931     0.633310  \n",
       "1        1.904012      2.313659     0.409647  \n",
       "2        2.641189      2.988603     0.347414  \n",
       "3        1.795023      2.037829     0.242806  \n",
       "4        1.653246      1.892593     0.239347  \n",
       "5        2.414960      2.613434     0.198474  \n",
       "6        2.734084      2.857643     0.123559  \n",
       "7        1.875288      1.992330     0.117042  \n",
       "8        1.897589      1.977381     0.079792  \n",
       "9        1.617947      1.685857     0.067909  \n",
       "10       1.261466      1.285411     0.023945  \n",
       "11       1.637714      1.553722    -0.083992  \n",
       "12       2.457580      2.326880    -0.130700  \n",
       "13       1.370748      1.222371    -0.148377  \n",
       "14       1.963959      1.631021    -0.332937  \n",
       "15       3.313624      2.864893    -0.448731  \n",
       "16       2.627102      2.104736    -0.522367  \n",
       "17       1.902421      1.272791    -0.629630  \n",
       "18       3.056041      2.210695    -0.845347  \n",
       "19       2.679392      1.747525    -0.931867  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33003e",
   "metadata": {},
   "source": [
    "## Challenges faced during the process\n",
    "\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "- Grid Search or Random Search: Experiment with different combinations of hyperparameters (like learning rate, batch size, etc.) using methods like grid search or random search.\n",
    "- Automated Tools: Consider using automated hyperparameter tuning tools like Hyperopt or Bayesian Optimization for more efficient searching.\n",
    "\n",
    "**Understanding Model Capacity in PEFT:**\n",
    "- Model Capacity: Refers to the ability of a model to capture complex patterns in data. In PEFT, you need to balance the capacity to avoid underfitting (too little capacity) and overfitting (too much capacity).\n",
    "- Tuning Approach: Adjust the extent of fine-tuning in PEFT. For instance, decide which layers to fine-tune and to what extent. Sometimes, fine-tuning only the top layers or just a fraction of all parameters is sufficient.\n",
    "\n",
    "**Managing Overfitting Risks:**\n",
    "- Regularization Techniques: Use methods like dropout, weight decay, or early stopping during training to prevent overfitting.\n",
    "- Data Splitting Strategy: Ensure proper division of data into training, validation, and test sets to evaluate the model's generalization capability effectively.\n",
    "\n",
    "\n",
    "In summary, the journey through hyperparameter optimization and understanding model capacity in PEFT involved a series of challenges, from the complexity of parameter space to balancing model capacity and avoiding overfitting. These challenges were overcome through a combination of strategic use of automated tools, methodical experimentation, and implementing best practices in model training and evaluation. This approach led to a more efficient and effective model tuning process, yielding a model that was well-balanced in terms of capacity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b12b6",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Generative-AI-with-LLMs - Retrieved from - https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/tree/5563fae447eb09be0cd1b8970fa8b07fa3b88042\n",
    "2. https://www.kaggle.com/code/yannicksteph/nlp-ppo-dialogsum-less-toxic-summarize\n",
    "3. [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n",
    "4. [Generative AI with Large Language Models | Coursera](https://www.coursera.org/learn/generative-ai-with-llms?utm_medium=sem&utm_source=gg&utm_campaign=B2C_NAMER_generative-ai-with-llms_deeplearning-ai_FTCOF_learn_country-US-country-CA&campaignid=20534248984&adgroupid=160068579824&device=c&keyword=&matchtype=&network=g&devicemodel=&adposition=&creativeid=673251286004&hide_mobile_promo&gclid=CjwKCAjwg4SpBhAKEiwAdyLwvEW_WnNyptOwzHtsGmn5-OxT5BKsQeUXHPahO-opBJ0JjsSynHkPAxoCaoAQAvD_BwE) - An informative guide that provides in-depth explanations and examples on various LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45afda",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2024 Aditi Deodhar\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
