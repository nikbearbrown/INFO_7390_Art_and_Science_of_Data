{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed56c809-2b26-4337-a31c-977b95302e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial index stats:\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "\n",
      "Processing batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 1 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 1 - Current vector count: 1229\n",
      "Total records processed so far: 500\n",
      "\n",
      "Processing batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 2 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 2 - Current vector count: 1229\n",
      "Total records processed so far: 1000\n",
      "\n",
      "Processing batch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 3 contains duplicate IDs!\n",
      "Batch 3 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 3 - Current vector count: 1229\n",
      "Total records processed so far: 1500\n",
      "WARNING: Discrepancy detected. Processed: 1500, Stored: 1229\n",
      "\n",
      "Processing batch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 4 contains 1 IDs that were seen in previous batches.\n",
      "Batch 4 contains duplicate IDs!\n",
      "Batch 4 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 4 - Current vector count: 1229\n",
      "Total records processed so far: 2000\n",
      "WARNING: Discrepancy detected. Processed: 2000, Stored: 1229\n",
      "\n",
      "Processing batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 5 contains 1 IDs that were seen in previous batches.\n",
      "Batch 5 contains duplicate IDs!\n",
      "Batch 5 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 5 - Current vector count: 1229\n",
      "Total records processed so far: 2500\n",
      "WARNING: Discrepancy detected. Processed: 2500, Stored: 1229\n",
      "\n",
      "Processing batch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 6 contains 1 IDs that were seen in previous batches.\n",
      "Batch 6 contains duplicate IDs!\n",
      "Batch 6 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 6 - Current vector count: 1229\n",
      "Total records processed so far: 3000\n",
      "WARNING: Discrepancy detected. Processed: 3000, Stored: 1229\n",
      "\n",
      "Processing batch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 7 contains 1 IDs that were seen in previous batches.\n",
      "Batch 7 contains duplicate IDs!\n",
      "Batch 7 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 7 - Current vector count: 1229\n",
      "Total records processed so far: 3500\n",
      "WARNING: Discrepancy detected. Processed: 3500, Stored: 1229\n",
      "\n",
      "Processing batch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 8 contains 1 IDs that were seen in previous batches.\n",
      "Batch 8 contains duplicate IDs!\n",
      "Batch 8 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 8 - Current vector count: 1229\n",
      "Total records processed so far: 4000\n",
      "WARNING: Discrepancy detected. Processed: 4000, Stored: 1229\n",
      "\n",
      "Processing batch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 9 contains 1 IDs that were seen in previous batches.\n",
      "Batch 9 contains duplicate IDs!\n",
      "Batch 9 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 9 - Current vector count: 1229\n",
      "Total records processed so far: 4500\n",
      "WARNING: Discrepancy detected. Processed: 4500, Stored: 1229\n",
      "\n",
      "Processing batch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 10 contains 1 IDs that were seen in previous batches.\n",
      "Batch 10 contains duplicate IDs!\n",
      "Batch 10 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 10 - Current vector count: 1229\n",
      "Total records processed so far: 5000\n",
      "WARNING: Discrepancy detected. Processed: 5000, Stored: 1229\n",
      "\n",
      "Processing batch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 11 contains 1 IDs that were seen in previous batches.\n",
      "Batch 11 contains duplicate IDs!\n",
      "Batch 11 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 11 - Current vector count: 1229\n",
      "Total records processed so far: 5500\n",
      "WARNING: Discrepancy detected. Processed: 5500, Stored: 1229\n",
      "\n",
      "Processing batch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 12 contains 1 IDs that were seen in previous batches.\n",
      "Batch 12 contains duplicate IDs!\n",
      "Batch 12 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 12 - Current vector count: 1229\n",
      "Total records processed so far: 6000\n",
      "WARNING: Discrepancy detected. Processed: 6000, Stored: 1229\n",
      "\n",
      "Processing batch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 13 contains 1 IDs that were seen in previous batches.\n",
      "Batch 13 contains duplicate IDs!\n",
      "Batch 13 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 13 - Current vector count: 1229\n",
      "Total records processed so far: 6500\n",
      "WARNING: Discrepancy detected. Processed: 6500, Stored: 1229\n",
      "\n",
      "Processing batch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 14 contains 1 IDs that were seen in previous batches.\n",
      "Batch 14 contains duplicate IDs!\n",
      "Batch 14 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 14 - Current vector count: 1229\n",
      "Total records processed so far: 7000\n",
      "WARNING: Discrepancy detected. Processed: 7000, Stored: 1229\n",
      "\n",
      "Processing batch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 15 contains 1 IDs that were seen in previous batches.\n",
      "Batch 15 contains duplicate IDs!\n",
      "Batch 15 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 15 - Current vector count: 1229\n",
      "Total records processed so far: 7500\n",
      "WARNING: Discrepancy detected. Processed: 7500, Stored: 1229\n",
      "\n",
      "Processing batch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 16 contains 1 IDs that were seen in previous batches.\n",
      "Batch 16 contains duplicate IDs!\n",
      "Batch 16 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 16 - Current vector count: 1229\n",
      "Total records processed so far: 8000\n",
      "WARNING: Discrepancy detected. Processed: 8000, Stored: 1229\n",
      "\n",
      "Processing batch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 17 contains 1 IDs that were seen in previous batches.\n",
      "Batch 17 contains duplicate IDs!\n",
      "Batch 17 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 17 - Current vector count: 1229\n",
      "Total records processed so far: 8500\n",
      "WARNING: Discrepancy detected. Processed: 8500, Stored: 1229\n",
      "\n",
      "Processing batch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 18 contains 1 IDs that were seen in previous batches.\n",
      "Batch 18 contains duplicate IDs!\n",
      "Batch 18 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 18 - Current vector count: 1229\n",
      "Total records processed so far: 9000\n",
      "WARNING: Discrepancy detected. Processed: 9000, Stored: 1229\n",
      "\n",
      "Processing batch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 19 contains 1 IDs that were seen in previous batches.\n",
      "Batch 19 contains duplicate IDs!\n",
      "Batch 19 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 19 - Current vector count: 1229\n",
      "Total records processed so far: 9500\n",
      "WARNING: Discrepancy detected. Processed: 9500, Stored: 1229\n",
      "\n",
      "Processing batch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 20 contains 1 IDs that were seen in previous batches.\n",
      "Batch 20 contains duplicate IDs!\n",
      "Batch 20 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 20 - Current vector count: 1229\n",
      "Total records processed so far: 10000\n",
      "WARNING: Discrepancy detected. Processed: 10000, Stored: 1229\n",
      "\n",
      "Processing batch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 21 contains 1 IDs that were seen in previous batches.\n",
      "Batch 21 contains duplicate IDs!\n",
      "Batch 21 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 21 - Current vector count: 1229\n",
      "Total records processed so far: 10500\n",
      "WARNING: Discrepancy detected. Processed: 10500, Stored: 1229\n",
      "\n",
      "Processing batch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 22 contains 1 IDs that were seen in previous batches.\n",
      "Batch 22 contains duplicate IDs!\n",
      "Batch 22 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 22 - Current vector count: 1229\n",
      "Total records processed so far: 11000\n",
      "WARNING: Discrepancy detected. Processed: 11000, Stored: 1229\n",
      "\n",
      "Processing batch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 23 contains 1 IDs that were seen in previous batches.\n",
      "Batch 23 contains duplicate IDs!\n",
      "Batch 23 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 23 - Current vector count: 1229\n",
      "Total records processed so far: 11500\n",
      "WARNING: Discrepancy detected. Processed: 11500, Stored: 1229\n",
      "\n",
      "Processing batch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 24 contains 1 IDs that were seen in previous batches.\n",
      "Batch 24 contains duplicate IDs!\n",
      "Batch 24 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 24 - Current vector count: 1229\n",
      "Total records processed so far: 12000\n",
      "WARNING: Discrepancy detected. Processed: 12000, Stored: 1229\n",
      "\n",
      "Processing batch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 25 contains 1 IDs that were seen in previous batches.\n",
      "Batch 25 contains duplicate IDs!\n",
      "Batch 25 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 25 - Current vector count: 1229\n",
      "Total records processed so far: 12500\n",
      "WARNING: Discrepancy detected. Processed: 12500, Stored: 1229\n",
      "\n",
      "Processing batch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 26 contains 1 IDs that were seen in previous batches.\n",
      "Batch 26 contains duplicate IDs!\n",
      "Batch 26 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 26 - Current vector count: 1229\n",
      "Total records processed so far: 13000\n",
      "WARNING: Discrepancy detected. Processed: 13000, Stored: 1229\n",
      "\n",
      "Processing batch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 27 contains 1 IDs that were seen in previous batches.\n",
      "Batch 27 contains duplicate IDs!\n",
      "Batch 27 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 27 - Current vector count: 1229\n",
      "Total records processed so far: 13500\n",
      "WARNING: Discrepancy detected. Processed: 13500, Stored: 1229\n",
      "\n",
      "Processing batch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 28 contains 1 IDs that were seen in previous batches.\n",
      "Batch 28 contains duplicate IDs!\n",
      "Batch 28 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 28 - Current vector count: 1229\n",
      "Total records processed so far: 14000\n",
      "WARNING: Discrepancy detected. Processed: 14000, Stored: 1229\n",
      "\n",
      "Processing batch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 29 contains 1 IDs that were seen in previous batches.\n",
      "Batch 29 contains duplicate IDs!\n",
      "Batch 29 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 29 - Current vector count: 1229\n",
      "Total records processed so far: 14500\n",
      "WARNING: Discrepancy detected. Processed: 14500, Stored: 1229\n",
      "\n",
      "Processing batch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 30 contains 1 IDs that were seen in previous batches.\n",
      "Batch 30 contains duplicate IDs!\n",
      "Batch 30 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 30 - Current vector count: 1229\n",
      "Total records processed so far: 15000\n",
      "WARNING: Discrepancy detected. Processed: 15000, Stored: 1229\n",
      "\n",
      "Processing batch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 31 contains 1 IDs that were seen in previous batches.\n",
      "Batch 31 contains duplicate IDs!\n",
      "Batch 31 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 31 - Current vector count: 1229\n",
      "Total records processed so far: 15500\n",
      "WARNING: Discrepancy detected. Processed: 15500, Stored: 1229\n",
      "\n",
      "Processing batch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 32 contains 1 IDs that were seen in previous batches.\n",
      "Batch 32 contains duplicate IDs!\n",
      "Batch 32 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 32 - Current vector count: 1229\n",
      "Total records processed so far: 16000\n",
      "WARNING: Discrepancy detected. Processed: 16000, Stored: 1229\n",
      "\n",
      "Processing batch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 33 contains 1 IDs that were seen in previous batches.\n",
      "Batch 33 contains duplicate IDs!\n",
      "Batch 33 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 33 - Current vector count: 1229\n",
      "Total records processed so far: 16500\n",
      "WARNING: Discrepancy detected. Processed: 16500, Stored: 1229\n",
      "\n",
      "Processing batch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 34 contains 1 IDs that were seen in previous batches.\n",
      "Batch 34 contains duplicate IDs!\n",
      "Batch 34 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 34 - Current vector count: 1229\n",
      "Total records processed so far: 17000\n",
      "WARNING: Discrepancy detected. Processed: 17000, Stored: 1229\n",
      "\n",
      "Processing batch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 35 contains 1 IDs that were seen in previous batches.\n",
      "Batch 35 contains duplicate IDs!\n",
      "Batch 35 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 35 - Current vector count: 1229\n",
      "Total records processed so far: 17500\n",
      "WARNING: Discrepancy detected. Processed: 17500, Stored: 1229\n",
      "\n",
      "Processing batch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 36 contains 1 IDs that were seen in previous batches.\n",
      "Batch 36 contains duplicate IDs!\n",
      "Batch 36 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 36 - Current vector count: 1229\n",
      "Total records processed so far: 18000\n",
      "WARNING: Discrepancy detected. Processed: 18000, Stored: 1229\n",
      "\n",
      "Processing batch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 37 contains 1 IDs that were seen in previous batches.\n",
      "Batch 37 contains duplicate IDs!\n",
      "Batch 37 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 37 - Current vector count: 1229\n",
      "Total records processed so far: 18500\n",
      "WARNING: Discrepancy detected. Processed: 18500, Stored: 1229\n",
      "\n",
      "Processing batch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 38 contains 1 IDs that were seen in previous batches.\n",
      "Batch 38 contains duplicate IDs!\n",
      "Batch 38 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 38 - Current vector count: 1229\n",
      "Total records processed so far: 19000\n",
      "WARNING: Discrepancy detected. Processed: 19000, Stored: 1229\n",
      "\n",
      "Processing batch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 39 contains 1 IDs that were seen in previous batches.\n",
      "Batch 39 contains duplicate IDs!\n",
      "Batch 39 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 39 - Current vector count: 1229\n",
      "Total records processed so far: 19500\n",
      "WARNING: Discrepancy detected. Processed: 19500, Stored: 1229\n",
      "\n",
      "Processing batch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 40 contains 1 IDs that were seen in previous batches.\n",
      "Batch 40 contains duplicate IDs!\n",
      "Batch 40 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 40 - Current vector count: 1229\n",
      "Total records processed so far: 20000\n",
      "WARNING: Discrepancy detected. Processed: 20000, Stored: 1229\n",
      "\n",
      "Processing batch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 41 contains 1 IDs that were seen in previous batches.\n",
      "Batch 41 contains duplicate IDs!\n",
      "Batch 41 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 41 - Current vector count: 1229\n",
      "Total records processed so far: 20500\n",
      "WARNING: Discrepancy detected. Processed: 20500, Stored: 1229\n",
      "\n",
      "Processing batch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 42 contains 1 IDs that were seen in previous batches.\n",
      "Batch 42 contains duplicate IDs!\n",
      "Batch 42 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 42 - Current vector count: 1229\n",
      "Total records processed so far: 21000\n",
      "WARNING: Discrepancy detected. Processed: 21000, Stored: 1229\n",
      "\n",
      "Processing batch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 43 contains 1 IDs that were seen in previous batches.\n",
      "Batch 43 contains duplicate IDs!\n",
      "Batch 43 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 43 - Current vector count: 1229\n",
      "Total records processed so far: 21500\n",
      "WARNING: Discrepancy detected. Processed: 21500, Stored: 1229\n",
      "\n",
      "Processing batch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 44 contains 1 IDs that were seen in previous batches.\n",
      "Batch 44 contains duplicate IDs!\n",
      "Batch 44 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 44 - Current vector count: 1229\n",
      "Total records processed so far: 22000\n",
      "WARNING: Discrepancy detected. Processed: 22000, Stored: 1229\n",
      "\n",
      "Processing batch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 45 contains 1 IDs that were seen in previous batches.\n",
      "Batch 45 contains duplicate IDs!\n",
      "Batch 45 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 45 - Current vector count: 1229\n",
      "Total records processed so far: 22500\n",
      "WARNING: Discrepancy detected. Processed: 22500, Stored: 1229\n",
      "\n",
      "Processing batch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 46 contains 1 IDs that were seen in previous batches.\n",
      "Batch 46 contains duplicate IDs!\n",
      "Batch 46 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 46 - Current vector count: 1229\n",
      "Total records processed so far: 23000\n",
      "WARNING: Discrepancy detected. Processed: 23000, Stored: 1229\n",
      "\n",
      "Processing batch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 47 contains 1 IDs that were seen in previous batches.\n",
      "Batch 47 contains duplicate IDs!\n",
      "Batch 47 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 47 - Current vector count: 1229\n",
      "Total records processed so far: 23500\n",
      "WARNING: Discrepancy detected. Processed: 23500, Stored: 1229\n",
      "\n",
      "Processing batch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 48 contains 1 IDs that were seen in previous batches.\n",
      "Batch 48 contains duplicate IDs!\n",
      "Batch 48 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 48 - Current vector count: 1229\n",
      "Total records processed so far: 24000\n",
      "WARNING: Discrepancy detected. Processed: 24000, Stored: 1229\n",
      "\n",
      "Processing batch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 49 contains 1 IDs that were seen in previous batches.\n",
      "Batch 49 contains duplicate IDs!\n",
      "Batch 49 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 49 - Current vector count: 1229\n",
      "Total records processed so far: 24500\n",
      "WARNING: Discrepancy detected. Processed: 24500, Stored: 1229\n",
      "\n",
      "Processing batch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 50 contains 1 IDs that were seen in previous batches.\n",
      "Batch 50 contains duplicate IDs!\n",
      "Batch 50 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 50 - Current vector count: 1229\n",
      "Total records processed so far: 25000\n",
      "WARNING: Discrepancy detected. Processed: 25000, Stored: 1229\n",
      "\n",
      "Processing batch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 51 contains 1 IDs that were seen in previous batches.\n",
      "Batch 51 contains duplicate IDs!\n",
      "Batch 51 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 51 - Current vector count: 1229\n",
      "Total records processed so far: 25500\n",
      "WARNING: Discrepancy detected. Processed: 25500, Stored: 1229\n",
      "\n",
      "Processing batch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 52 contains 1 IDs that were seen in previous batches.\n",
      "Batch 52 contains duplicate IDs!\n",
      "Batch 52 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 52 - Current vector count: 1229\n",
      "Total records processed so far: 26000\n",
      "WARNING: Discrepancy detected. Processed: 26000, Stored: 1229\n",
      "\n",
      "Processing batch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 53 contains 1 IDs that were seen in previous batches.\n",
      "Batch 53 contains duplicate IDs!\n",
      "Batch 53 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 53 - Current vector count: 1229\n",
      "Total records processed so far: 26500\n",
      "WARNING: Discrepancy detected. Processed: 26500, Stored: 1229\n",
      "\n",
      "Processing batch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 54 contains 1 IDs that were seen in previous batches.\n",
      "Batch 54 contains duplicate IDs!\n",
      "Batch 54 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 54 - Current vector count: 1229\n",
      "Total records processed so far: 27000\n",
      "WARNING: Discrepancy detected. Processed: 27000, Stored: 1229\n",
      "\n",
      "Processing batch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 55 contains 1 IDs that were seen in previous batches.\n",
      "Batch 55 contains duplicate IDs!\n",
      "Batch 55 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 55 - Current vector count: 1229\n",
      "Total records processed so far: 27500\n",
      "WARNING: Discrepancy detected. Processed: 27500, Stored: 1229\n",
      "\n",
      "Processing batch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 56 contains 1 IDs that were seen in previous batches.\n",
      "Batch 56 contains duplicate IDs!\n",
      "Batch 56 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 56 - Current vector count: 1229\n",
      "Total records processed so far: 28000\n",
      "WARNING: Discrepancy detected. Processed: 28000, Stored: 1229\n",
      "\n",
      "Processing batch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 57 contains 1 IDs that were seen in previous batches.\n",
      "Batch 57 contains duplicate IDs!\n",
      "Batch 57 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 57 - Current vector count: 1229\n",
      "Total records processed so far: 28500\n",
      "WARNING: Discrepancy detected. Processed: 28500, Stored: 1229\n",
      "\n",
      "Processing batch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 58 contains 1 IDs that were seen in previous batches.\n",
      "Batch 58 contains duplicate IDs!\n",
      "Batch 58 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 58 - Current vector count: 1229\n",
      "Total records processed so far: 29000\n",
      "WARNING: Discrepancy detected. Processed: 29000, Stored: 1229\n",
      "\n",
      "Processing batch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 59 contains 1 IDs that were seen in previous batches.\n",
      "Batch 59 contains duplicate IDs!\n",
      "Batch 59 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 59 - Current vector count: 1229\n",
      "Total records processed so far: 29500\n",
      "WARNING: Discrepancy detected. Processed: 29500, Stored: 1229\n",
      "\n",
      "Processing batch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 60 contains 1 IDs that were seen in previous batches.\n",
      "Batch 60 contains duplicate IDs!\n",
      "Batch 60 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 60 - Current vector count: 1229\n",
      "Total records processed so far: 30000\n",
      "WARNING: Discrepancy detected. Processed: 30000, Stored: 1229\n",
      "\n",
      "Processing batch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 61 contains 1 IDs that were seen in previous batches.\n",
      "Batch 61 contains duplicate IDs!\n",
      "Batch 61 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 61 - Current vector count: 1229\n",
      "Total records processed so far: 30500\n",
      "WARNING: Discrepancy detected. Processed: 30500, Stored: 1229\n",
      "\n",
      "Processing batch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 62 contains 1 IDs that were seen in previous batches.\n",
      "Batch 62 contains duplicate IDs!\n",
      "Batch 62 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 62 - Current vector count: 1229\n",
      "Total records processed so far: 31000\n",
      "WARNING: Discrepancy detected. Processed: 31000, Stored: 1229\n",
      "\n",
      "Processing batch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 63 contains 1 IDs that were seen in previous batches.\n",
      "Batch 63 contains duplicate IDs!\n",
      "Batch 63 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 63 - Current vector count: 1229\n",
      "Total records processed so far: 31500\n",
      "WARNING: Discrepancy detected. Processed: 31500, Stored: 1229\n",
      "\n",
      "Processing batch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 64 contains 1 IDs that were seen in previous batches.\n",
      "Batch 64 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 64:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 65 contains 1 IDs that were seen in previous batches.\n",
      "Batch 65 contains duplicate IDs!\n",
      "Batch 65 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 65 - Current vector count: 1229\n",
      "Total records processed so far: 32000\n",
      "WARNING: Discrepancy detected. Processed: 32000, Stored: 1229\n",
      "\n",
      "Processing batch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 66 contains 1 IDs that were seen in previous batches.\n",
      "Batch 66 contains duplicate IDs!\n",
      "Batch 66 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 66 - Current vector count: 1229\n",
      "Total records processed so far: 32500\n",
      "WARNING: Discrepancy detected. Processed: 32500, Stored: 1229\n",
      "\n",
      "Processing batch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 67 contains 1 IDs that were seen in previous batches.\n",
      "Batch 67 contains duplicate IDs!\n",
      "Batch 67 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 67 - Current vector count: 1229\n",
      "Total records processed so far: 33000\n",
      "WARNING: Discrepancy detected. Processed: 33000, Stored: 1229\n",
      "\n",
      "Processing batch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 68 contains 1 IDs that were seen in previous batches.\n",
      "Batch 68 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 68:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 69 contains 1 IDs that were seen in previous batches.\n",
      "Batch 69 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 69:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 70 contains 1 IDs that were seen in previous batches.\n",
      "Batch 70 contains duplicate IDs!\n",
      "Batch 70 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 70 - Current vector count: 1229\n",
      "Total records processed so far: 33500\n",
      "WARNING: Discrepancy detected. Processed: 33500, Stored: 1229\n",
      "\n",
      "Processing batch 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 71 contains 1 IDs that were seen in previous batches.\n",
      "Batch 71 contains duplicate IDs!\n",
      "Batch 71 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 71 - Current vector count: 1229\n",
      "Total records processed so far: 34000\n",
      "WARNING: Discrepancy detected. Processed: 34000, Stored: 1229\n",
      "\n",
      "Processing batch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 72 contains 1 IDs that were seen in previous batches.\n",
      "Batch 72 contains duplicate IDs!\n",
      "Batch 72 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 72 - Current vector count: 1229\n",
      "Total records processed so far: 34500\n",
      "WARNING: Discrepancy detected. Processed: 34500, Stored: 1229\n",
      "\n",
      "Processing batch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 73 contains 1 IDs that were seen in previous batches.\n",
      "Batch 73 contains duplicate IDs!\n",
      "Batch 73 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 73 - Current vector count: 1229\n",
      "Total records processed so far: 35000\n",
      "WARNING: Discrepancy detected. Processed: 35000, Stored: 1229\n",
      "\n",
      "Processing batch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 74 contains 1 IDs that were seen in previous batches.\n",
      "Batch 74 contains duplicate IDs!\n",
      "Batch 74 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 74 - Current vector count: 1229\n",
      "Total records processed so far: 35500\n",
      "WARNING: Discrepancy detected. Processed: 35500, Stored: 1229\n",
      "\n",
      "Processing batch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 75 contains 1 IDs that were seen in previous batches.\n",
      "Batch 75 contains duplicate IDs!\n",
      "Batch 75 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 75 - Current vector count: 1229\n",
      "Total records processed so far: 36000\n",
      "WARNING: Discrepancy detected. Processed: 36000, Stored: 1229\n",
      "\n",
      "Processing batch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 76 contains 1 IDs that were seen in previous batches.\n",
      "Batch 76 contains duplicate IDs!\n",
      "Batch 76 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 76 - Current vector count: 1229\n",
      "Total records processed so far: 36500\n",
      "WARNING: Discrepancy detected. Processed: 36500, Stored: 1229\n",
      "\n",
      "Processing batch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 77 contains 1 IDs that were seen in previous batches.\n",
      "Batch 77 contains duplicate IDs!\n",
      "Batch 77 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 77 - Current vector count: 1229\n",
      "Total records processed so far: 37000\n",
      "WARNING: Discrepancy detected. Processed: 37000, Stored: 1229\n",
      "\n",
      "Processing batch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 78 contains 1 IDs that were seen in previous batches.\n",
      "Batch 78 contains duplicate IDs!\n",
      "Batch 78 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 78 - Current vector count: 1229\n",
      "Total records processed so far: 37500\n",
      "WARNING: Discrepancy detected. Processed: 37500, Stored: 1229\n",
      "\n",
      "Processing batch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 79 contains 1 IDs that were seen in previous batches.\n",
      "Batch 79 contains duplicate IDs!\n",
      "Batch 79 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 79 - Current vector count: 1229\n",
      "Total records processed so far: 38000\n",
      "WARNING: Discrepancy detected. Processed: 38000, Stored: 1229\n",
      "\n",
      "Processing batch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 80 contains 1 IDs that were seen in previous batches.\n",
      "Batch 80 contains duplicate IDs!\n",
      "Batch 80 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 80 - Current vector count: 1229\n",
      "Total records processed so far: 38500\n",
      "WARNING: Discrepancy detected. Processed: 38500, Stored: 1229\n",
      "\n",
      "Processing batch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 81 contains 1 IDs that were seen in previous batches.\n",
      "Batch 81 contains duplicate IDs!\n",
      "Batch 81 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 81 - Current vector count: 1229\n",
      "Total records processed so far: 39000\n",
      "WARNING: Discrepancy detected. Processed: 39000, Stored: 1229\n",
      "\n",
      "Processing batch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 82 contains 1 IDs that were seen in previous batches.\n",
      "Batch 82 contains duplicate IDs!\n",
      "Batch 82 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 82 - Current vector count: 1229\n",
      "Total records processed so far: 39500\n",
      "WARNING: Discrepancy detected. Processed: 39500, Stored: 1229\n",
      "\n",
      "Processing batch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 83 contains 1 IDs that were seen in previous batches.\n",
      "Batch 83 contains duplicate IDs!\n",
      "Batch 83 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 83 - Current vector count: 1229\n",
      "Total records processed so far: 40000\n",
      "WARNING: Discrepancy detected. Processed: 40000, Stored: 1229\n",
      "\n",
      "Processing batch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 84 contains 1 IDs that were seen in previous batches.\n",
      "Batch 84 contains duplicate IDs!\n",
      "Batch 84 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 84 - Current vector count: 1229\n",
      "Total records processed so far: 40500\n",
      "WARNING: Discrepancy detected. Processed: 40500, Stored: 1229\n",
      "\n",
      "Processing batch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 85 contains 1 IDs that were seen in previous batches.\n",
      "Batch 85 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 85:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 86 contains 1 IDs that were seen in previous batches.\n",
      "Batch 86 contains duplicate IDs!\n",
      "Batch 86 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 86 - Current vector count: 1229\n",
      "Total records processed so far: 41000\n",
      "WARNING: Discrepancy detected. Processed: 41000, Stored: 1229\n",
      "\n",
      "Processing batch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 87 contains 1 IDs that were seen in previous batches.\n",
      "Batch 87 contains duplicate IDs!\n",
      "Batch 87 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 87 - Current vector count: 1229\n",
      "Total records processed so far: 41500\n",
      "WARNING: Discrepancy detected. Processed: 41500, Stored: 1229\n",
      "\n",
      "Processing batch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 88 contains 1 IDs that were seen in previous batches.\n",
      "Batch 88 contains duplicate IDs!\n",
      "Batch 88 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 88 - Current vector count: 1229\n",
      "Total records processed so far: 42000\n",
      "WARNING: Discrepancy detected. Processed: 42000, Stored: 1229\n",
      "\n",
      "Processing batch 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 89 contains 1 IDs that were seen in previous batches.\n",
      "Batch 89 contains duplicate IDs!\n",
      "Batch 89 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 89 - Current vector count: 1229\n",
      "Total records processed so far: 42500\n",
      "WARNING: Discrepancy detected. Processed: 42500, Stored: 1229\n",
      "\n",
      "Processing batch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 90 contains 1 IDs that were seen in previous batches.\n",
      "Batch 90 contains duplicate IDs!\n",
      "Batch 90 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 90 - Current vector count: 1229\n",
      "Total records processed so far: 43000\n",
      "WARNING: Discrepancy detected. Processed: 43000, Stored: 1229\n",
      "\n",
      "Processing batch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 91 contains 1 IDs that were seen in previous batches.\n",
      "Batch 91 contains duplicate IDs!\n",
      "Batch 91 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 91 - Current vector count: 1229\n",
      "Total records processed so far: 43500\n",
      "WARNING: Discrepancy detected. Processed: 43500, Stored: 1229\n",
      "\n",
      "Processing batch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 92 contains 1 IDs that were seen in previous batches.\n",
      "Batch 92 contains duplicate IDs!\n",
      "Batch 92 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 92 - Current vector count: 1229\n",
      "Total records processed so far: 44000\n",
      "WARNING: Discrepancy detected. Processed: 44000, Stored: 1229\n",
      "\n",
      "Processing batch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 93 contains 1 IDs that were seen in previous batches.\n",
      "Batch 93 contains duplicate IDs!\n",
      "Batch 93 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 93 - Current vector count: 1229\n",
      "Total records processed so far: 44500\n",
      "WARNING: Discrepancy detected. Processed: 44500, Stored: 1229\n",
      "\n",
      "Processing batch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 94 contains 1 IDs that were seen in previous batches.\n",
      "Batch 94 contains duplicate IDs!\n",
      "Batch 94 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 94 - Current vector count: 1229\n",
      "Total records processed so far: 45000\n",
      "WARNING: Discrepancy detected. Processed: 45000, Stored: 1229\n",
      "\n",
      "Processing batch 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 95 contains 1 IDs that were seen in previous batches.\n",
      "Batch 95 contains duplicate IDs!\n",
      "Batch 95 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 95 - Current vector count: 1229\n",
      "Total records processed so far: 45500\n",
      "WARNING: Discrepancy detected. Processed: 45500, Stored: 1229\n",
      "\n",
      "Processing batch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 96 contains 1 IDs that were seen in previous batches.\n",
      "Batch 96 contains duplicate IDs!\n",
      "Batch 96 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 96 - Current vector count: 1229\n",
      "Total records processed so far: 46000\n",
      "WARNING: Discrepancy detected. Processed: 46000, Stored: 1229\n",
      "\n",
      "Processing batch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 97 contains 1 IDs that were seen in previous batches.\n",
      "Batch 97 contains duplicate IDs!\n",
      "Batch 97 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 97 - Current vector count: 1229\n",
      "Total records processed so far: 46500\n",
      "WARNING: Discrepancy detected. Processed: 46500, Stored: 1229\n",
      "\n",
      "Processing batch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 98 contains 1 IDs that were seen in previous batches.\n",
      "Batch 98 contains duplicate IDs!\n",
      "Batch 98 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 98 - Current vector count: 1229\n",
      "Total records processed so far: 47000\n",
      "WARNING: Discrepancy detected. Processed: 47000, Stored: 1229\n",
      "\n",
      "Processing batch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 99 contains 1 IDs that were seen in previous batches.\n",
      "Batch 99 contains duplicate IDs!\n",
      "Batch 99 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 99 - Current vector count: 1229\n",
      "Total records processed so far: 47500\n",
      "WARNING: Discrepancy detected. Processed: 47500, Stored: 1229\n",
      "\n",
      "Processing batch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 100 contains 1 IDs that were seen in previous batches.\n",
      "Batch 100 contains duplicate IDs!\n",
      "Batch 100 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 100 - Current vector count: 1229\n",
      "Total records processed so far: 48000\n",
      "WARNING: Discrepancy detected. Processed: 48000, Stored: 1229\n",
      "\n",
      "Processing batch 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 101 contains 1 IDs that were seen in previous batches.\n",
      "Batch 101 contains duplicate IDs!\n",
      "Batch 101 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 101 - Current vector count: 1229\n",
      "Total records processed so far: 48500\n",
      "WARNING: Discrepancy detected. Processed: 48500, Stored: 1229\n",
      "\n",
      "Processing batch 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 102 contains 1 IDs that were seen in previous batches.\n",
      "Batch 102 contains duplicate IDs!\n",
      "Batch 102 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 102 - Current vector count: 1229\n",
      "Total records processed so far: 49000\n",
      "WARNING: Discrepancy detected. Processed: 49000, Stored: 1229\n",
      "\n",
      "Processing batch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 103 contains 1 IDs that were seen in previous batches.\n",
      "Batch 103 contains duplicate IDs!\n",
      "Batch 103 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 103 - Current vector count: 1229\n",
      "Total records processed so far: 49500\n",
      "WARNING: Discrepancy detected. Processed: 49500, Stored: 1229\n",
      "\n",
      "Processing batch 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 104 contains 1 IDs that were seen in previous batches.\n",
      "Batch 104 contains duplicate IDs!\n",
      "Batch 104 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 104 - Current vector count: 1229\n",
      "Total records processed so far: 50000\n",
      "WARNING: Discrepancy detected. Processed: 50000, Stored: 1229\n",
      "\n",
      "Processing batch 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 105 contains 1 IDs that were seen in previous batches.\n",
      "Batch 105 contains duplicate IDs!\n",
      "Batch 105 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 105 - Current vector count: 1229\n",
      "Total records processed so far: 50500\n",
      "WARNING: Discrepancy detected. Processed: 50500, Stored: 1229\n",
      "\n",
      "Processing batch 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 106 contains 1 IDs that were seen in previous batches.\n",
      "Batch 106 contains duplicate IDs!\n",
      "Batch 106 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 106 - Current vector count: 1229\n",
      "Total records processed so far: 51000\n",
      "WARNING: Discrepancy detected. Processed: 51000, Stored: 1229\n",
      "\n",
      "Processing batch 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 107 contains 1 IDs that were seen in previous batches.\n",
      "Batch 107 contains duplicate IDs!\n",
      "Batch 107 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 107 - Current vector count: 1229\n",
      "Total records processed so far: 51500\n",
      "WARNING: Discrepancy detected. Processed: 51500, Stored: 1229\n",
      "\n",
      "Processing batch 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 108 contains 1 IDs that were seen in previous batches.\n",
      "Batch 108 contains duplicate IDs!\n",
      "Batch 108 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 108 - Current vector count: 1229\n",
      "Total records processed so far: 52000\n",
      "WARNING: Discrepancy detected. Processed: 52000, Stored: 1229\n",
      "\n",
      "Processing batch 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 109 contains 1 IDs that were seen in previous batches.\n",
      "Batch 109 contains duplicate IDs!\n",
      "Batch 109 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 109 - Current vector count: 1229\n",
      "Total records processed so far: 52500\n",
      "WARNING: Discrepancy detected. Processed: 52500, Stored: 1229\n",
      "\n",
      "Processing batch 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 110 contains 1 IDs that were seen in previous batches.\n",
      "Batch 110 contains duplicate IDs!\n",
      "Batch 110 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 110 - Current vector count: 1229\n",
      "Total records processed so far: 53000\n",
      "WARNING: Discrepancy detected. Processed: 53000, Stored: 1229\n",
      "\n",
      "Processing batch 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 111 contains 1 IDs that were seen in previous batches.\n",
      "Batch 111 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 111:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 112 contains 1 IDs that were seen in previous batches.\n",
      "Batch 112 contains duplicate IDs!\n",
      "Batch 112 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 112 - Current vector count: 1229\n",
      "Total records processed so far: 53500\n",
      "WARNING: Discrepancy detected. Processed: 53500, Stored: 1229\n",
      "\n",
      "Processing batch 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 113 contains 1 IDs that were seen in previous batches.\n",
      "Batch 113 contains duplicate IDs!\n",
      "Batch 113 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 113 - Current vector count: 1229\n",
      "Total records processed so far: 54000\n",
      "WARNING: Discrepancy detected. Processed: 54000, Stored: 1229\n",
      "\n",
      "Processing batch 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 114 contains 1 IDs that were seen in previous batches.\n",
      "Batch 114 contains duplicate IDs!\n",
      "Batch 114 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 114 - Current vector count: 1229\n",
      "Total records processed so far: 54500\n",
      "WARNING: Discrepancy detected. Processed: 54500, Stored: 1229\n",
      "\n",
      "Processing batch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 115 contains 1 IDs that were seen in previous batches.\n",
      "Batch 115 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 115:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 116 contains 1 IDs that were seen in previous batches.\n",
      "Batch 116 contains duplicate IDs!\n",
      "Batch 116 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 116 - Current vector count: 1229\n",
      "Total records processed so far: 55000\n",
      "WARNING: Discrepancy detected. Processed: 55000, Stored: 1229\n",
      "\n",
      "Processing batch 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 117 contains 1 IDs that were seen in previous batches.\n",
      "Batch 117 contains duplicate IDs!\n",
      "Batch 117 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 117 - Current vector count: 1229\n",
      "Total records processed so far: 55500\n",
      "WARNING: Discrepancy detected. Processed: 55500, Stored: 1229\n",
      "\n",
      "Processing batch 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 118 contains 1 IDs that were seen in previous batches.\n",
      "Batch 118 contains duplicate IDs!\n",
      "Batch 118 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 118 - Current vector count: 1229\n",
      "Total records processed so far: 56000\n",
      "WARNING: Discrepancy detected. Processed: 56000, Stored: 1229\n",
      "\n",
      "Processing batch 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 119 contains 1 IDs that were seen in previous batches.\n",
      "Batch 119 contains duplicate IDs!\n",
      "Batch 119 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 119 - Current vector count: 1229\n",
      "Total records processed so far: 56500\n",
      "WARNING: Discrepancy detected. Processed: 56500, Stored: 1229\n",
      "\n",
      "Processing batch 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 120 contains 1 IDs that were seen in previous batches.\n",
      "Batch 120 contains duplicate IDs!\n",
      "Batch 120 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 120 - Current vector count: 1229\n",
      "Total records processed so far: 57000\n",
      "WARNING: Discrepancy detected. Processed: 57000, Stored: 1229\n",
      "\n",
      "Processing batch 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 121 contains 1 IDs that were seen in previous batches.\n",
      "Batch 121 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 121:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 122 contains 1 IDs that were seen in previous batches.\n",
      "Batch 122 contains duplicate IDs!\n",
      "Batch 122 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 122 - Current vector count: 1229\n",
      "Total records processed so far: 57500\n",
      "WARNING: Discrepancy detected. Processed: 57500, Stored: 1229\n",
      "\n",
      "Processing batch 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 123 contains 1 IDs that were seen in previous batches.\n",
      "Batch 123 contains duplicate IDs!\n",
      "Batch 123 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 123 - Current vector count: 1229\n",
      "Total records processed so far: 58000\n",
      "WARNING: Discrepancy detected. Processed: 58000, Stored: 1229\n",
      "\n",
      "Processing batch 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 124 contains 1 IDs that were seen in previous batches.\n",
      "Batch 124 contains duplicate IDs!\n",
      "Batch 124 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 124 - Current vector count: 1229\n",
      "Total records processed so far: 58500\n",
      "WARNING: Discrepancy detected. Processed: 58500, Stored: 1229\n",
      "\n",
      "Processing batch 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 125 contains 1 IDs that were seen in previous batches.\n",
      "Batch 125 contains duplicate IDs!\n",
      "Batch 125 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 125 - Current vector count: 1229\n",
      "Total records processed so far: 59000\n",
      "WARNING: Discrepancy detected. Processed: 59000, Stored: 1229\n",
      "\n",
      "Processing batch 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 126 contains 1 IDs that were seen in previous batches.\n",
      "Batch 126 contains duplicate IDs!\n",
      "Batch 126 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 126 - Current vector count: 1229\n",
      "Total records processed so far: 59500\n",
      "WARNING: Discrepancy detected. Processed: 59500, Stored: 1229\n",
      "\n",
      "Processing batch 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 127 contains 1 IDs that were seen in previous batches.\n",
      "Batch 127 contains duplicate IDs!\n",
      "Batch 127 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 127 - Current vector count: 1229\n",
      "Total records processed so far: 60000\n",
      "WARNING: Discrepancy detected. Processed: 60000, Stored: 1229\n",
      "\n",
      "Processing batch 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 128 contains 1 IDs that were seen in previous batches.\n",
      "Batch 128 contains duplicate IDs!\n",
      "Batch 128 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 128 - Current vector count: 1229\n",
      "Total records processed so far: 60500\n",
      "WARNING: Discrepancy detected. Processed: 60500, Stored: 1229\n",
      "\n",
      "Processing batch 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 129 contains 1 IDs that were seen in previous batches.\n",
      "Batch 129 contains duplicate IDs!\n",
      "Batch 129 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 129 - Current vector count: 1229\n",
      "Total records processed so far: 61000\n",
      "WARNING: Discrepancy detected. Processed: 61000, Stored: 1229\n",
      "\n",
      "Processing batch 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 130 contains 1 IDs that were seen in previous batches.\n",
      "Batch 130 contains duplicate IDs!\n",
      "Batch 130 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 130 - Current vector count: 1229\n",
      "Total records processed so far: 61500\n",
      "WARNING: Discrepancy detected. Processed: 61500, Stored: 1229\n",
      "\n",
      "Processing batch 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 131 contains 1 IDs that were seen in previous batches.\n",
      "Batch 131 contains duplicate IDs!\n",
      "Batch 131 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 131 - Current vector count: 1229\n",
      "Total records processed so far: 62000\n",
      "WARNING: Discrepancy detected. Processed: 62000, Stored: 1229\n",
      "\n",
      "Processing batch 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 132 contains 1 IDs that were seen in previous batches.\n",
      "Batch 132 contains duplicate IDs!\n",
      "Batch 132 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 132 - Current vector count: 1229\n",
      "Total records processed so far: 62500\n",
      "WARNING: Discrepancy detected. Processed: 62500, Stored: 1229\n",
      "\n",
      "Processing batch 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 133 contains 1 IDs that were seen in previous batches.\n",
      "Batch 133 contains duplicate IDs!\n",
      "Batch 133 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 133 - Current vector count: 1229\n",
      "Total records processed so far: 63000\n",
      "WARNING: Discrepancy detected. Processed: 63000, Stored: 1229\n",
      "\n",
      "Processing batch 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 134 contains 1 IDs that were seen in previous batches.\n",
      "Batch 134 contains duplicate IDs!\n",
      "Batch 134 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 134 - Current vector count: 1229\n",
      "Total records processed so far: 63500\n",
      "WARNING: Discrepancy detected. Processed: 63500, Stored: 1229\n",
      "\n",
      "Processing batch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 135 contains 1 IDs that were seen in previous batches.\n",
      "Batch 135 contains duplicate IDs!\n",
      "Batch 135 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 135 - Current vector count: 1229\n",
      "Total records processed so far: 64000\n",
      "WARNING: Discrepancy detected. Processed: 64000, Stored: 1229\n",
      "\n",
      "Processing batch 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 136 contains 1 IDs that were seen in previous batches.\n",
      "Batch 136 contains duplicate IDs!\n",
      "Batch 136 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 136 - Current vector count: 1229\n",
      "Total records processed so far: 64500\n",
      "WARNING: Discrepancy detected. Processed: 64500, Stored: 1229\n",
      "\n",
      "Processing batch 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 137 contains 1 IDs that were seen in previous batches.\n",
      "Batch 137 contains duplicate IDs!\n",
      "Batch 137 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 137 - Current vector count: 1229\n",
      "Total records processed so far: 65000\n",
      "WARNING: Discrepancy detected. Processed: 65000, Stored: 1229\n",
      "\n",
      "Processing batch 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 138 contains 1 IDs that were seen in previous batches.\n",
      "Batch 138 contains duplicate IDs!\n",
      "Batch 138 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 138 - Current vector count: 1229\n",
      "Total records processed so far: 65500\n",
      "WARNING: Discrepancy detected. Processed: 65500, Stored: 1229\n",
      "\n",
      "Processing batch 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 139 contains 1 IDs that were seen in previous batches.\n",
      "Batch 139 contains duplicate IDs!\n",
      "Batch 139 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 139 - Current vector count: 1229\n",
      "Total records processed so far: 66000\n",
      "WARNING: Discrepancy detected. Processed: 66000, Stored: 1229\n",
      "\n",
      "Processing batch 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 140 contains 1 IDs that were seen in previous batches.\n",
      "Batch 140 contains duplicate IDs!\n",
      "Batch 140 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 140 - Current vector count: 1229\n",
      "Total records processed so far: 66500\n",
      "WARNING: Discrepancy detected. Processed: 66500, Stored: 1229\n",
      "\n",
      "Processing batch 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 141 contains 1 IDs that were seen in previous batches.\n",
      "Batch 141 contains duplicate IDs!\n",
      "Batch 141 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 141 - Current vector count: 1229\n",
      "Total records processed so far: 67000\n",
      "WARNING: Discrepancy detected. Processed: 67000, Stored: 1229\n",
      "\n",
      "Processing batch 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 142 contains 1 IDs that were seen in previous batches.\n",
      "Batch 142 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 142:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 143 contains 1 IDs that were seen in previous batches.\n",
      "Batch 143 contains duplicate IDs!\n",
      "Batch 143 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 143 - Current vector count: 1229\n",
      "Total records processed so far: 67500\n",
      "WARNING: Discrepancy detected. Processed: 67500, Stored: 1229\n",
      "\n",
      "Processing batch 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 144 contains 1 IDs that were seen in previous batches.\n",
      "Batch 144 contains duplicate IDs!\n",
      "Batch 144 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 144 - Current vector count: 1229\n",
      "Total records processed so far: 68000\n",
      "WARNING: Discrepancy detected. Processed: 68000, Stored: 1229\n",
      "\n",
      "Processing batch 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 145 contains 1 IDs that were seen in previous batches.\n",
      "Batch 145 contains duplicate IDs!\n",
      "Batch 145 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 145 - Current vector count: 1229\n",
      "Total records processed so far: 68500\n",
      "WARNING: Discrepancy detected. Processed: 68500, Stored: 1229\n",
      "\n",
      "Processing batch 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 146 contains 1 IDs that were seen in previous batches.\n",
      "Batch 146 contains duplicate IDs!\n",
      "Batch 146 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 146 - Current vector count: 1229\n",
      "Total records processed so far: 69000\n",
      "WARNING: Discrepancy detected. Processed: 69000, Stored: 1229\n",
      "\n",
      "Processing batch 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 147 contains 1 IDs that were seen in previous batches.\n",
      "Batch 147 contains duplicate IDs!\n",
      "Batch 147 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 147 - Current vector count: 1229\n",
      "Total records processed so far: 69500\n",
      "WARNING: Discrepancy detected. Processed: 69500, Stored: 1229\n",
      "\n",
      "Processing batch 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 148 contains 1 IDs that were seen in previous batches.\n",
      "Batch 148 contains duplicate IDs!\n",
      "Batch 148 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 148 - Current vector count: 1229\n",
      "Total records processed so far: 70000\n",
      "WARNING: Discrepancy detected. Processed: 70000, Stored: 1229\n",
      "\n",
      "Processing batch 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 149 contains 1 IDs that were seen in previous batches.\n",
      "Batch 149 contains duplicate IDs!\n",
      "Batch 149 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 149 - Current vector count: 1229\n",
      "Total records processed so far: 70500\n",
      "WARNING: Discrepancy detected. Processed: 70500, Stored: 1229\n",
      "\n",
      "Processing batch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 150 contains 1 IDs that were seen in previous batches.\n",
      "Batch 150 contains duplicate IDs!\n",
      "Batch 150 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 150 - Current vector count: 1229\n",
      "Total records processed so far: 71000\n",
      "WARNING: Discrepancy detected. Processed: 71000, Stored: 1229\n",
      "\n",
      "Processing batch 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 151 contains 1 IDs that were seen in previous batches.\n",
      "Batch 151 contains duplicate IDs!\n",
      "Batch 151 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 151 - Current vector count: 1229\n",
      "Total records processed so far: 71500\n",
      "WARNING: Discrepancy detected. Processed: 71500, Stored: 1229\n",
      "\n",
      "Processing batch 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 152 contains 1 IDs that were seen in previous batches.\n",
      "Batch 152 contains duplicate IDs!\n",
      "Batch 152 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 152 - Current vector count: 1229\n",
      "Total records processed so far: 72000\n",
      "WARNING: Discrepancy detected. Processed: 72000, Stored: 1229\n",
      "\n",
      "Processing batch 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 153 contains 1 IDs that were seen in previous batches.\n",
      "Batch 153 contains duplicate IDs!\n",
      "Batch 153 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 153 - Current vector count: 1229\n",
      "Total records processed so far: 72500\n",
      "WARNING: Discrepancy detected. Processed: 72500, Stored: 1229\n",
      "\n",
      "Processing batch 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 154 contains 1 IDs that were seen in previous batches.\n",
      "Batch 154 contains duplicate IDs!\n",
      "Batch 154 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 154 - Current vector count: 1229\n",
      "Total records processed so far: 73000\n",
      "WARNING: Discrepancy detected. Processed: 73000, Stored: 1229\n",
      "\n",
      "Processing batch 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 155 contains 1 IDs that were seen in previous batches.\n",
      "Batch 155 contains duplicate IDs!\n",
      "Batch 155 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 155 - Current vector count: 1229\n",
      "Total records processed so far: 73500\n",
      "WARNING: Discrepancy detected. Processed: 73500, Stored: 1229\n",
      "\n",
      "Processing batch 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 156 contains 1 IDs that were seen in previous batches.\n",
      "Batch 156 contains duplicate IDs!\n",
      "Batch 156 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 156 - Current vector count: 1229\n",
      "Total records processed so far: 74000\n",
      "WARNING: Discrepancy detected. Processed: 74000, Stored: 1229\n",
      "\n",
      "Processing batch 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 157 contains 1 IDs that were seen in previous batches.\n",
      "Batch 157 contains duplicate IDs!\n",
      "Batch 157 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 157 - Current vector count: 1229\n",
      "Total records processed so far: 74500\n",
      "WARNING: Discrepancy detected. Processed: 74500, Stored: 1229\n",
      "\n",
      "Processing batch 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 158 contains 1 IDs that were seen in previous batches.\n",
      "Batch 158 contains duplicate IDs!\n",
      "Batch 158 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 158 - Current vector count: 1229\n",
      "Total records processed so far: 75000\n",
      "WARNING: Discrepancy detected. Processed: 75000, Stored: 1229\n",
      "\n",
      "Processing batch 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 159 contains 1 IDs that were seen in previous batches.\n",
      "Batch 159 contains duplicate IDs!\n",
      "Batch 159 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 159 - Current vector count: 1229\n",
      "Total records processed so far: 75500\n",
      "WARNING: Discrepancy detected. Processed: 75500, Stored: 1229\n",
      "\n",
      "Processing batch 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 160 contains 1 IDs that were seen in previous batches.\n",
      "Batch 160 contains duplicate IDs!\n",
      "Batch 160 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 160 - Current vector count: 1229\n",
      "Total records processed so far: 76000\n",
      "WARNING: Discrepancy detected. Processed: 76000, Stored: 1229\n",
      "\n",
      "Processing batch 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 161 contains 1 IDs that were seen in previous batches.\n",
      "Batch 161 contains duplicate IDs!\n",
      "Batch 161 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 161 - Current vector count: 1229\n",
      "Total records processed so far: 76500\n",
      "WARNING: Discrepancy detected. Processed: 76500, Stored: 1229\n",
      "\n",
      "Processing batch 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 162 contains 1 IDs that were seen in previous batches.\n",
      "Batch 162 contains duplicate IDs!\n",
      "Batch 162 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 162 - Current vector count: 1229\n",
      "Total records processed so far: 77000\n",
      "WARNING: Discrepancy detected. Processed: 77000, Stored: 1229\n",
      "\n",
      "Processing batch 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 163 contains 1 IDs that were seen in previous batches.\n",
      "Batch 163 contains duplicate IDs!\n",
      "Error upserting embeddings for batch 163:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 11, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 175, in upsert\n",
      "    return self._upsert_batch(vectors, namespace, _check_type, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/data/index.py\", line 204, in _upsert_batch\n",
      "    return self._vector_api.upsert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 761, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/data/api/data_plane_api.py\", line 811, in __upsert\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 819, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 380, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 175, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/api_client.py\", line 460, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 345, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/core/openapi/shared/rest.py\", line 190, in request\n",
      "    r = self.pool_manager.request(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/anaconda3/lib/python3.11/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/2294989208.py\", line 25, in upsert_embeddings\n",
      "    response = index.upsert(vectors=vectors)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sripoojitha/Desktop/recipe-curator/env/lib/python3.11/site-packages/pinecone/utils/error_handling.py\", line 18, in inner_func\n",
      "    raise ProtocolError(f\"Failed to connect; did you specify the correct index name?\") from e\n",
      "urllib3.exceptions.ProtocolError: Failed to connect; did you specify the correct index name?\n",
      "\n",
      "\n",
      "Processing batch 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 164 contains 1 IDs that were seen in previous batches.\n",
      "Batch 164 contains duplicate IDs!\n",
      "Batch 164 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 164 - Current vector count: 1229\n",
      "Total records processed so far: 77500\n",
      "WARNING: Discrepancy detected. Processed: 77500, Stored: 1229\n",
      "\n",
      "Processing batch 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/713jb3f916n662_fxtm8kgy40000gn/T/ipykernel_15624/111194393.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size read from Snowflake: 500 records\n",
      "Records in this batch: 500\n",
      "Batch 165 contains 1 IDs that were seen in previous batches.\n",
      "Batch 165 contains duplicate IDs!\n",
      "Batch 165 - Upsert response: {'upserted_count': 500}\n",
      "Current index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1229}},\n",
      " 'total_vector_count': 1229}\n",
      "Batch 165 - Current vector count: 1229\n",
      "Total records processed so far: 78000\n",
      "WARNING: Discrepancy detected. Processed: 78000, Stored: 1229\n",
      "\n",
      "Processing batch 166\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "250001: Could not connect to Snowflake backend after 1 attempt(s).Aborting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/contrib/pyopenssl.py:331\u001b[0m, in \u001b[0;36mWrappedSocket.recv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m util\u001b[38;5;241m.\u001b[39mwait_for_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mgettimeout()):\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m timeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe read operation timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:358\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='enb95886.us-east-1.snowflakecomputing.com', port=443): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:1071\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec\u001b[0;34m(self, session, method, full_url, headers, data, token, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# socket timeout is constant. You should be able to receive\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# the response within the time. If not, ConnectReadTimeout or\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# ReadTimeout is raised.\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m raw_ret \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msocket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_raw_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSnowflakeAuth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m download_end_time \u001b[38;5;241m=\u001b[39m get_time_millis()\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/vendored/requests/adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='enb95886.us-east-1.snowflakecomputing.com', port=443): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:1414\u001b[0m, in \u001b[0;36mSnowflakeConnection._authenticate\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1414\u001b[0m     \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarehouse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpasscode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_passcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpasscode_in_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_passcode_in_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmfa_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mfa_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_password_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/auth/_auth.py:250\u001b[0m, in \u001b[0;36mAuth.authenticate\u001b[0;34m(self, auth_instance, account, user, database, schema, warehouse, role, passcode, passcode_in_password, mfa_callback, password_callback, session_parameters, timeout)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ForbiddenError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# HTTP 403\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:739\u001b[0m, in \u001b[0;36mSnowflakeRestful._post_request\u001b[0;34m(self, url, headers, body, token, timeout, socket_timeout, _no_results, no_retry, _include_retry_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m     pprint(ret)\n\u001b[0;32m--> 739\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msocket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret[code] = \u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[38;5;124m, after post request\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    752\u001b[0m         code\u001b[38;5;241m=\u001b[39m(ret\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    753\u001b[0m     )\n\u001b[1;32m    754\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:854\u001b[0m, in \u001b[0;36mSnowflakeRestful.fetch\u001b[0;34m(self, method, full_url, headers, data, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 854\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_exec_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:982\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec_wrapper\u001b[0;34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    980\u001b[0m     raise_raw_http_failure \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError)\n\u001b[1;32m    981\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_retry:\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    983\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgnored error\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:904\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec_wrapper\u001b[0;34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     return_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/network.py:1158\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec\u001b[0;34m(self, session, method, full_url, headers, data, token, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit a timeout error while logging in. Will be handled by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthenticator. Ignore the following. Error stack: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1156\u001b[0m         exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m     )\n\u001b[0;32m-> 1158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(\n\u001b[1;32m   1159\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnectionTimeout occurred during login. Will be handled by authenticator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1160\u001b[0m         errno\u001b[38;5;241m=\u001b[39mER_CONNECTION_TIMEOUT,\n\u001b[1;32m   1161\u001b[0m     )\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: 251011: ConnectionTimeout occurred during login. Will be handled by authenticator",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal records processed according to our script: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m batch_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data_from_snowflake\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more data to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m, in \u001b[0;36mread_data_from_snowflake\u001b[0;34m(offset, limit)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data_from_snowflake\u001b[39m(offset, limit):\n\u001b[0;32m---> 78\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43msnowflake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_USER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_PASSWORD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_ACCOUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_WAREHOUSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_DATABASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNOWFLAKE_SCHEMA\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124m        SELECT \u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124m            RecipeId, Name, Description, RecipeCategory, Keywords, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124m        LIMIT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m OFFSET \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/__init__.py:55\u001b[0m, in \u001b[0;36mConnect\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(SnowflakeConnection\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mConnect\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeConnection:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSnowflakeConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:456\u001b[0m, in \u001b[0;36mSnowflakeConnection.__init__\u001b[0;34m(self, connection_name, connections_file_path, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m _get_default_connection_params()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_error_attributes()\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry \u001b[38;5;241m=\u001b[39m TelemetryClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpired \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:771\u001b[0m, in \u001b[0;36mSnowflakeConnection.connect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(exceptions_dict))\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:1099\u001b[0m, in \u001b[0;36mSnowflakeConnection.__open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class \u001b[38;5;241m=\u001b[39m AuthByOkta(\n\u001b[1;32m   1094\u001b[0m         application\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplication,\n\u001b[1;32m   1095\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogin_timeout,\n\u001b[1;32m   1096\u001b[0m         backoff_generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backoff_generator,\n\u001b[1;32m   1097\u001b[0m     )\n\u001b[0;32m-> 1099\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_password \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class\u001b[38;5;241m.\u001b[39mreset_secrets()\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:1386\u001b[0m, in \u001b[0;36mSnowflakeConnection.authenticate_with_retry\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauthenticate_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1384\u001b[0m     \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1386\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID token expired. Reauthenticating...: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, ex)\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:1458\u001b[0m, in \u001b[0;36mSnowflakeConnection._authenticate\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m auth_op:\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auth_op\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m ER_FAILED_TO_CONNECT_TO_DB:\n\u001b[0;32m-> 1458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m auth_op \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuing authenticator specific timeout handling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/connection.py:1435\u001b[0m, in \u001b[0;36mSnowflakeConnection._authenticate\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1435\u001b[0m         \u001b[43mauth_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_timeout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauthenticator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authenticator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mservice_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1442\u001b[0m         auth\u001b[38;5;241m.\u001b[39mauthenticate(\n\u001b[1;32m   1443\u001b[0m             auth_instance\u001b[38;5;241m=\u001b[39mauth_instance,\n\u001b[1;32m   1444\u001b[0m             account\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             session_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session_parameters,\n\u001b[1;32m   1455\u001b[0m         )\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m auth_op:\n",
      "File \u001b[0;32m~/Desktop/recipe-curator/env/lib/python3.11/site-packages/snowflake/connector/auth/by_plugin.py:212\u001b[0m, in \u001b[0;36mAuthByPlugin.handle_timeout\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_ctx\u001b[38;5;241m.\u001b[39mshould_retry:\n\u001b[1;32m    207\u001b[0m     error \u001b[38;5;241m=\u001b[39m OperationalError(\n\u001b[1;32m    208\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not connect to Snowflake backend after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_ctx\u001b[38;5;241m.\u001b[39mcurrent_retry_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attempt(s).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAborting\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    210\u001b[0m         errno\u001b[38;5;241m=\u001b[39mER_FAILED_TO_CONNECT_TO_DB,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit connection timeout, attempt number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_ctx\u001b[38;5;241m.\u001b[39mcurrent_retry_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Will retry in a bit...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n",
      "\u001b[0;31mOperationalError\u001b[0m: 250001: Could not connect to Snowflake backend after 1 attempt(s).Aborting"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def check_index_stats():\n",
    "    stats = index.describe_index_stats()\n",
    "    print(f\"Current index stats: {stats}\")\n",
    "    return stats['total_vector_count']\n",
    "\n",
    "def upsert_embeddings(vectors, batch_number):\n",
    "    try:\n",
    "        # Check for duplicate IDs\n",
    "        ids = [v[0] for v in vectors]\n",
    "        if len(ids) != len(set(ids)):\n",
    "            print(f\"Batch {batch_number} contains duplicate IDs!\")\n",
    "        \n",
    "        response = index.upsert(vectors=vectors)\n",
    "        print(f\"Batch {batch_number} - Upsert response: {response}\")\n",
    "        time.sleep(1)  # Add a small delay after each upsert\n",
    "        current_count = check_index_stats()\n",
    "        print(f\"Batch {batch_number} - Current vector count: {current_count}\")\n",
    "        return response, current_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting embeddings for batch {batch_number}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    print(\"Initial index stats:\")\n",
    "    initial_count = check_index_stats()\n",
    "\n",
    "    offset = 0\n",
    "    total_records = min(VECTOR_LIMIT, 1000000)\n",
    "    batch_number = 0\n",
    "    total_processed = 0\n",
    "    all_ids = set()\n",
    "\n",
    "    while offset < total_records:\n",
    "        batch_number += 1\n",
    "        print(f\"\\nProcessing batch {batch_number}\")\n",
    "        \n",
    "        df = read_data_from_snowflake(offset, BATCH_SIZE)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No more data to process.\")\n",
    "            break\n",
    "\n",
    "        records_in_batch = len(df)\n",
    "        print(f\"Records in this batch: {records_in_batch}\")\n",
    "\n",
    "        # Check for duplicate IDs across batches\n",
    "        batch_ids = set(df['RECIPEID'].astype(str))\n",
    "        duplicate_ids = batch_ids.intersection(all_ids)\n",
    "        if duplicate_ids:\n",
    "            print(f\"Batch {batch_number} contains {len(duplicate_ids)} IDs that were seen in previous batches.\")\n",
    "        all_ids.update(batch_ids)\n",
    "\n",
    "        text_to_embed = [\n",
    "            ' '.join([str(row[col]) for col in ['NAME', 'DESCRIPTION', 'RECIPECATEGORY', 'KEYWORDS'] if not pd.isnull(row[col])])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            embeddings = generate_embeddings(text_to_embed)\n",
    "            vectors = [(str(row['RECIPEID']), embeddings[idx].tolist()) for idx, row in df.iterrows()]\n",
    "            \n",
    "            upsert_response, current_count = upsert_embeddings(vectors, batch_number)\n",
    "            if upsert_response:\n",
    "                total_processed += records_in_batch\n",
    "                print(f\"Total records processed so far: {total_processed}\")\n",
    "                if current_count and current_count < total_processed:\n",
    "                    print(f\"WARNING: Discrepancy detected. Processed: {total_processed}, Stored: {current_count}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_number} starting at index {offset}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "        offset += BATCH_SIZE\n",
    "        time.sleep(2)  # Add a delay between batches\n",
    "\n",
    "    print(\"\\nEmbeddings generation and storage process completed.\")\n",
    "\n",
    "    print(\"Final index stats:\")\n",
    "    final_count = check_index_stats()\n",
    "\n",
    "    print(f\"Initial vector count: {initial_count}\")\n",
    "    print(f\"Final vector count: {final_count}\")\n",
    "    print(f\"Vectors added according to Pinecone: {final_count - initial_count}\")\n",
    "    print(f\"Total records processed according to our script: {total_processed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdb637-421f-483c-86c7-a296613f77de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8b357a39b9cfa652ce5eb494eea21f40e33742edb285e95e5860972a45bd4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
