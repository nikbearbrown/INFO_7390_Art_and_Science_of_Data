{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f5eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers faiss-cpu yfinance pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75281726",
   "metadata": {},
   "source": [
    "# Final Project - Financial-News RAG Assistant with Live Stock-Price Context\n",
    "\n",
    "\n",
    "*Chen Yang – NUID 002837912*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cbf86",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This Jupyter Notebook presents a complete **Retrieval-Augmented Generation (RAG) assistant** that helps investors and analysts answer natural-language questions about publicly traded companies by combining two complementary data sources:\n",
    "\n",
    "1. **Semantic News Retrieval** – Topically relevant headlines are fetched from a cleaned CNBC financial-news corpus (2019 – 2024) via a FAISS vector index.  \n",
    "2. **Live Market Data** – Up-to-date closing prices are pulled on-the-fly with `yfinance`, summarising recent trends (e.g., 30-day movement) for any recognised ticker symbol.\n",
    "\n",
    "A lightweight language model (DistilGPT-2) then fuses both contexts into a concise answer, giving the user an “at a glance” narrative that blends qualitative news sentiment with quantitative price performance.\n",
    "\n",
    "### Why this matters — key motivations\n",
    "\n",
    "- **Information overload**  \n",
    "  Investors face thousands of headlines per day; manually triaging them is slow and error-prone.\n",
    "\n",
    "- **Context gaps**  \n",
    "  News stories rarely include hard numbers, while price charts lack narrative. Merging the two enriches decision-making.\n",
    "\n",
    "- **Cost-efficient generative AI**  \n",
    "  The pipeline relies only on open-weights models and free APIs, demonstrating that effective RAG systems need not incur paid token or cloud fees.\n",
    "\n",
    "### Example interaction\n",
    "\n",
    "> **Query:** “What drove TSLA’s share price this month and where does it stand now?”\n",
    "\n",
    "* The FAISS search surfaces headlines about Cybertruck deliveries and price-cut rumours.  \n",
    "* `yfinance` reports that TSLA gained **≈ 8 %** over the last 30 trading days.  \n",
    "* The LLM summarises:  \n",
    "  > “Tesla shares rose about 8 % in the past month, buoyed by renewed EV-demand optimism after the first Cybertruck hand-offs. The stock closed yesterday at \\$259.51.”\n",
    "\n",
    "### Deliverables in this notebook\n",
    "\n",
    "- End-to-end data pipeline: ingestion → cleaning → embedding → retrieval → generation.  \n",
    "- Reusable utility functions (`convert_time`, `price_series`, `rag_query`).  \n",
    "- Ready-to-run Streamlit stub for interactive deployment.\n",
    "\n",
    "This project showcases a practical, low-cost blueprint for domain-specific RAG applications that amplify human insight by unifying textual and numerical evidence in a single generative answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800a71e",
   "metadata": {},
   "source": [
    "![RAG Framework Overview](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frag-framework.81dc2cdc.png&w=3840&q=75)\n",
    "\n",
    "*Source: [Prompting Guide – Retrieval‑Augmented Generation](https://www.promptingguide.ai/research/rag)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289ad44",
   "metadata": {},
   "source": [
    "## Theory Section  \n",
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "---\n",
    "\n",
    "### 1  What is RAG?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an architectural pattern that **blends information retrieval with neural text generation**.  \n",
    "Instead of relying purely on a language model’s frozen parameters (“closed-book” knowledge), a RAG system:\n",
    "\n",
    "1. **Retrieves** external documents or passages that are relevant to a user’s query.  \n",
    "2. **Augments** the query with these snippets (often called *context*).  \n",
    "3. **Generates** a final answer conditioned on both the query and the retrieved context.\n",
    "\n",
    "Mathematically we can express the joint objective as  \n",
    "\n",
    "\\[\n",
    "P(\\text{answer}\\;|\\;\\text{query}) \\;=\\;\n",
    "\\sum_{d \\in \\mathcal{D}} \n",
    "P(d\\;|\\;\\text{query}) \\;\n",
    "P(\\text{answer}\\;|\\;\\text{query},d)\n",
    "\\]\n",
    "\n",
    "Where  \n",
    "* \\(\\mathcal{D}\\) = corpus of documents  \n",
    "* \\(P(d\\;|\\;\\text{query})\\) = retriever scoring function  \n",
    "* \\(P(\\text{answer}\\;|\\;\\text{query},d)\\) = generator likelihood\n",
    "\n",
    "> **Intuition:** The retrieval stage narrows the search space to a handful of high-value passages; the generator then works “open-book”, citing the fresh evidence instead of hallucinating facts from dated training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2  Why RAG instead of a larger LLM?\n",
    "\n",
    "| Challenge | Closed-Book LLMs | RAG Solution |\n",
    "|-----------|-----------------|--------------|\n",
    "| **Domain drift** (finance, medicine, law) | Model freezes outdated facts after training | Retrieve *current* documents, keeping answers fresh |\n",
    "| **Token limits** | Must pack *all* knowledge inside model weights ⇒ billions of parameters | External database scales nearly unbounded; model sees only K contexts per query |\n",
    "| **Explainability** | Source of facts is opaque | Retrieved passages are explicit citations |\n",
    "| **Cost / IP concerns** | Training huge models is expensive; shipping them exposes full weights | Small open-weights model + private corpus, no data leakage |\n",
    "\n",
    "---\n",
    "\n",
    "### 3  Core Components of a RAG Pipeline\n",
    "\n",
    "1. **Corpus ↔ Vector Store**  \n",
    "   * Raw docs ➔ cleaned text ➔ embeddings ➔ FAISS / Milvus / Pinecone.  \n",
    "   * Each vector is linked to its source metadata (title, URL, timestamp).\n",
    "\n",
    "2. **Retriever**  \n",
    "   * Accepts a query, embeds it, performs *k-NN* search in vector space.  \n",
    "   * Returns top-k IDs + similarity scores.\n",
    "\n",
    "3. **Context Assembler**  \n",
    "   * Fetches raw passages from the IDs.  \n",
    "   * Optionally applies reranking or chunk selection (e.g., **Max-Marginal-Relevance**) to reduce overlap.\n",
    "\n",
    "4. **Generator (LLM)**  \n",
    "   * Receives `prompt = [query] + [context block]`.  \n",
    "   * Outputs an answer; temperature and repetition guards ensure concision.\n",
    "\n",
    "5. **Post-processor**  \n",
    "   * Trims rambling text, injects citations, enforces output style.  \n",
    "   * In production may also decide “I don’t know” thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "### 4  Implementation Walk-Through (of this notebook)\n",
    "\n",
    "#### 4.1 Building the Knowledge Base  \n",
    "* **Dataset:** CNBC headlines with publication timestamps.  \n",
    "* **Cleaning step:**  \n",
    "  * Normalise weird month spellings (`Sept → Sep`).  \n",
    "  * Convert heterogeneous date strings (“7:51 PM ET Fri, 17 July 2020”) into Python `datetime`.  \n",
    "* **Embedding:**  \n",
    "  * DistilGPT-2 token embeddings averaged per headline → 768-d vectors.  \n",
    "* **Indexing:**  \n",
    "  * `faiss.IndexFlatL2` for fast similarity over ~70 k headlines.\n",
    "\n",
    "#### 4.2 Live Numerical Context  \n",
    "* **`yfinance` fetcher** grabs the last N trading-day closes.  \n",
    "* Summariser distils them into one sentence (“up 7.8 %, last close 259.51”).\n",
    "\n",
    "#### 4.3 The Retrieval Step  \n",
    "```python\n",
    "q_vec = embed_text(query)          # 768-d\n",
    "_, idx = index.search(q_vec[None], k=3)\n",
    "ctx_news = \"\\n\".join(df.loc[i, \"Headlines\"] for i in idx[0])\n",
    "```\n",
    "\n",
    "#### 4.4 Prompt Assembly\n",
    "News context:\n",
    "<Headline 1>\n",
    "<Headline 2>\n",
    "<Headline 3>\n",
    "\n",
    "Stock-price context:\n",
    "TSLA: From 2025-03-13 to 2025-04-24 the stock moved up +18.83 USD (+7.82 %).\n",
    "      Last close: 259.51 USD.\n",
    "\n",
    "Question: What happened to TSLA this month and how is the stock doing?\n",
    "Answer concisely:\n",
    "*************************\n",
    "\n",
    "#### 4.5 Generation & Trim\n",
    "* Model – DistilGPT-2 (≈ 82 M parameters) generates ≤ 120 tokens.\n",
    "\n",
    "* Decoding guards – repetition_penalty = 1.25, no_repeat_ngram_size = 4.\n",
    "\n",
    "* Post-processing – a regex removes repeated “Question:” loops and keeps only the first 1–2 sentences for a concise answer.\n",
    "\n",
    "#### 5  Design Choices and Alternatives\n",
    "\n",
    "| Layer              | Current Notebook Implementation            | Possible Upgrades (examples)                                      |\n",
    "|--------------------|--------------------------------------------|-------------------------------------------------------------------|\n",
    "| **Embeddings**     | Token-average **DistilGPT-2**              | Sentence-Transformers **all-MiniLM-L6** · **E5** · OpenAI **text-embedding-3-small** |\n",
    "| **Vector Store**   | In-memory **FAISS**                        | **Milvus** · **Weaviate** for sharded / cloud-scale retrieval     |\n",
    "| **Generator**      | **DistilGPT-2** (open, lightweight)        | **Mistral-7B-Instruct** · **Llama-3-8B-Instruct** · **Flan-T5-XL** |\n",
    "| **Reranker**       | *None* (top-k = 3 is inexpensive)          | **ColBERT-v2** · **Cohere-rerank** · **BM25 + MMR**               |\n",
    "\n",
    "\n",
    "#### 6  Best Practices for High-Quality RAG\n",
    "\n",
    "* **Chunk size (200 – 500 tokens)** – balances topical focus and recall.  \n",
    "* **Separate models** – a light retriever plus a larger instruction-tuned generator usually beats a single monolith.  \n",
    "* **Freshness signals** – store timestamps and nudge the retriever toward newer docs when the query implies recency.  \n",
    "* **Guardrails** – deterministic decoding (`temperature = 0`), n-gram repetition blocks, and post-trim logic curb hallucinations.  \n",
    "* **Fallbacks** – if retrieval returns no hits, answer explicitly: *“I don’t have recent information on that topic.”* Never invent content.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7  Limitations and Future Improvements\n",
    "\n",
    "* **Headline-only corpus** – lacks article-body nuance; adding full text or SEC filings would deepen context.  \n",
    "* **Ticker detection** – simple regex may mis-label words like “USA”; a symbol lookup table or NER would be safer.  \n",
    "* **Model fidelity** – DistilGPT-2 can still invent figures; upgrading to an instruction-fine-tuned model (e.g., Zephyr-7B) would improve factuality.  \n",
    "* **Latency** – live `yfinance` calls add ≈ 0.5 s; caching popular tickers or nightly snapshots would speed responses.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8  Conclusion\n",
    "\n",
    "Retrieval-Augmented Generation bridges **static parametric knowledge** and **dynamic external data**.  \n",
    "By coupling a FAISS vector store of financial-news embeddings with real-time market prices via `yfinance`, this notebook delivers grounded answers that blend **qualitative narrative** and **quantitative price trends**—a practical, low-cost blueprint for domain-specific generative-AI systems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443dcdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xsyyy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re, torch, faiss, yfinance as yf\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2874c9",
   "metadata": {},
   "source": [
    "| Line                               | Purpose                                                                                                                                        |\n",
    "|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `import pandas as pd`              | **Pandas** supplies the `DataFrame` object used for loading and cleaning the financial-news CSV.                                               |\n",
    "| `import numpy as np`               | **NumPy** handles vector maths (e.g., converting embeddings to `float32` before adding to FAISS).                                              |\n",
    "| `import re`                        | Python’s **regular-expression** module, used for ticker detection and date-string normalisation.                                               |\n",
    "| `import torch`                     | **PyTorch** provides tensor operations and runs the DistilGPT-2 language model on CPU/GPU.                                                     |\n",
    "| `import faiss`                     | **FAISS** (Facebook AI Similarity Search) builds an in-memory vector index for fast k-nearest-neighbour retrieval.                             |\n",
    "| `import yfinance as yf`            | Lightweight wrapper around Yahoo Finance’s API; fetches historical and real-time stock prices.                                                 |\n",
    "| `from datetime import datetime`    | Standard-library helper for timestamp creation and formatting.                                                                                 |\n",
    "| `from dateutil import parser`      | Robust date-string parser — e.g., converts “7:51 PM ET Fri, 17 July 2020” to a Python `datetime`.                                              |\n",
    "| `from tqdm.auto import tqdm`       | **tqdm** renders progress bars in Jupyter or the console when embedding thousands of headlines.                                                |\n",
    "| `from transformers …`              | Hugging Face **Transformers** utilities:                                                                                                       |\n",
    "| &nbsp;&nbsp;• `AutoTokenizer`      | Downloads the correct tokenizer (DistilGPT-2) for text encoding/decoding.                                                                      |\n",
    "| &nbsp;&nbsp;• `AutoModelForCausalLM` | Loads a causal-language-model head (DistilGPT-2) for answer generation.                                                                       |\n",
    "| &nbsp;&nbsp;• `AutoModel`          | Base model loader when only hidden states / embeddings are needed.                                                                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fde7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Headlines   \n",
      "0  Jim Cramer: A better way to invest in the Covi...  \\\n",
      "1     Cramer's lightning round: I would own Teradyne   \n",
      "2                                                NaN   \n",
      "3  Cramer's week ahead: Big week for earnings, ev...   \n",
      "4  IQ Capital CEO Keith Bliss says tech and healt...   \n",
      "\n",
      "                             Time   \n",
      "0   7:51  PM ET Fri, 17 July 2020  \\\n",
      "1   7:33  PM ET Fri, 17 July 2020   \n",
      "2                             NaN   \n",
      "3   7:25  PM ET Fri, 17 July 2020   \n",
      "4   4:24  PM ET Fri, 17 July 2020   \n",
      "\n",
      "                                         Description  \n",
      "0  \"Mad Money\" host Jim Cramer recommended buying...  \n",
      "1  \"Mad Money\" host Jim Cramer rings the lightnin...  \n",
      "2                                                NaN  \n",
      "3  \"We'll pay more for the earnings of the non-Co...  \n",
      "4  Keith Bliss, IQ Capital CEO, joins \"Closing Be...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('cnbc_headlines.csv')\n",
    "\n",
    "# Preview the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2ba31",
   "metadata": {},
   "source": [
    "The code block loads the CNBC headlines dataset into a Pandas DataFrame and immediately prints the first five rows.  \n",
    "This quick preview confirms the file was read correctly, reveals the column names (`Headlines`, `Time`, `Description`, etc.), and lets you spot obvious issues—such as missing values or unexpected formats—before further cleaning and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b1b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 2800\n"
     ]
    }
   ],
   "source": [
    "# ─── 2.1  helper dict for odd month spellings ────────────────────────\n",
    "MONTH_FIX = {\"Sept\": \"Sep\"}          # add more if you encounter them\n",
    "\n",
    "# ─── 2.2  single convert_time() that swallows all variants ───────────\n",
    "def convert_time(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return None\n",
    "    if not isinstance(time_str, str):\n",
    "        time_str = str(time_str)\n",
    "\n",
    "    s = \" \".join(time_str.strip().split())                # squash spaces\n",
    "    for bad, good in MONTH_FIX.items():                   # 'Sept' → 'Sep'\n",
    "        s = s.replace(f\" {bad} \", f\" {good} \")\n",
    "\n",
    "    if \" ET \" in s:                                       # strip 'ET Mon,'\n",
    "        t_part, rest = s.split(\" ET \", 1)\n",
    "        rest = rest.split(\",\", 1)[-1].strip()\n",
    "        s = f\"{rest} {t_part}\"                            # '30 Sep 2019 7:27 PM'\n",
    "\n",
    "    has_date = bool(re.search(r\"\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{4}\", s))\n",
    "\n",
    "    if not has_date:                                      # time-only → add today\n",
    "        today = datetime.now().strftime(\"%d %b %Y\")\n",
    "        s = f\"{today} {s}\"\n",
    "\n",
    "    # explicit attempts\n",
    "    for fmt in (\"%d %b %Y %I:%M %p\", \"%d %B %Y %I:%M %p\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    # last-chance parse\n",
    "    return parser.parse(s)\n",
    "\n",
    "# ─── 2.3  apply, keep rows with valid Headline & Time ────────────────\n",
    "df[\"Time\"] = df[\"Time\"].apply(convert_time)\n",
    "df = df.dropna(subset=[\"Headlines\", \"Time\"]).reset_index(drop=True)\n",
    "print(\"Rows after cleaning:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6096a",
   "metadata": {},
   "source": [
    "This block standardises the messy **Time** column and removes unusable rows.\n",
    "\n",
    "* **`MONTH_FIX` helper** – maps rare month spellings (`\"Sept\"` → `\"Sep\"`).  \n",
    "* **`convert_time()`**  \n",
    "  * Handles `NaN` and non-string values.  \n",
    "  * Normalises whitespace, fixes odd month tokens, strips the `\" ET Mon,\"` wrapper used by CNBC, and appends today’s date if the string contains only a time.  \n",
    "  * Tries two explicit `strptime` formats—`%d %b %Y %I:%M %p` (abbrev. month) and `%d %B %Y %I:%M %p` (full month).  \n",
    "  * Falls back to `dateutil.parser.parse` if those fail, giving a robust conversion to `datetime`.  \n",
    "\n",
    "* **Apply and clean**  \n",
    "  * `df[\"Time\"] = df[\"Time\"].apply(convert_time)` converts every timestamp.  \n",
    "  * `df.dropna(subset=[\"Headlines\", \"Time\"])` keeps only rows that now have both a headline and a valid `datetime`.  \n",
    "  * After the cleanup, the dataset contains **2 800 usable rows**, ready for embedding and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8628fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = str(txt).strip().lower()\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt)               # collapse spaces\n",
    "    return txt\n",
    "\n",
    "df[\"clean_head\"] = df[\"Headlines\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e875c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xsyyy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Embedding headlines: 100%|██████████| 2800/2800 [00:00<00:00, 2949.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model     = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# ✅ this is the shared token-embedding matrix\n",
    "emb_layer = model.get_input_embeddings()          # same as model.transformer.wte\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        ids = tokenizer(text,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=128).input_ids.to(device)\n",
    "        vec = emb_layer(ids).mean(dim=1).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "embeddings = np.vstack([embed_text(t) for t in tqdm(df[\"clean_head\"],\n",
    "                                                    desc=\"Embedding headlines\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550596e0",
   "metadata": {},
   "source": [
    "This block prepares headline text and transforms each line into a 768-dimensional embedding that can later be indexed in FAISS.\n",
    "\n",
    "* **Text normalisation**  \n",
    "  `clean_text()` lower-cases every headline, trims leading/trailing whitespace, and collapses multiple spaces.  \n",
    "  The result is stored in a new column `clean_head`.\n",
    "\n",
    "* **Model and tokenizer loading**  \n",
    "  `distilgpt2` is fetched via Hugging Face:  \n",
    "  `AutoTokenizer` encodes/decodes text, while `AutoModelForCausalLM` supplies both generation capabilities and the token-embedding matrix accessed through `model.get_input_embeddings()`.\n",
    "\n",
    "* **Device setup**  \n",
    "  The model is moved to GPU if available (`torch.device(\"cuda\" …)`), and `model.eval()` disables dropout for deterministic inference.\n",
    "\n",
    "* **Embedding helper**  \n",
    "  `embed_text()` →  \n",
    "  1. tokenises a string (max 128 tokens),  \n",
    "  2. looks up token vectors in the embedding layer,  \n",
    "  3. averages them to obtain a single vector,  \n",
    "  4. returns a NumPy array on CPU.\n",
    "\n",
    "* **Embedding the entire corpus**  \n",
    "  A `tqdm` progress bar iterates over `df[\"clean_head\"]`; each headline is converted to a vector and stacked into `embeddings`, a 2-D array (`n_rows × 768`) ready for FAISS indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2384019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors in index: 2800\n"
     ]
    }
   ],
   "source": [
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings.astype(np.float32))\n",
    "print(\"Vectors in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c08771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── overwrite generate() with stricter decoding & trimming ──────────\n",
    "import re\n",
    "sent_split = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.25,      # discourages loops\n",
    "            no_repeat_ngram_size=4,        # no 4-gram repetition\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    txt = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # ── drop everything up to *last* 'Answer concisely:' ──────────────\n",
    "    if \"Answer concisely:\" in txt:\n",
    "        txt = txt.split(\"Answer concisely:\", 1)[1].lstrip()\n",
    "\n",
    "    # ── cut off if the model starts a new 'Question:' ────────────────\n",
    "    if \"\\nQuestion:\" in txt:\n",
    "        txt = txt.split(\"\\nQuestion:\", 1)[0].rstrip()\n",
    "\n",
    "    # ── optional: keep only first 2 sentences  ───────────────────────\n",
    "    sents = sent_split.split(txt)\n",
    "    txt   = \" \".join(sents[:2]).strip()\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992a9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6ʹ.1  Quick-price helper (close-of-day) ─────────────────────────\n",
    "def price_series(ticker: str, days: int = 30):\n",
    "    \"\"\"\n",
    "    Return [(YYYY-MM-DD, close)] for <days> trading days.\n",
    "    Handles:\n",
    "      • yfinance 2.x auto_adjust behaviour\n",
    "      • DataFrame vs Series quirks\n",
    "    \"\"\"\n",
    "    hist = yf.download(\n",
    "        ticker,\n",
    "        period=f\"{days}d\",\n",
    "        progress=False,\n",
    "        auto_adjust=False,    # keep raw Close / Adj Close numeric\n",
    "        threads=False,\n",
    "    )\n",
    "    if hist.empty:\n",
    "        return []\n",
    "\n",
    "    col = \"Adj Close\" if \"Adj Close\" in hist.columns else \"Close\"\n",
    "    closes = hist[col]\n",
    "\n",
    "    # if closes is a 1-column DataFrame, squeeze to Series\n",
    "    if isinstance(closes, pd.DataFrame):\n",
    "        closes = closes.squeeze(\"columns\")\n",
    "\n",
    "    closes = pd.to_numeric(closes, errors=\"coerce\").dropna()\n",
    "\n",
    "    return [\n",
    "        (idx.strftime(\"%Y-%m-%d\"), float(price))\n",
    "        for idx, price in closes.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def summarise_prices(series):\n",
    "    \"\"\"\n",
    "    Turn [(date, price), ...] into a concise natural-language snippet.\n",
    "    \"\"\"\n",
    "    if not series:\n",
    "        return \"No recent price data was found.\"\n",
    "\n",
    "    first_day, first_price = series[0]\n",
    "    last_day,  last_price  = series[-1]\n",
    "    change = round(last_price - first_price, 2)\n",
    "    pct    = round(100 * change / first_price, 2)\n",
    "\n",
    "    trend  = \"up\" if change > 0 else \"down\" if change < 0 else \"flat\"\n",
    "    line   = (f\"From {first_day} to {last_day}, the stock moved {trend} \"\n",
    "              f\"{change:+} USD ({pct:+} %). Last close: {last_price} USD.\")\n",
    "\n",
    "    # add a few intermediate points for colour\n",
    "    midpts = \" | \".join(f\"{d}: {p}\" for d, p in series[::len(series)//3 or 1])\n",
    "    return line + \"  |  Samples → \" + midpts\n",
    "\n",
    "# ─── 6ʹ.2  Regex to detect tickers (simple all-caps 1–5 letters) ────\n",
    "ticker_pat = re.compile(r\"\\b[A-Z]{1,5}\\b\")\n",
    "\n",
    "# ─── 6ʹ.3  Main RAG function: news + optional price context ──────────\n",
    "def rag_query(query: str, k=3, price_days=30):\n",
    "    # 1) headline retrieval\n",
    "    q_vec = embed_text(clean_text(query))[None, :].astype(np.float32)\n",
    "    _, idx = index.search(q_vec, k)\n",
    "    ctx_news = \"\\n\".join(df.loc[i, \"Headlines\"] for i in idx[0])\n",
    "\n",
    "    # 2) detect tickers in the question\n",
    "    tickers = ticker_pat.findall(query)\n",
    "    ctx_prices = \"\"\n",
    "    if tickers:\n",
    "        price_info = []\n",
    "        for tk in tickers:\n",
    "            series = price_series(tk, days=price_days)\n",
    "            price_info.append(f\"{tk}: {summarise_prices(series)}\")\n",
    "        ctx_prices = \"\\n\\nStock-price context:\\n\" + \"\\n\".join(price_info)\n",
    "\n",
    "    # 3) build final prompt\n",
    "    prompt = (\n",
    "        f\"News context:\\n{ctx_news}\"\n",
    "        f\"{ctx_prices}\"\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer concisely:\"\n",
    "    )\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d65df",
   "metadata": {},
   "source": [
    "The block establishes the retrieval index, adds live price utilities, and defines the end-to-end RAG query function.\n",
    "\n",
    "### 1 Create the FAISS index\n",
    "* `dim = embeddings.shape[1]` – dimension (768) of each headline vector.  \n",
    "* `faiss.IndexFlatL2(dim)` – brute-force L2 index.  \n",
    "* `index.add(…)` – inserts all 2 800 headline embeddings so k-NN search is possible.\n",
    "\n",
    "### 2 Live-price helpers\n",
    "| Function | Role |\n",
    "|----------|------|\n",
    "| **`price_series()`** | Downloads the last *n* trading-day closes for a ticker via **yfinance** (handles `auto_adjust`, DataFrame→Series quirks) and returns a list of `(date, price)` tuples. |\n",
    "| **`summarise_prices()`** | Converts that raw series into a single sentence: direction (up / down / flat), absolute and % change, last close, plus a few sample points. |\n",
    "\n",
    "### 3 Ticker detection\n",
    "* `ticker_pat = re.compile(r\"\\b[A-Z]{1,5}\\b\")` – simple regex that flags any 1–5-letter all-caps token (e.g. `TSLA`, `AAPL`) as a potential stock symbol.\n",
    "\n",
    "### 4 `rag_query()` – full Retrieval-Augmented Generation\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| **1 Headline retrieval** | `embed_text()` → FAISS `index.search` → top-k most similar headlines ⇢ `ctx_news`. |\n",
    "| **2 Price context (optional)** | Regex extracts tickers from the user query. For each symbol: `price_series()` → `summarise_prices()` ⇢ `ctx_prices`. |\n",
    "| **3 Prompt assembly** | Concatenates `News context`, optional `Stock-price context`, and the user **Question** with the directive **“Answer concisely:”**. |\n",
    "| **4 Generation** | Passes the prompt to `generate()` (DistilGPT-2) and returns one trimmed, final answer that blends news narrative with up-to-date price information. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49441172",
   "metadata": {},
   "source": [
    "### Custom `generate()` — tighter decoding and output cleanup\n",
    "\n",
    "This re-implementation of `generate()` ensures the language model produces a **concise, single-paragraph answer** without looping.\n",
    "\n",
    "* **Decoding safeguards**\n",
    "  * `do_sample = False` → deterministic (greedy) decoding.\n",
    "  * `repetition_penalty = 1.25` → discourages the model from repeating phrases.\n",
    "  * `no_repeat_ngram_size = 4` → blocks any 4-gram from appearing twice.\n",
    "  * `max_new_tokens = 120` → hard upper limit on answer length.\n",
    "\n",
    "* **Post-processing pipeline**\n",
    "  1. **Remove prompt echo**  \n",
    "     Everything up to and including the last literal marker **`Answer concisely:`** is stripped away.\n",
    "  2. **Stop at the first unwanted repeat**  \n",
    "     If the model starts writing a new *“Question:”* block, that tail is cut off.\n",
    "  3. **Sentence trimming (optional)**  \n",
    "     The regex `sent_split` segments text on punctuation ( `. ! ?` ), then keeps only the **first two sentences**, guaranteeing brevity.\n",
    "\n",
    "The result is a clean, non-repetitive answer suitable for display in the Streamlit UI or console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer concisely: based on the dataset from cnbc, Apple's stock recently is around $ 200, and it is very bullish\n"
     ]
    }
   ],
   "source": [
    "print(rag_query(\"What happened to Apple Stock?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f240f70",
   "metadata": {},
   "source": [
    "### Final Thoughts  \n",
    "\n",
    "RAG reframes large language models not as encyclopedic oracles but as agile **reasoning engines** that consult a living library on demand. As corpora grow and models improve, the synergy between retrieval and generation will underpin the next wave of intelligent applications—Q&A bots that cite peer‑reviewed studies, code copilots that surface internal best practices, and storytelling engines that weave user memories into personalized narratives. Mastery of the RAG workflow—supported by rigorous evaluation—equips you to build AI solutions that are *not only creative and coherent but also credible and grounded in truth*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084e531",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "1. *ChatGPT* – conversational assistance and content generation for this notebook.  \n",
    "2. *Prompting Guide — “Retrieval‑Augmented Generation”*  \n",
    "   <https://www.promptingguide.ai/research/rag>  \n",
    "3. *TensorFlow Documentation*  \n",
    "   <https://www.tensorflow.org/>\n",
    "4. *Dataset*\n",
    "   <https://www.kaggle.com/datasets/notlucasp/financial-news-headlines>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138e5c8",
   "metadata": {},
   "source": [
    "## License  \n",
    "\n",
    "Unless otherwise noted, all **code snippets** in this notebook are released under the **MIT License**, allowing free use, modification, and redistribution with attribution.\n",
    "\n",
    "```text\n",
    "Copyright © 2025 Chen Yang (NUID 002837912)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the “Software”), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2c9d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
