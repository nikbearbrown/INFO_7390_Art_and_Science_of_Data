{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train IMDb Classifier\n",
        " This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. Here we are training an IMDb classifier with DistilBERT.\n",
        "\n",
        "The original lab exercises focused on the fine-tune a FLAN-T5 model to generate less toxic content with Meta AI's hate speech reward model.\n"
      ],
      "metadata": {
        "id": "t84ZBgWTDEdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the original lab exercises\n",
        "\n",
        "The original lab exercise was using the meta AIâ€™s hate speech reward model to calculate the rewards for content based on how hurtful it is. They were using the logits hate and not hate to classify the text input. They were then evaluating toxicity and fine tuning the model.\n",
        ""
      ],
      "metadata": {
        "id": "znbA2Y7FJEog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modifications and Improvements in this lab exercise\n",
        "\n",
        "As the original exercise used Reinforcement learning to identify hate and not hate, We tried to use a different model in a similar use case. In this lab exercise, we used the DistilBert model on our dataset to train a model that can be used to classify if a specific sentence review is positive or negative. We used a BERT model because it offers these advantages:-\n",
        "\n",
        "Transfer Learning: BERT is pre-trained on a massive amount of data, allowing it to capture general language patterns. Fine-tuning on a smaller dataset for a specific task leverages this pre-trained knowledge.\n",
        "\n",
        "Contextual Embeddings: BERT provides contextual embeddings, considering the surrounding words in a sentence. This helps capture the meaning of words in different contexts.\n"
      ],
      "metadata": {
        "id": "X_EOU__gI72_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model description\n",
        "DistilBERT is a transformers-based model that offers a more compact and faster alternative to BERT. It underwent pretraining on the same corpus in a self-supervised manner, leveraging the BERT base model as a teacher. During pretraining, the model focused on three key objectives:\n",
        "\n",
        "Distillation Loss: The model was trained to produce probabilities consistent with those of the BERT base model, essentially distilling the knowledge from its teacher.\n",
        "\n",
        "Masked Language Modeling (MLM): This involves a process where 15% of words in a sentence are randomly masked, and the model predicts these masked words. This approach, different from traditional RNNs or autoregressive models like GPT, enables the model to learn a bidirectional representation of the sentence.\n",
        "\n",
        "Cosine Embedding Loss: The model was also trained to generate hidden states that closely match those of the BERT base model, reinforcing the learning of similar inner representations of the English language.\n",
        "\n",
        "In summary, DistilBERT's training process involves distillation from a BERT base model, masked language modeling for bidirectional sentence representation, and cosine embedding loss to ensure similarity in hidden states. This results in a model that learns the nuances of the English language akin to its teacher model but with the advantage of faster inference and downstream task execution."
      ],
      "metadata": {
        "id": "4RtqgTJEDP7B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Hij9xHM5A7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d09607-85b8-4e65-b3f1-c3e613a4f7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLoSU4W65A7U"
      },
      "source": [
        "## Load IMDb dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The load_dataset function is used to load a dataset from the Hugging Face datasets library. Hugging Face provides a wide range of datasets for NLP tasks, and this function allows you to easily download and use them in your Python code. When you specify the name of the dataset you want to load, it will return a dictionary-like object that contains the dataset. The load_metric function is used to load an evaluation metric from the Hugging Face datasets library. Specific metrics are used to evaluate the performance of models on NLP tasks. This function allows to easily load such metrics. Both functions are part of the datasets library, which aims to provide a unified interface for working with various datasets and metrics in the field of natural language processing. Before running this code, you  need to have the datasets library installed, which you can do using pip."
      ],
      "metadata": {
        "id": "pA5IM5OXDzDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o42eJBIP5A7V"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The load_dataset function is used to load the \"imdb\" dataset. The \"imdb\" dataset refers to the Internet Movie Database dataset, which contains movie reviews labeled as positive or negative sentiment.\n",
        "\n",
        "The function returns a dictionary-like object (ds in this case) that contains information about the dataset, including the training, validation, and test splits, as well as metadata such as column names and data types."
      ],
      "metadata": {
        "id": "gkjq5IdwEnlb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VbU0TDCn5A7V"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GtBJ2hGK5A7V",
        "outputId": "4dba1296-843c-4f2c-c15e-fd66588a9997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e9NgYaLp5A7W",
        "outputId": "3558e880-d646-4376-d6a9-9b3613db81ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ds['train'].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmPkWqrZ5A7X"
      },
      "source": [
        "## Load Pretrained DistilBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the AutoModelForSequenceClassification and AutoTokenizer classes from the transformers library. These classes provide a convenient way to load pre-trained models and tokenizers without having to specify the exact model architecture.\n",
        "\n",
        "Here, the pre-trained DistilBERT model with an uncased vocabulary is used. The uncased models are trained on lowercase text, which is suitable for many English NLP tasks.\n",
        "\n",
        "The AutoTokenizer.from_pretrained method is used to load the tokenizer associated with the same pre-trained DistilBERT model. Tokenizers are used for converting text data into a format suitable for input to the model.\n",
        "\n",
        "After running this code, we will have a pre-trained DistilBERT model ready for sequence classification tasks and a corresponding tokenizer for processing input text. These components can then be used to tokenize input text and make predictions using the pre-trained model."
      ],
      "metadata": {
        "id": "q2KA2_UMEzwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kdcE2vKd5A7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d777b0f-4708-4d64-fbcd-013b548a0af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnKPCSie5A7Y"
      },
      "source": [
        "## Prepocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenize function takes a dictionary-like object as input, typically representing a dataset example. It extracts the text data from the 'text' field in each example and applies the tokenizer to tokenize the text.\n",
        "\n",
        "The map method is used to apply the tokenize function to each example in the dataset. The batched=True argument indicates that the tokenization should be applied in batches, which improves efficiency when dealing with large datasets.\n",
        "\n",
        "The result is a new dataset (tokenized_ds) where the 'text' field in each example has been replaced with tokenized representations suitable for input to a transformer model.\n"
      ],
      "metadata": {
        "id": "aRYJmKWtHmFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TrZwxH-S5A7Y"
      },
      "outputs": [],
      "source": [
        "def tokenize(examples):\n",
        "    outputs = tokenizer(examples['text'], truncation=True)\n",
        "    return outputs\n",
        "\n",
        "tokenized_ds = ds.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTMXP4dH5A7Y"
      },
      "source": [
        "## Prepare Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8EqJBboA5A7Y"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dnMNUB9M5A7Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate==0.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56qQ4JKf8Smz",
        "outputId": "72271953-415c-4942-be8c-d21eee51baeb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.20.3 in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we specify the parameters for training."
      ],
      "metadata": {
        "id": "JcTSNw_pIoHX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6vyGAWa15A7Z"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(num_train_epochs=1,\n",
        "                                  output_dir=\"distilbert-imdb\",\n",
        "                                  push_to_hub=True,\n",
        "                                  per_device_train_batch_size=1,\n",
        "                                  per_device_eval_batch_size=1,\n",
        "                                  evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5Xh4XJl35A7Z"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jaLufKqD5A7Z"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model, tokenizer=tokenizer,\n",
        "                  data_collator=data_collator,\n",
        "                  args=training_args,\n",
        "                  train_dataset=tokenized_ds[\"train\"],\n",
        "                  eval_dataset=tokenized_ds[\"test\"],\n",
        "                  compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgJhSHU_5A7Z"
      },
      "source": [
        "## Train Model and Push to Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip0hx14u5A7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "0aae58a2-7405-49a4-aa99-9c1a429c75e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1955' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1955/25000 1:17:54 < 15:19:22, 0.42 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovedVqQL5A7Z"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenges Faced during the process\n",
        "\n",
        "In this process, the first challenge faced was that of choosing a dataset which is not too small as that would be a problem that could lead to overfitting. Another problem face was getting the pre installed libraries and their version correct to use with the model while passing arguments. The hyperparameters that were used for training the model such as epoch size, batch size, learning rates had to be chosen carefully as to achieve optimal performance on the IMDB dataset. One main challenge was that depending on the available computational resources, training and fine-tuning DistilBERT was  slower compared to smaller models. Thus, to check if the model was getting trained properly, the batch size had to be changed to reduce the computational time. Regardless, the model took 15 hours to train."
      ],
      "metadata": {
        "id": "0b3e4-qtKDdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results\n",
        "\n",
        "This model can be used to classify if a review is positive or negative. This exercise gave us a model that is effective at understanding context and nuances in language, which is crucial for accurately analyzing movie reviews where sentiment can be subtle or complex. It is robust to the noise present in real-world data i.e online movie reviews. It can handle varied sentence structures, slang, and other idiosyncrasies common in user-generated content.\n",
        "\n",
        "In summary,we found an efficient, robust, and effective solution for sentiment analysis, especially when resources or processing power are limited."
      ],
      "metadata": {
        "id": "PXeoaUIxKG68"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}