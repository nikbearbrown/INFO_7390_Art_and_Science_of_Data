{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Data with Generative AI\n",
        "*Authors: Yash Gopalji Pankhania, Jeel Kanzaria, Megha Patel*\n",
        "\n",
        "**Abstract:**\n",
        "\n",
        "In this notebook, we'll use OpenAI's Generative Pre-Trained Transformer model to create a personalized chatbot. This chatbot will be trained on user-provided details on their age, gender, weight, height in addition to their health goals to gain a comprehensize plan on diet and fitness. This app will use AI to become a personalized trainer and help achieve health goals using data across the web. The popularity of transformer models, notably seen in OpenAI's ChatGPT and image generator DALL-E, has inspired this research. Our goal is to explore the transformative impact of transformers in generative AI, emphasizing their relevance in data science. The study includes an investigation into the reasons behind the success of transformers, supported by code examples demonstrating their potential in data generation. Additionally, we present the generated results with data summaries, aiming to offer a comprehensive understanding of transformers in generative AI—blending theoretical insights with practical applications for enhanced comprehension in the field of data science.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cjhr4CLDG3Nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have publicly hosted our health and fitness assistant on Streamlit: https://fitness-gpt.streamlit.app/"
      ],
      "metadata": {
        "id": "6li6wD0prN1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Health](https://github.com/jeelkan/Exploratory-Data-Analysis/assets/122841430/7323e108-d263-4377-aff8-cf6a9c0169d3)"
      ],
      "metadata": {
        "id": "Jh90jdMKoG5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Foundations of Generative AI"
      ],
      "metadata": {
        "id": "bB3onFDqv3CC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Introduction to generative AI and its applications.**\n",
        "\n",
        "Generative AI refers to models or algorithms that create (generate) brand-new output, such as text, images, videos, code, or data. The models makes new content by referring back to the vast amount of data they have been trained on, making new predictions. Generative AI differs from other forms of AI in that its sole purpose is to create new content without needing to gather data where as other forms of AI analyze existing data or perform tasks.\n",
        "\n",
        "Generative AI has become popular recently due to the development of the transformer models. The transformer model was introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, and it has since led to a significant leap in AI capabilities, outperforming any previous deep learning techniques and enabling groundbreaking progress in AI research. This innovative architecture has managed to effectively harness the growing computational resources, enabling AI models to scale and tackle more complex problems than ever before.\n",
        "\n",
        "**The relevance of data generation in various data science tasks.**\n",
        "\n",
        "In data science, the availability of large and diverse datasets is often a bottleneck. Generative AI addresses this issue by enabling the creation of synthetic data, reducing the reliance on limited or biased datasets. This is especially valuable when dealing with sensitive or scarce data, such as medical records or rare events.\n",
        "\n",
        "**Theoretical underpinnings of the chosen generative AI method.**\n",
        "\n",
        "1. **Attention Mechanism:**\n",
        "\n",
        "  The attention mechanism is fundamental to transformers. Traditional sequence-to-sequence models, like recurrent neural networks (RNNs), struggle with capturing long-range dependencies in sequential data. The attention mechanism allows transformers to focus on different parts of the input sequence when making predictions. This self-attention mechanism enables the model to weigh the importance of different elements dynamically.\n",
        "2. **Self-Attention and Multi-Head Attention:**\n",
        "\n",
        "  Self-attention allows each element in the sequence to consider other elements' information, capturing complex relationships within the data. Multi-head attention extends this concept by using multiple sets of attention weights, allowing the model to learn different aspects of the relationships simultaneously. This makes transformers highly effective at understanding contextual information across sequences.\n",
        "3. **Positional Encoding:**\n",
        "\n",
        "  Transformers do not inherently understand the order of elements in a sequence, as self-attention treats inputs independently. To introduce positional information, positional encodings are added to the input embeddings. These encodings provide the model with information about the positions of elements in the sequence, allowing it to consider the order of the data.\n",
        "4. **Encoder-Decoder Architecture:**\n",
        "\n",
        "  The transformer architecture is composed of an encoder and a decoder stack. The encoder processes the input sequence, while the decoder generates the output sequence. This separation allows transformers to handle various sequence-to-sequence tasks effectively.\n",
        "5. **Layer Normalization and Residual Connections:**\n",
        "\n",
        "  Each sub-layer in the transformer contains layer normalization and a residual connection. Layer normalization helps stabilize training by normalizing the inputs, and residual connections aid in the flow of information through the network. These components contribute to the stability and efficiency of training transformers.\n",
        "6. **Position-wise Feedforward Networks:**\n",
        "\n",
        "  Transformers include position-wise feedforward networks in each layer. These networks consist of fully connected layers and provide the model with the ability to capture non-linear relationships in the data. The inclusion of feedforward networks contributes to the expressive power of transformers.\n",
        "7. **Scaled Dot-Product Attention:**\n",
        "\n",
        "  The attention mechanism in transformers often uses scaled dot-product attention. This involves scaling the dot product of the query and key vectors to prevent the magnitudes of the dot products from becoming too large, making the learning process more stable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**How generative AI contributes to solving data-related problems.**\n",
        "\n",
        "  Generative AI contributes to data science by offering a solution to the problem of limited or biased datasets. By learning the underlying patterns and distributions from a training dataset, generative AI models can then generate new samples that share similar characteristics. This, in turn, enhances the diversity and representativeness of the available data, leading to improved model generalization and performance.\n"
      ],
      "metadata": {
        "id": "tFnjRXfvxR2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Data Generation\n"
      ],
      "metadata": {
        "id": "Ri-YXUHXwLMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Context"
      ],
      "metadata": {
        "id": "HfSdVy0nzwrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data generation using generative AI involves creating synthetic data samples that resemble real-world data. This process is particularly valuable when faced with challenges such as limited, biased, or sensitive datasets. Generative AI models, such as transformers, can learn underlying patterns from existing data and generate new, diverse samples that reflect those patterns.\n",
        "\n",
        "![Doctor](https://github.com/jeelkan/Exploratory-Data-Analysis/assets/122841430/7e05b4a3-d89f-4274-bf99-b18c639c5303)\n",
        "\n"
      ],
      "metadata": {
        "id": "zYt7ee0azrP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Signficance"
      ],
      "metadata": {
        "id": "uDKD0nz50PH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Significance of Generative AI in Data Generation**: Overcomes challenges of traditional datasets.\n",
        "- **Solution for Limited Data Scenarios**: Ideal for situations where acquiring extensive, diverse, or unbiased datasets is difficult.\n",
        "- **Enhancement of Dataset Diversity**: Creates synthetic data that mirrors real-world patterns, improving diversity and representation.\n",
        "- **Training Robust Machine Learning Models**: Essential for models to generalize effectively across various scenarios.\n",
        "- **Addressing Data Scarcity**: Supplements existing datasets for better model training and testing.\n",
        "- **Privacy Preservation in Sensitive Fields**: Generates synthetic data that maintains statistical properties while protecting sensitive information.\n",
        "- **Data Augmentation Capability**: Creates variations of existing data to train more resilient models.\n",
        "- **Beyond Traditional Approaches**: Offers innovative solutions to complex data-related challenges in modern applications.\n",
        "\n",
        "\n",
        "\n",
        "![Roadmap](https://github.com/jeelkan/Exploratory-Data-Analysis/assets/122841430/d14af4c5-bd93-4f2e-a511-88e96230fa68)\n",
        "\n"
      ],
      "metadata": {
        "id": "waDoSB82z6Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Principles of Data Generation\n"
      ],
      "metadata": {
        "id": "ZchAT06Q0LQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Learning Underlying Patterns:**\n",
        "\n",
        "  Generative AI models, based on principles such as neural networks, learn the underlying patterns and distributions from the input data during training. This knowledge is then used to generate new samples.\n",
        "2. **Transfer Learning:**\n",
        "\n",
        "  Pre-trained generative AI models, such as transformers, leverage transfer learning. They are trained on large, diverse datasets for a specific domain and can then be fine-tuned for various data generation tasks with smaller datasets.\n",
        "3. **Contextual Understanding:**\n",
        "\n",
        "  Models like transformers excel in capturing contextual relationships within the data. This contextual understanding is crucial for generating coherent and contextually relevant synthetic data.\n",
        "4. **Balancing Fidelity and Diversity:**\n",
        "\n",
        "  A key challenge in data generation is striking the right balance between fidelity to the original data and introducing diversity. Generative AI models aim to generate samples that are both realistic and diverse to improve the robustness of downstream applications.\n",
        "5. **Evaluation and Validation:**\n",
        "\n",
        "  The quality of generated data is evaluated using metrics specific to the task at hand. Validation ensures that the synthetic data aligns with the intended characteristics and is suitable for the desired application.\n",
        "\n",
        "### Transformers\n",
        "\n",
        "- **Popularity in Generative AI**: Transformers are widely used in natural language processing tasks.\n",
        "- **Versatile Applications**: Not just for data generation, but also for text generation, language translation, summarization, etc.\n",
        "- **Focus on General Characteristics**: Understanding transformers in the context of data generation tasks.\n",
        "- **Origins of Transformers**: Introduced in \"Attention is All You Need\" by Vaswani et al. (2017).\n",
        "- **Difference from RNNs**: Unlike Recurrent Neural Networks, transformers process sequences using self-attention, not sequentially.\n",
        "- **Efficient Data Processing**: Self-attention mechanism enables capturing long-range dependencies in data."
      ],
      "metadata": {
        "id": "3HctbzIG0AWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Purpose of Data Generation\n"
      ],
      "metadata": {
        "id": "diNXSC1h0Ggh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Text Generation:**\n",
        "\n",
        "  Transformers excel at generating coherent and contextually relevant text. This makes them valuable for tasks such as automatic content creation, creative writing, and text completion.\n",
        "2. **Language Translation:**\n",
        "\n",
        "  Transformers have been widely used for language translation tasks. They can generate translations with high fluency and accuracy, leveraging their ability to understand and capture contextual dependencies.\n",
        "3. **Summarization:**\n",
        "\n",
        "  Transformers can be applied to generate concise and informative summaries of longer texts. They can identify essential information and create coherent summaries.\n",
        "4. **Data Augmentation:**\n",
        "\n",
        "  In data science, transformers can be used for data augmentation by generating additional examples for training datasets. This helps improve model robustness and generalization."
      ],
      "metadata": {
        "id": "Yj7VKPIq0EEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analyzing the Generated Data\n"
      ],
      "metadata": {
        "id": "mjcQU0QewQi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Characteristics"
      ],
      "metadata": {
        "id": "8NWsuWgZxkZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated data from OpenAI's ChatGPT model consists of human-like text responses. The nature of the data is diverse, encompassing a wide range of topics, writing styles, and contextual information. The data is derived from a pre-training process where the model is exposed to vast amounts of publicly available text data from the internet, allowing it to learn grammar, facts, reasoning abilities, and some level of world knowledge. The properties of the data include variability in language use, context sensitivity, and the ability to generate coherent and contextually relevant responses to a wide array of user inputs."
      ],
      "metadata": {
        "id": "LVOmpTUyxo_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Application Areas"
      ],
      "metadata": {
        "id": "3xspxendxyql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated data from ChatGPT can find applications in various domains, including:\n",
        "\n",
        "* Content Creation: ChatGPT can assist in generating creative writing, stories, articles, and other forms of content.\n",
        "\n",
        "* Conversational Interfaces: The data is well-suited for building chatbots and virtual assistants, providing natural and contextually relevant responses to user queries.\n",
        "\n",
        "* Educational Tools: The generated content can be applied in educational settings to provide explanations, answer questions, and aid in learning across different subjects.\n",
        "\n",
        "* Programming Assistance: ChatGPT can assist developers by providing code snippets, explanations, and guidance in programming-related queries.\n",
        "\n",
        "* Idea Generation: It can be used to brainstorm and generate ideas for various tasks, projects, or creative endeavors.\n",
        "\n",
        "* Language Translation: While not explicitly trained for translation, ChatGPT can offer assistance in generating translated sentences or phrases.\n",
        "\n",
        "\n",
        "\n",
        "![Application](https://github.com/jeelkan/Exploratory-Data-Analysis/assets/122841430/a7323f9d-f854-403c-87d8-d56a7e87d3cf)\n"
      ],
      "metadata": {
        "id": "cLwXNinYx23s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analytical Insights"
      ],
      "metadata": {
        "id": "KsAM8Jqgx7o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The potential insights derived from analyzing the generated data are varied and depend on the context of use. Some potential analytical insights include:\n",
        "\n",
        "* Language Understanding:\n",
        "\n",
        "  Analysis of the generated text can provide insights into the model's understanding of language, including grammar, syntax, and semantics.\n",
        "\n",
        "* Contextual Adaptability:\n",
        "\n",
        "  Studying how well the model maintains context in conversations offers insights into its contextual adaptability and the ability to generate coherent responses.\n",
        "\n",
        "* Bias and Fairness:\n",
        "\n",
        "  Analyzing the content for biases and fairness considerations can provide insights into potential ethical considerations and areas for improvement in the training data.\n",
        "\n",
        "* Performance Metrics:\n",
        "\n",
        "  Using metrics like response coherence, relevance, and informativeness can offer insights into the model's overall performance in specific applications.\n",
        "\n",
        "* Limitations and Challenges:\n",
        "\n",
        "  By examining cases where the model struggles or provides inaccurate information, insights can be gained into its limitations and areas where additional fine-tuning or improvements may be necessary."
      ],
      "metadata": {
        "id": "jwnjsmQcyUiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Engaging with Generative AI for Data Generation\n",
        "\n"
      ],
      "metadata": {
        "id": "w1h74K3vwT2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Insights into ChatGPT's Data Generation Process"
      ],
      "metadata": {
        "id": "KFkUSIjEvtb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have given a brief introduction of Open AI's API structure, let's have ChatGPT show case its capabilities itself."
      ],
      "metadata": {
        "id": "9EoqTXyxva3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! The data generation process for training ChatGPT involves two main steps: pretraining and fine-tuning.\n",
        "\n",
        "1. **Pretraining:**\n",
        "   - In the pretraining phase, the model is trained on a large corpus of publicly available text from the internet. This corpus is diverse and covers a wide range of topics, styles, and domains.\n",
        "   - The model learns to predict the next word in a sentence based on the context provided by the preceding words. This process allows the model to capture grammar, syntax, facts, and reasoning abilities.\n",
        "   - It's important to note that during pretraining, the model doesn't have specific information about the documents in its training set, and it doesn't have awareness of individual documents or sources.\n",
        "\n",
        "2. **Fine-tuning:**\n",
        "   - After pretraining, the model is fine-tuned on a more narrow dataset that is generated with human reviewers following guidelines provided by OpenAI.\n",
        "   - The fine-tuning process involves iterative feedback between the human reviewers and the model. The reviewers review and rate model outputs for a range of example inputs. The model then generalizes from this feedback to respond to a wider array of user inputs.\n",
        "   - The fine-tuning process is designed to make the model more controlled and align it with human values. OpenAI provides guidelines to reviewers to ensure the model's behavior is consistent with certain ethical standards.\n",
        "\n",
        "It's important to note that the training data is anonymized and carefully stripped of personally identifiable information to respect privacy. The goal of the process is to create a language model that can generate coherent and contextually relevant responses while being mindful of ethical considerations and user safety. OpenAI continues to iterate on its models and training processes to improve performance and address potential biases or issues."
      ],
      "metadata": {
        "id": "Iw-ipWIlk1xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Generation Scenarios using ChatGPT"
      ],
      "metadata": {
        "id": "GkkyCJxuv_Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a better understanding of ChatGPT's architechure and data generation process, let's delve into some examples of data generation using this technique.\n",
        "\n",
        "1. Content Creation:\n",
        "\n",
        "    Prompt: \"Write a short story about a robot and its creator.\"\n",
        "\n",
        "    Response: \"In a small workshop, Dr. Smith toiled day and night, creating a sentient robot named Luna. As Luna's artificial intelligence developed, a unique bond formed between them...\"\n",
        "\n",
        "2. Programming Help:\n",
        "\n",
        "    Prompt: \"How do I write a Python program to calculate the factorial of a number?\"\n",
        "\n",
        "    Response: \"To calculate the factorial in Python, you can use a loop or recursion. Here's an example using a loop:\"\n",
        "3. Language Translation:\n",
        "\n",
        "    Prompt: \"Translate the following English sentence to French: 'Hello, how are you?'\"\n",
        "\n",
        "    Response: \"'Bonjour, comment ça va ?'\"\n",
        "4. Learning New Topics:\n",
        "\n",
        "    Prompt: \"Explain the concept of black holes in astrophysics.\"\n",
        "\n",
        "    Response: \"Black holes are regions in space where gravitational forces are so strong that nothing, not even light, can escape. They are formed from the remnants of massive stars...\"\n",
        "5. Creative Writing Assistance:\n",
        "\n",
        "    Prompt: \"Help me come up with a catchy slogan for a new coffee shop.\"\n",
        "\n",
        "    Response: \"Brewing Joy, One Cup at a Time!\"\n",
        "6. Idea Generation:\n",
        "\n",
        "    Prompt: \"Generate ideas for a science fair project related to renewable energy.\"\n",
        "\n",
        "    Response:\n",
        "    \"1. Solar-powered phone charger with recycled materials. 2. Wind turbine efficiency study. 3. Bioenergy from organic waste.\"\n",
        "7. Casual Conversation:\n",
        "\n",
        "    User: \"Tell me a joke!\"\n",
        "\n",
        "    Response: \"Why did the computer keep its drink on the windowsill? Because it wanted a byte with a view!\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gbg-muFocZt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation of the Quality and Diversity of Generated Data"
      ],
      "metadata": {
        "id": "IQV2nDqewUcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quality of the responses depends on the clarity and specificity of the prompts. The model's responses are based on patterns learned during training and may not always reflect the most up-to-date or accurate information so it is important to verify the data. Let's see how:\n",
        "\n",
        "* Human Evaluation:\n",
        "  * *Expert Review*: Have domain experts or knowledgeable individuals review the generated content. They can assess the accuracy, relevance, and overall quality of the information.\n",
        "  * *Crowdsourced Evaluation*: Gather feedback from a diverse group of human evaluators to obtain opinions on the quality, clarity, and appropriateness of generated responses (e.g. Amazon Mechanical Turk).\n",
        "\n",
        "* Diversity Metrics:\n",
        "  * *N-gram Analysis*: Examine the diversity of n-grams (sequences of adjacent words) in the generated text. Higher diversity indicates a broader range of language use.\n",
        "  * *Topic Coverage*: Check if the generated content covers a diverse set of topics. This can be done by analyzing the distribution of topics or keywords in the generated data.\n",
        "\n",
        "* Contextual Relevance:\n",
        "  * *Prompt Variability*: Test the model's ability to handle a variety of prompts. A good model should be able to provide relevant and coherent responses across different types of inputs.\n",
        "  * *Contextual Consistency*: Evaluate how well the model maintains context within a conversation. Responses should logically follow from the preceding dialogue or prompt.\n",
        "\n",
        "* Avoidance of Biases:\n",
        "  * *Bias Assessment*: Check for biases in the generated content, such as gender bias, cultural bias, or other forms of unfair or inappropriate behavior. This can be done manually or using automated tools designed to identify biases.\n",
        "* User Feedback:\n",
        "  * *Collect User Feedback*: Gather feedback from actual users who interact with the model. This can provide insights into how well the model meets user expectations and whether it produces useful and satisfactory responses.\n",
        "\n",
        "* Performance Benchmarks:\n",
        "  * *Task-Specific Metrics*: If the model is designed for a specific task (e.g., translation, summarization), use task-specific metrics to evaluate its performance. For instance, in translation tasks, BLEU scores can be used to measure translation quality.\n",
        "\n",
        "* Adversarial Testing:\n",
        "  * *Adversarial Input*: Test the model's robustness by providing challenging or adversarial inputs. This helps identify potential weaknesses and areas for improvement.\n"
      ],
      "metadata": {
        "id": "pLe4PoxOwRm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crafting Generative Data\n",
        "\n"
      ],
      "metadata": {
        "id": "GXXdEjUVwXuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Our Data Generation Task"
      ],
      "metadata": {
        "id": "IxyHa7YA4RoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Chatbot for Personalized Wellness Plans**: Developing a chatbot using OpenAI's ChatGPT to create diet and exercise plans based on user biometrics and goals, hosted on Streamlit.\n",
        "- **Holistic Health Management Approach**: Aims to integrate technology into personal health management seamlessly.\n",
        "- **Tailored Plans for Diverse Goals**: Plans designed for weight loss, muscle gain, or overall well-being, considering lifestyle factors.\n",
        "- **Adaptive and Iterative Feedback Incorporation**: Chatbot will adjust recommendations based on user feedback and progress.\n",
        "\n",
        "- **Generative AI in Healthcare**: Multiple applications in healthcare for content and behavior generation.\n",
        "    - **Medical Image Generation**: Synthetic MRI or CT images; histopathology images for algorithm training.\n",
        "    - **Drug Discovery**: Generating molecular structures for drug candidate exploration.\n",
        "    - **Clinical Data Augmentation**: Creating synthetic patient data to enhance datasets while ensuring privacy.\n",
        "    - **NLP in Healthcare**: Automated clinical documentation generation to reduce administrative tasks.\n",
        "    - **Chatbots for Patient Interaction**: Providing information, mental health support, and patient engagement.\n",
        "    - **Personalized Medicine**: Generating tailored treatment plans based on individual data.\n",
        "    - **Simulation for Medical Training**: Creating virtual patients for risk-free practice and training.\n",
        "    - **Genomic Data Analysis**: Generating synthetic genomic data for rare disease study and genetic variation analysis.\n",
        "    - **Disease Prediction and Early Diagnosis**: Developing risk prediction models for early intervention.\n",
        "    - **Healthcare Robotics**: Simulating human-like behavior in healthcare robots for patient interaction.\n",
        "\n",
        "![use case health](https://github.com/jeelkan/Exploratory-Data-Analysis/assets/122841430/b2b5f4ee-53e6-43c5-95ad-b90cfcd39149)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfaGZpXJu73T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Expected Format of Our Data"
      ],
      "metadata": {
        "id": "dDLTPZLE4sN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the generated data will be separated into two forms.\n",
        "\n",
        "For the diet plan, we specifically asked for the following format:\n",
        "Breakfast, Mid-Morning Snack, Lunch, Evening Snack, Dinner.\n",
        "\n",
        "For the exercise plan, we expect workouts broken down by the day of the week. This plan is more flexible based on the goal of the user and expected to vary each time it is ran.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I6Yel_Jf4oKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our Streamlit App, we added constraints to the input to ensure the generated data meets the desired criteria. For example, we must not provide a diet plan for a 5 year old nor should the weight inputted be 50 lbs. Accordingly, the following constraints are added to the web app:\n",
        "*  age: number from 18 - 100\n",
        "*  gender: choices  (male or female)\n",
        "*  height: number in cm from 100 - 250\n",
        "*  weight: number in kg from 30 - 200\n",
        "*  goal: choices (Gain Muscle, Lose Weight, or Maintain Weight)\n",
        "* activity: choices (Sedentary, Light, Moderate, or Active)\n"
      ],
      "metadata": {
        "id": "RiEBRgqyEikQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will provide illustrative examples in the next section."
      ],
      "metadata": {
        "id": "rlYVWMQ74q56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demonstrating Data Generation"
      ],
      "metadata": {
        "id": "q6gCNjngwbXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Our Code"
      ],
      "metadata": {
        "id": "dv_GmL2AunGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our code is a Streamlit app for a Health and Fitness Assistant that utilizes OpenAI's GPT-3.5 Turbo model to generate personalized diet plans and workout plans based on user input.\n",
        "\n",
        "PLEASE NOTE: Streamlit apps are currently not support on Google Colab. You will get an error if it is run here. We recommend running this on your own machine.\n",
        "\n",
        "Let's break down the code:"
      ],
      "metadata": {
        "id": "V13Z7xtA03X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXkYxp0m_TNT",
        "outputId": "9807b4e5-87c4-4255-9e83-547f422e6baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.28.2 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2UzDOO_qcx",
        "outputId": "8073d6e4-3d97-4b2c-96e2-8fa88713b68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required packages\n",
        "import streamlit as st\n",
        "import openai\n",
        "import os\n",
        "import toml"
      ],
      "metadata": {
        "id": "TQOgsAq1u_YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize session state for conversation history\n",
        "if 'conversation' not in st.session_state:\n",
        "    st.session_state.conversation = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKuDD9R5vjMP",
        "outputId": "4e70c7d7-224c-4f6e-8438-4085f452f41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-20 22:04:02.623 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetching the OpenAI API Key\n",
        "openai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #Enter your OPENAI API Key here"
      ],
      "metadata": {
        "id": "ZVgIi1lKvkry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit app layout\n",
        "st.title('Health and Fitness Assistant')\n",
        "\n",
        "# Dropdown for selecting between Diet Plan and Workout Plan\n",
        "with st.sidebar:\n",
        "    st.header(\"Choose Your Plan\")\n",
        "    plan_type = st.selectbox('Select a Plan', ['Diet Plan and Recipes', 'Workout Plan'])\n",
        "\n",
        "    if plan_type == 'Diet Plan and Recipes':\n",
        "        # Input fields for Diet Plan\n",
        "        st.header(\"Input Your Details for Diet Plan\")\n",
        "        age = st.number_input('Age', min_value=18, max_value=100)\n",
        "        gender = st.selectbox('Gender', ['Male', 'Female'])\n",
        "        height = st.number_input('Height in cm', min_value=100, max_value=250)\n",
        "        weight = st.number_input('Weight in kg', min_value=30, max_value=200)\n",
        "        goal = st.selectbox('Goal', ['Gain Muscle', 'Lose Weight'])\n",
        "        activity = st.selectbox('Activity Level', ['Sedentary', 'Light', 'Moderate', 'Active'])\n",
        "        submit_button_diet = st.button('Generate Diet Plan and Recipes')\n",
        "\n",
        "    elif plan_type == 'Workout Plan':\n",
        "        # Input fields for Workout Plan\n",
        "        st.header(\"Input Your Details for Workout Plan\")\n",
        "        age_w = st.number_input('Age', min_value=18, max_value=100, key='workout_age')\n",
        "        gender_w = st.selectbox('Gender', ['Male', 'Female'], key='workout_gender')\n",
        "        height_w = st.number_input('Height in cm', min_value=100, max_value=250, key='workout_height')\n",
        "        weight_w = st.number_input('Weight in kg', min_value=30, max_value=200, key='workout_weight')\n",
        "        weight_goal = st.selectbox('Weight Goal', ['Gain Muscle', 'Lose Weight', 'Maintain Weight'], key='workout_goal')\n",
        "        submit_button_workout = st.button('Generate Workout Plan')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT8V55acv1eQ",
        "outputId": "fcb9dbe7-5e22-485b-dfb0-94a549478eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-20 22:04:02.967 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the output generation outside the sidebar block\n",
        "if plan_type == 'Diet Plan and Recipes' and submit_button_diet:\n",
        "    st.session_state.user_text = \"\"\n",
        "    st.session_state.conversation = None\n",
        "\n",
        "    initial_prompt = \"Act like a ChatGPT. You know everything.\"\n",
        "    if st.session_state.conversation is None:\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        st.session_state.conversation = []\n",
        "\n",
        "        question1 = f\"Generate a diet plan for a {age}-year-old {gender}, height {height} cm, weight {weight} kg, goal: {goal}, activity level: {activity}. The diet plan should be in the following format: Breakfast, Mid-Morning Snack, Lunch, Evening Snack, Dinner. Generate it in proper markdown format.\"\n",
        "\n",
        "        response1 = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": initial_prompt},\n",
        "                {\"role\": \"user\", \"content\": question1},\n",
        "            ],\n",
        "        )['choices'][0]['message']['content']\n",
        "\n",
        "        question2 = f\"Generate simple recipes for the dishes present in the following diet plan: {response1}. The recipes should be in the following format: Dish name, Ingredients, Instructions. Generate it in proper markdown format.\"\n",
        "\n",
        "        response2 = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": initial_prompt},\n",
        "                {\"role\": \"user\", \"content\": question2},\n",
        "            ],\n",
        "        )['choices'][0]['message']['content']\n",
        "\n",
        "        st.session_state.conversation.extend([response1, response2])\n",
        "\n",
        "    st.write(\"Diet Plan:\\n\\n\" + st.session_state.conversation[0])\n",
        "    st.write(\"Recipes:\\n\\n\" + st.session_state.conversation[1])\n",
        "\n",
        "elif plan_type == 'Workout Plan' and submit_button_workout:\n",
        "    st.session_state.user_text = \"\"\n",
        "    st.session_state.conversation = None\n",
        "\n",
        "    initial_prompt = \"Act like a ChatGPT. You know everything.\"\n",
        "    if st.session_state.conversation is None:\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        st.session_state.conversation = []\n",
        "\n",
        "        question = f\"Generate a workout plan for a {age_w}-year-old {gender_w}, height {height_w} cm, weight {weight_w} kg, weight goal: {weight_goal}. The workout plan should be tailored to the individual's needs. Generate it in proper markdown format.\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": initial_prompt},\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "            ],\n",
        "        )['choices'][0]['message']['content']\n",
        "\n",
        "        st.session_state.conversation.append(response)\n",
        "\n",
        "    st.write(\"Workout Plan:\\n\\n\" + st.session_state.conversation[0])"
      ],
      "metadata": {
        "id": "ynrDdAB50-n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Streamlit Setup:\n",
        "  * Import the required libraries: streamlit, openai, dotenv, and os.\n",
        "  * Initialize Streamlit app layout with st.title and set up the sidebar for plan selection.\n",
        "2. Session State Initialization:\n",
        "  * Initialize session state using Streamlit's st.session_state to store the conversation history.\n",
        "3. Environment Variables:\n",
        "  * Load environment variables using dotenv to secure the OpenAI API key.\n",
        "4. Sidebar Plan Selection:\n",
        "  * Allow the user to choose between a \"Diet Plan and Recipes\" or a \"Workout Plan\" using a dropdown in the sidebar.\n",
        "  * Collect user input for the selected plan type (diet or workout).\n",
        "5. User Input Fields:\n",
        "  * Inside the sidebar block, capture user details for the selected plan type using Streamlit input fields (e.g., age, gender, height, weight, goal, activity level for diet; age, gender, height, weight, weight goal for workout).\n",
        "6. Generate Plan Button:\n",
        "  * Include a button to trigger the generation of the selected plan (diet or workout).\n",
        "7. Output Handling:\n",
        "  * Outside the sidebar block, handle the output generation based on the selected plan type and the submission of the corresponding button.\n",
        "  * For \"Diet Plan and Recipes,\" use the OpenAI ChatGPT model to generate a diet plan and associated recipes. Display the generated content in proper markdown format.\n",
        "  * For \"Workout Plan,\" use the model to generate a personalized workout plan. Display the generated workout plan in proper markdown format.\n",
        "8. Conversation History:\n",
        "  * Use Streamlit's session state to store and manage the conversation history, allowing for a coherent interaction with the model across multiple user inputs.\n",
        "9. OpenAI API Calls:\n",
        "  * Utilize the OpenAI GPT-3.5 Turbo model for generating responses based on user prompts. The questions are formatted according to the plan type and user input.\n",
        "10. Display Results:\n",
        "  * Display the generated diet plan, recipes, or workout plan in the Streamlit app.\n",
        "\n",
        "#### **Note:** Ensure that you have the necessary environment variables, including the OpenAI API key, to run this code successfully. Also, keep in mind OpenAI's usage policies and guidelines when making API requests."
      ],
      "metadata": {
        "id": "hn3PrM2X1ii_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizing the ChatGPT API for crafting a health and fitness app has demonstrated the versatility and responsiveness achievable through generative AI. The personalized diet plans and workout routines generated by ChatGPT exemplify the potential of AI in promoting individual well-being."
      ],
      "metadata": {
        "id": "EKIjn1LK7aJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generated Data"
      ],
      "metadata": {
        "id": "XJ-1Ce5k0xNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following example, we entered an 18 year old man's biodata to obtain a diet plan. On the left is the input section and on the right is the generated diet plan that is broken down into meals showing an example of what to eat followed by simple reminders to ensure the user is following healthy habits."
      ],
      "metadata": {
        "id": "8W-WuzyDpQ71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![dietscreenshot](https://github.com/Draconian10/AI_Health_Assistant/assets/32498703/86d2999b-aa05-4e87-80bd-f5f948c3c6ad)"
      ],
      "metadata": {
        "id": "iQGH7QDsoRtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next example, we have entered the biodata for a 24 year old woman with a similar format. As you see on the right hand side, we have generated a workout plan broken down by the day of the week followed by similar advise on healthy habits."
      ],
      "metadata": {
        "id": "UKCPuBIIpxnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fitnessscreenshot](https://github.com/Draconian10/AI_Health_Assistant/assets/32498703/b79147ee-b4d8-4d1e-b39b-7f0fbdf918c7)\n"
      ],
      "metadata": {
        "id": "8adlrz1ipPe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation and Justification"
      ],
      "metadata": {
        "id": "OM35OqNQwdzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Assessing the Effectiveness"
      ],
      "metadata": {
        "id": "n7DZ63OluJEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the generated outputs from our chatgpt model, we can clearly see the plans are well designed and well written. The model has learned to provide a detailed example of dietary and fitness plans as well as reminders of maintaining a healthy lifestyle.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lKTeiypUMLBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation of the Data using ROUGE\n",
        "\n"
      ],
      "metadata": {
        "id": "QwvI7bw3uAwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE, which stands for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics used for the automatic evaluation of machine-generated text, particularly in the context of text summarization and document clustering. The primary goal of ROUGE is to measure the quality of the generated summary by comparing it with one or more reference summaries or human-created model summaries."
      ],
      "metadata": {
        "id": "Kt1ADjLWuFao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the rouge package\n",
        "!pip install rouge-score\n",
        "\n",
        "# Import necessary libraries\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Example of reference and generated summaries\n",
        "reference_summary = \"\"\"# Workout Plan for Muscle Gain\n",
        "\n",
        "## Personal Information:\n",
        "- **Name:** [Individual's Name]\n",
        "- **Age:** 24\n",
        "- **Gender:** Female\n",
        "- **Height:** 165 cm\n",
        "- **Weight:** 54 kg\n",
        "- **Weight Goal:** Gain Muscle\n",
        "\n",
        "## Workout Schedule:\n",
        "\n",
        "### Day 1: Upper Body\n",
        "1. **Warm-up (10 minutes):**\n",
        "   - Jumping jacks, arm circles, light cardio.\n",
        "\n",
        "2. **Strength Training:**\n",
        "   - Bench Press: 3 sets x 10 reps\n",
        "   - Bent-over Rows: 3 sets x 12 reps\n",
        "   - Overhead Shoulder Press: 3 sets x 10 reps\n",
        "\n",
        "3. **Isolation Exercises:**\n",
        "   - Bicep Curls: 3 sets x 12 reps\n",
        "   - Tricep Dips: 3 sets x 15 reps\n",
        "\n",
        "4. **Core Work:**\n",
        "   - Plank: 3 sets x 30 seconds\n",
        "   - Russian Twists: 3 sets x 15 reps each side\n",
        "\n",
        "### Day 2: Lower Body\n",
        "1. **Warm-up (10 minutes):**\n",
        "   - Jumping jacks, leg swings, light cardio.\n",
        "\n",
        "2. **Strength Training:**\n",
        "   - Squats: 4 sets x 12 reps\n",
        "   - Deadlifts: 3 sets x 10 reps\n",
        "   - Lunges: 3 sets x 15 reps each leg\n",
        "\n",
        "3. **Isolation Exercises:**\n",
        "   - Leg Press: 3 sets x 12 reps\n",
        "   - Leg Curls: 3 sets x 15 reps\n",
        "\n",
        "4. **Core Work:**\n",
        "   - Leg Raises: 3 sets x 15 reps\n",
        "   - Bicycle Crunches: 3 sets x 20 reps\n",
        "\n",
        "### Day 3: Rest or Active Recovery\n",
        "- Light activities such as walking, yoga, or stretching.\n",
        "\n",
        "### Day 4: Full Body\n",
        "1. **Warm-up (10 minutes):**\n",
        "   - Jumping jacks, dynamic stretches, light cardio.\n",
        "\n",
        "2. **Compound Movements:**\n",
        "   - Deadlifts: 3 sets x 10 reps\n",
        "   - Pull-ups or Lat Pulldowns: 3 sets x 10 reps\n",
        "   - Push-ups: 3 sets x 15 reps\n",
        "\n",
        "3. **Isolation Exercises:**\n",
        "   - Hammer Curls: 3 sets x 12 reps\n",
        "   - Tricep Kickbacks: 3 sets x 15 reps\n",
        "\n",
        "4. **Core Work:**\n",
        "   - Plank Variations: Side plank, high plank, 3 sets x 30 seconds each.\n",
        "\n",
        "### Day 5: Cardio and Active Recovery\n",
        "- Cardiovascular exercise of choice (running, cycling, swimming) for 30-40 minutes.\n",
        "\n",
        "### Day 6: Rest or Active Recovery\n",
        "- Focus on restorative activities to allow muscles to recover.\n",
        "\n",
        "### Day 7: Rest\n",
        "- Complete rest day.\n",
        "\n",
        "## Notes:\n",
        "- **Progression:** Increase weights gradually to challenge muscles.\n",
        "- **Nutrition:** Consume a balanced diet with a focus on protein for muscle repair.\n",
        "- **Hydration:** Stay well-hydrated throughout the day.\n",
        "- **Sleep:** Aim for 7-8 hours of quality sleep for optimal recovery.\n",
        "\n",
        "Remember to listen to your body, and if you have any medical conditions or concerns, consult with a healthcare professional or fitness expert before starting this workout plan.\n",
        "\"\"\"\n",
        "generated_summary = \"\"\"Certainly! Here's a sample workout plan tailored for a 24-year-old female with a height of 165 cm, weight of 54 kg, and a goal to gain muscle. This is a general plan, and it's always advisable to consult with a fitness professional or healthcare provider before starting a new workout program.\n",
        "\n",
        "```markdown\n",
        "# Workout Plan for a 24-year-old Female (Goal: Gain Muscle)\n",
        "\n",
        "## Personal Information\n",
        "- **Age:** 24\n",
        "- **Gender:** Female\n",
        "- **Height:** 165 cm\n",
        "- **Weight:** 54 kg\n",
        "- **Weight Goal:** Gain Muscle\n",
        "\n",
        "## Overview\n",
        "This workout plan focuses on a combination of strength training and cardiovascular exercises to help achieve the goal of gaining muscle. The plan is designed for four workout days per week with one day of active rest or light activity.\n",
        "\n",
        "### Day 1: Full Body Strength\n",
        "- **Warm-up:** 10 minutes of light cardio (jogging, jumping jacks)\n",
        "- **Strength Training:**\n",
        "  - Squats: 3 sets x 8-10 reps\n",
        "  - Bench Press: 3 sets x 8-10 reps\n",
        "  - Bent Over Rows: 3 sets x 10 reps\n",
        "  - Plank: 3 sets x 30 seconds\n",
        "- **Cardio:** 15 minutes of moderate-intensity cardio (running, cycling)\n",
        "\n",
        "### Day 2: Active Rest or Light Activity\n",
        "- **Active Rest:** Yoga, walking, or light stretching\n",
        "\n",
        "### Day 3: Upper Body Strength\n",
        "- **Warm-up:** 10 minutes of light cardio\n",
        "- **Strength Training:**\n",
        "  - Overhead Press: 3 sets x 8-10 reps\n",
        "  - Pull-Ups or Lat Pulldowns: 3 sets x 8-10 reps\n",
        "  - Tricep Dips: 3 sets x 12 reps\n",
        "  - Plank: 3 sets x 30 seconds\n",
        "- **Cardio:** 15 minutes of moderate-intensity cardio\n",
        "\n",
        "### Day 4: Active Rest or Light Activity\n",
        "- **Active Rest:** Yoga, walking, or light stretching\n",
        "\n",
        "### Day 5: Lower Body Strength\n",
        "- **Warm-up:** 10 minutes of light cardio\n",
        "- **Strength Training:**\n",
        "  - Deadlifts: 3 sets x 8-10 reps\n",
        "  - Lunges: 3 sets x 12 reps (each leg)\n",
        "  - Leg Press: 3 sets x 10 reps\n",
        "  - Plank: 3 sets x 30 seconds\n",
        "- **Cardio:** 15 minutes of moderate-intensity cardio\n",
        "\n",
        "### Day 6: Cardiovascular Workout\n",
        "- **Cardio:** 30-45 minutes of your favorite cardiovascular exercise (running, cycling, swimming)\n",
        "\n",
        "### Day 7: Rest\n",
        "- **Rest day:** Allow your body to recover.\n",
        "\n",
        "## Important Notes\n",
        "- Ensure proper form during strength training exercises to prevent injury.\n",
        "- Gradually increase weights and intensity as your strength improves.\n",
        "- Stay hydrated and maintain a balanced diet to support muscle growth.\n",
        "- Listen to your body, and if you experience pain beyond normal muscle soreness, consult with a fitness professional or healthcare provider.\n",
        "\n",
        "Remember, consistency is key to seeing results. Adjust the intensity and duration based on your fitness level and how your body responds. If in doubt, seek guidance from a fitness professional.\n",
        "```\n",
        "\n",
        "Feel free to modify the plan based on personal preferences, fitness level, and any specific considerations or restrictions.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(\"ROUGE-1 F1 Score:\", rouge_scores['rouge1'].fmeasure)\n",
        "print(\"ROUGE-2 F1 Score:\", rouge_scores['rouge2'].fmeasure)\n",
        "print(\"ROUGE-L F1 Score:\", rouge_scores['rougeLsum'].fmeasure)"
      ],
      "metadata": {
        "id": "RjId65Q_OzT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1da115-10a3-4f30-c009-7ba9fdc7e304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=e009831e3f70174b1d3c8da26187ccd1793ba177c5258e58826a53b49c9993d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "ROUGE-1 F1 Score: 0.5739130434782609\n",
            "ROUGE-2 F1 Score: 0.32876712328767127\n",
            "ROUGE-L F1 Score: 0.5540372670807454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting these scores involves understanding the trade-off between precision and recall. A higher F1 score indicates a better balance between precision (correctly identified relevant terms) and recall (coverage of all relevant terms). These scores are crucial in evaluating the effectiveness of text generation models, especially in tasks such as summarization, where capturing the essence of the reference text is essential. However, ROUGE scores may not fully capture the quality, coherence, or fluency of the generated text. Let's interpret it's scores:\n",
        "1. ROUGE-1 F1 Score: 0.5739130434782609\n",
        "    \n",
        "    This score is based on the overlap of unigrams (single words) between the generated text and the reference text(s). An F1 score of 0.57 indicates that about 57% of the unigrams in the generated text match those in the reference text(s). A higher ROUGE-1 score generally suggests better precision and recall of individual words.\n",
        "2. ROUGE-2 F1 Score: 0.32876712328767127\n",
        "    \n",
        "    This score involves the overlap of bigrams (two consecutive words) between the generated text and the reference text(s). An F1 score of 0.33 suggests that approximately 33% of the bigrams in the generated text match those in the reference text(s). ROUGE-2 is more specific than ROUGE-1 and evaluates the quality of two-word sequences.\n",
        "3. ROUGE-L F1 Score: 0.5540372670807454\n",
        "\n",
        "    The ROUGE-L score is based on the longest common subsequence between the generated text and the reference text(s). An F1 score of 0.55 implies that about 55% of the longest common subsequences in the generated text match those in the reference text(s). ROUGE-L is more flexible and does not require consecutive word matches, considering subsequence similarity.\n",
        "\n",
        "These ROUGE scores demonstrate approximately 50% commonality between the example and generated outputs. However, we can see that the reason for this reason can be caused by the difference in the plan itself. This variability of the plans is the strength of ChatGPT and expected behavior. Physically observing the generated and reference outputs, we see that the format and advise between the two are similar and indeed accurate. Therefore, we can verify the model's data as acceptable and well performed."
      ],
      "metadata": {
        "id": "O8h8GhShpL2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Potential Applications of the Generated Data in Data Science tasks\n",
        "Using Open AI API's chatgpt model, we generated a comprensive plan for dieting and fitness. This plan, while backed with science found on the internet, is always unique and changes everytime we run the code. Issues that may arise is that since the model is trained by data found on the internet, our model may be susceptible to fad diets that are unhealthy and dangerous. Users need to be cautious and critically evaluate the information provided. The model can generate coherent and contextually relevant responses over extended lengths of text. However, the coherence may vary, and long passages might start to lose focus or deviate from the initial prompt.\n",
        "\n",
        "This generated data can be applied for those who are working towards a better lifestyle and do not know where to start. By providing, ideas for meals based on their weight goal, we can mimic advice that personal trainers would give without paying for additional costs.\n",
        "\n",
        "From the generated data, some potential insights we gain are:\n",
        "1. Personalized Recommendations:\n",
        "\n",
        "  Tailored diet and exercise plans based on individual characteristics such as age, gender, weight, height, and health goals.\n",
        "2. Nutritional Guidance:\n",
        "\n",
        "  Information on the nutritional content of recommended foods and details about macronutrients (carbohydrates, proteins, fats), micronutrients, and calorie intake.\n",
        "3. Fitness Routine Optimization:\n",
        "\n",
        "  Optimized workout routines that align with the user's fitness level, preferences, and goals, taking into account factors like strength training, cardio, and flexibility exercises.\n",
        "4. Educational Content:\n",
        "\n",
        "  Informational insights to enhance the user's understanding of nutrition, exercise physiology, and the science behind the recommended plans.\n",
        "\n",
        "By harnessing the potential of ChatGPT, we have successfully developed a Health and Fitness Assistant, capable of providing personalized diet and workout plans tailored to individual user specifications. The utilization of this cutting-edge technology not only showcases the advancements in artificial intelligence but also highlights the practical implications in promoting health and well-being."
      ],
      "metadata": {
        "id": "NgiHBbL-tOAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "LbWYEETx6B4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, this paper has explored the transformative potential of generative artificial intelligence, particularly the transformer architecture, through the development of a Health and Fitness Assistant using OpenAI's ChatGPT API. This innovation exemplifies the versatility and adaptability of generative models in creating personalized solutions, in this case, for health and fitness. The application provides tailored diet and workout plans, showcasing how AI advancements can be practically applied to enhance health and well-being. Overall, this work highlights the significant role of generative AI in shaping the future of personal health management, marking a step towards an era where AI is a key contributor to our holistic well-being."
      ],
      "metadata": {
        "id": "aI6GNx8e6DSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Sckit learn offcial documentation\n",
        "2. Refered Towards Data Science\n",
        "3. Eli5 official documentation\n",
        "4. https://www.zdnet.com/article/what-is-generative-ai-and-why-is-it-so-popular-heres-everything-you-need-to-know/\n",
        "5. https://www.linkedin.com/pulse/rise-transformers-why-sudden-jump-ai-capabilities-steve-wilson/\n",
        "\n",
        "The algorithms were referred directly from the **Sckit learn official documentation**. Visualization was referred from the  Machine Learning with scikit-learn Quick Start Guide and **Towards Data Science** (How do you check the quality of your regression model in Python?).  The remaining code was written independently. Feature importance reference is taken from **eli5 offical documentation**"
      ],
      "metadata": {
        "id": "6qkFuvsjgAmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# License\n",
        "Copyright [2023]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "WPmr6XlPCHyW"
      }
    }
  ]
}