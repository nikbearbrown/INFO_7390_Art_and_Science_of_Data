{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c5b300",
   "metadata": {},
   "source": [
    "# Crash Course in Generative AI Worked Examples\n",
    "\n",
    "By: Shivani Sahu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdd8d9",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "In this notebook, we will delve into various inference techniques, including zero-shot, one-shot, and few-shot methods, to assess their impact on the quality of outputs. We will focus on prompt engineering to understand how it affects model responses. Utilizing different Google models as foundational architectures, we aim to deepen our understanding of in-context learning. We will also introduce the Instruct model, a variant fine-tuned for specific instructions, to explore how models can be tailored for particular tasks. Our analysis will include both manual and quantitative evaluations, using ROUGE scores to measure performance. Additionally, we will investigate the application of Prompt-based Extractive Fine-tuning (PEFT) models to see how variations in prompts influence summarization results. The notebook will further cover the process of fine-tuning models to detoxify summaries, using reinforcement learning strategies like feedback and rewards, and qualitative assessments to highlight noticeable differences in model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09f970",
   "metadata": {},
   "source": [
    "# Breif about the 3 week's Lab:\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c5b29",
   "metadata": {},
   "source": [
    "- Week 1 of the course covers the basics of Generative AI, including model pre-training, the architecture of large language models (LLMs), and the project lifecycle, with a focus on the computational and strategic decisions involved. \n",
    "- Week 2 delves into fine-tuning LLMs using prompt datasets to enhance performance and introduces Parameter-efficient Fine Tuning (PEFT) to mitigate computational costs and prevent catastrophic forgetting.\n",
    "- Week 3 explores reinforcement learning, specifically RLHF (Reinforcement Learning from Human Feedback), to improve model alignment and performance, and discusses methods to enhance LLM reasoning through chain-of-thought prompting and overcome knowledge cut-offs with advanced information retrieval techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5334b9c",
   "metadata": {},
   "source": [
    "# Lab 1 - Generative AI Use Case: Dialogue Summarization\n",
    "Welcome to the hands-on component of this course. In this lab, we will tackle the task of summarizing dialogues using generative AI. We'll examine how different input texts influence the model's output and engage in prompt engineering to steer the model toward our desired task. Through comparisons of zero-shot, one-shot, and few-shot inferences, we will begin to explore prompt engineering and discover how it can improve the generative capabilities of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3b331",
   "metadata": {},
   "source": [
    "## 1 - Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea2fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./anaconda3/lib/python3.10/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/lvwerra/trl.git@25fa1bd\n",
      "  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /private/var/folders/1t/5d1gml0j7gd7ff6tt8s4rzfh0000gn/T/pip-req-build-j_h68r0b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /private/var/folders/1t/5d1gml0j7gd7ff6tt8s4rzfh0000gn/T/pip-req-build-j_h68r0b\n",
      "\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 25fa1bd\n",
      "  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in ./anaconda3/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in ./anaconda3/lib/python3.10/site-packages (from trl==0.4.2.dev0) (4.27.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in ./anaconda3/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.23.5)\n",
      "Requirement already satisfied: accelerate in ./anaconda3/lib/python3.10/site-packages (from trl==0.4.2.dev0) (0.29.2)\n",
      "Requirement already satisfied: datasets in ./anaconda3/lib/python3.10/site-packages (from trl==0.4.2.dev0) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.4.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2022.7.9)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.64.1)\n",
      "Requirement already satisfied: psutil in ./anaconda3/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./anaconda3/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (0.4.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in ./anaconda3/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets->trl==0.4.2.dev0) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.9.4)\n",
      "Requirement already satisfied: responses<0.19 in ./anaconda3/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./anaconda3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./anaconda3/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n",
      "Building wheels for collected packages: trl\n",
      "  Building wheel for trl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67534 sha256=7d2c266181197f28acf97c14c1d47888b1d244dcd73a7616b798e2d057616300\n",
      "  Stored in directory: /private/var/folders/1t/5d1gml0j7gd7ff6tt8s4rzfh0000gn/T/pip-ephem-wheel-cache-xruc8kzd/wheels/24/b4/20/2fa3a1e47c0411c39e198029315e3af2a2c1d59132913f136f\n",
      "Successfully built trl\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.4.2.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " %pip install --upgrade pip\n",
    " %pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "     torchdata==0.5.1 --quiet\n",
    "\n",
    " %pip install \\\n",
    "     transformers==4.27.2 \\\n",
    "     datasets==2.11.0  --quiet\n",
    "\n",
    " %pip install \\\n",
    "     evaluate==0.4.0 \\\n",
    "     rouge_score==0.1.2 \\\n",
    "     loralib==0.1.1 \\\n",
    "     peft==0.3.0 --quiet\n",
    "\n",
    " # Installing the Reinforcement Learning library directly from github.\n",
    "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108ac527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e15fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py7zr\n",
      "  Downloading py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.16.0 (from py7zr)\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.15.9 (from py7zr)\n",
      "  Downloading pyzstd-0.15.10-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n",
      "  Downloading pyppmd-1.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.7 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading inflate64-1.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr)\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: psutil in ./anaconda3/lib/python3.10/site-packages (from py7zr) (5.9.0)\n",
      "Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-macosx_10_9_universal2.whl (873 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m873.0/873.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-macosx_11_0_arm64.whl (35 kB)\n",
      "Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading pybcj-1.0.2-cp310-cp310-macosx_11_0_arm64.whl (23 kB)\n",
      "Downloading pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.15.10-cp310-cp310-macosx_11_0_arm64.whl (331 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
      "Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.10 texttable-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7932694",
   "metadata": {},
   "source": [
    "Problems encountered here:\n",
    "\n",
    "datasets was not upgraded ran the following code to fix it pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567296b",
   "metadata": {},
   "source": [
    "## 2 - Summarize Dialogue without Prompt Engineering\n",
    "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face transformers package can be found here.\n",
    "\n",
    "Let's upload some simple dialogues from the Samsum Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics.\n",
    "\n",
    "Changes: Changed the dialog dataset to samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19266ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset samsum/samsum to /Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cceb0d31b4644018b658a38c9777e05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samsum downloaded and prepared to /Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125a9481cbf047e09b57ecbe571e0ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"samsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148cbdf",
   "metadata": {},
   "source": [
    "Print a couple of dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da6595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \r\n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\r\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\r\n",
      "Nick: Why would you think that hon?\r\n",
      "Jane: Because I'm not that desperate.\r\n",
      "Nick: That was a bit below the belt.\r\n",
      "Nick: You're nice but you're not THAT hot.\r\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\r\n",
      "Nick: Actually I'll take it back. Forget about the drink.\r\n",
      "Nick: Forget I ever wrote to you.\r\n",
      "Jane: Bye loser!\r\n",
      "Nick: Fucking bitch!\r\n",
      "Jane: You're welcome!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "Jill: so how was your date anyway? :)\r\n",
      "Susan: it was perfect, he was so sweet!! <3\r\n",
      "Jill: tell me everything!!\r\n",
      "Susan: so first he picked me up from home, all dressed up and everything\r\n",
      "Jill: suit on? :D\r\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\r\n",
      "Jill: hahahahaha\r\n",
      "Susan: <file_gif>\r\n",
      "Jill: so where did he take you?\r\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\r\n",
      "Jill: rollerskating what? are you serious? :/\r\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\r\n",
      "Jill: if you say so...\r\n",
      "Susan: you are just jealous Jill :D\r\n",
      "Jill: I am not!!!!!!!\r\n",
      "Susan: yes you are :*\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [50, 400]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9253422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f790843c9a418ea6d8393477a359e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e0724e8294249b709841dd98f6356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49c914c0de4999bde927414e1f572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6285c",
   "metadata": {},
   "source": [
    "To handle encoding and decoding, it's necessary to work with text in a tokenized form. Tokenization involves breaking down texts into smaller pieces that can be processed by LLM models.\n",
    "\n",
    "You can download the tokenizer for the FLAN-T5 model by using the AutoTokenizer.from_pretrained() method. Enable the fast tokenizer by setting the use_fast parameter to True. While the specifics of this parameter are not crucial at this point, you can learn more about the tokenizer's parameters in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db00ef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2dceb58d9845ea837e828b931efca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bfdd441d0e4d159b0e603a2d6bc790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d0f97d8e9e44c8a0abbe07685f58dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1d762d6e0f4d18b519543cc75b196c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90365973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([1521,   39,    3, 3770,  376,  147, 8988,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "Are your bringing him over tonight\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Are your bringing him over tonight\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d29ac8",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c85c1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Susan's date was a sweet man. Susan and her date went to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45dc1b",
   "metadata": {},
   "source": [
    "# 3 - Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77eb32",
   "metadata": {},
   "source": [
    "## 3 - Summarize Dialogue with an Instruction Prompt\n",
    "3.1 - Zero Shot Inference with an Instruction Prompt\n",
    "In order to instruct the model to perform a task - summarize a dialogue - we can take the dialogue and convert it into an instruction prompt. This is often called zero shot inference. Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6d4f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd52c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dda21a",
   "metadata": {},
   "source": [
    "Observation: Even though the model is able to understand and summarize parts of the conversation, it still does not pick up on the nuance of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea652774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Write a short summary for the given conversation:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Write a short summary for the given conversation:\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Write a short summary for the given conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307260b3",
   "metadata": {},
   "source": [
    "3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n",
    "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks here. In the following code, we will use one of the pre-built FLAN-T5 prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f646f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\n",
      "Nick: Why would you think that hon?\n",
      "Jane: Because I'm not that desperate.\n",
      "Nick: That was a bit below the belt.\n",
      "Nick: You're nice but you're not THAT hot.\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\n",
      "Nick: Actually I'll take it back. Forget about the drink.\n",
      "Nick: Forget I ever wrote to you.\n",
      "Jane: Bye loser!\n",
      "Nick: Fucking bitch!\n",
      "Jane: You're welcome!\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Jill: so how was your date anyway? :)\n",
      "Susan: it was perfect, he was so sweet!! <3\n",
      "Jill: tell me everything!!\n",
      "Susan: so first he picked me up from home, all dressed up and everything\n",
      "Jill: suit on? :D\n",
      "Susan: nooo, not like that, but he had a really nice black shirt, elegant shoes, looked like Brad Pitt hahaha\n",
      "Jill: hahahahaha\n",
      "Susan: <file_gif>\n",
      "Jill: so where did he take you?\n",
      "Susan: that's the best part! We went to rollerskating disco!!!\n",
      "Jill: rollerskating what? are you serious? :/\n",
      "Susan: no no, listen - it was so awesome, I am tired of all this dull restaurant dates and stuff, it was actually something original\n",
      "Jill: if you say so...\n",
      "Susan: you are just jealous Jill :D\n",
      "Jill: I am not!!!!!!!\n",
      "Susan: yes you are :*\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Susan went on a date to the rollerskating disco and she enjoyed it. Jill doesn't find it exciting.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Susan's date picked her up from home and took her to rollerskating disco.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac7abc",
   "metadata": {},
   "source": [
    "# 4 - Summarize Dialogue with One Shot and Few Shot Inference\n",
    "One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task. .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302ba13",
   "metadata": {},
   "source": [
    "## 4.1 - One Shot Inference\n",
    "Let's build a function that takes a list of example_indices_full, generates a prompt with full examples, then at the end appends the prompt which we want the model to complete (example_index_to_summarize). We will use the FLAN-T5 prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34fa8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a7dee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Ryan: I have a bad feeling about this\r\n",
      "Ryan: <file_other>\r\n",
      "Sebastian: Ukraine...\r\n",
      "Sebastian: This russian circus will never end...\r\n",
      "Ryan: I hope the leaders of of nations will react somehow to this shit.\r\n",
      "Sebastian: I hope so too :(\n",
      "\n",
      "What was going on?\n",
      "Ryan and Sebastian are worried about the political situation in Ukraine.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Shaldona: WE ARE GONNA GET MARRIED ❤️❤️\n",
      "Shaldona: <file_others>\n",
      "Shaldona: This is our mobile inviation for our wedding.\n",
      "Shaldona: Invitation*\n",
      "Piper: Hey. You haven’t sent me any messages for a few years.\n",
      "Piper: And now you are sending me your wedding invitation \n",
      "Piper: THROUGH MESSENGER?\n",
      "Shaldona: .....\n",
      "Shaldona: Well..\n",
      "Shaldona: I had no enough time to meet everybody and give this in person.\n",
      "Shaldona: Hope you understand.\n",
      "Piper: If you don't have time to give the invitation card in person but expect people go to your wedding\n",
      "Piper: Shaldona, if so, you are too greedy.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [80]\n",
    "example_index_to_summarize = 250\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5cd1cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Shaldona sends mobile invitations to her wedding, as she has no time to give them in person.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "Shaldona and Piper are getting married. Shaldona hasn't sent Piper messages for a few years. Piper is worried about Shaldona's wedding invitation.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c6214",
   "metadata": {},
   "source": [
    "# 4.2 - Few Shot Inference\n",
    "Let's explore few shot inference by adding two more full dialogue-summary pairs to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c079eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Ali: I think I left my wallet at your place yesterday. Could you check? \r\n",
      "Mohammad: Give me a sec, I'll have a look around my room.\r\n",
      "Ali: OK.\r\n",
      "Mohammad: Found it!\r\n",
      "Ali: Phew, I don't know what I'd do if it wasn't there. Can you bring it to uni tomorrow?\r\n",
      "Mohammad: Sure thing.\n",
      "\n",
      "What was going on?\n",
      "Ali left his wallet at Mohammad's place. Mohammad'll bring it to uni tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Chris: Hi there! Where are you? Any chance of skyping?\r\n",
      "Rick: Hi! Our last two days in Cancun before flying to Havana. Yeah, skyping is an idea. When would it suit you?\r\n",
      "Rick: We don't have the best of connections in the room but I can get you pretty well in the lobby.\r\n",
      "Chris: What's the time in your place now?\r\n",
      "Rick: 6:45 pm\r\n",
      "Chris: It's a quarter to one in the morning here. Am still in front of the box.\r\n",
      "Rick: Gracious me! Sorry mate. You needn't have answered.\r\n",
      "Chris: 8-D\r\n",
      "Rick: Just tell me when we could skype.\r\n",
      "Chris: Preferably in the evening. Just a few hours earlier than now. And not tomorrow.\r\n",
      "Rick: Shute! Only tomorrow makes sense as there's no workable internet in Cuba.\r\n",
      "Chris: Could you make it like 3 pm your time?\r\n",
      "Rick: Sure.\r\n",
      "Chris: Perfect. So talk to you tomorrow.\r\n",
      "Chris: Give my love to Helen please.\r\n",
      "Rick: I will. Thx.\n",
      "\n",
      "What was going on?\n",
      "Rick and Helen are in Cancun. They're flying to Havana in two days. Chris and Rick will talk on Skype at 3 PM in Mexico.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Abdellilah: Where are you?\r\n",
      "Sam: work\r\n",
      "Abdellilah: What time you finish?\r\n",
      "Sam: Not til 5\r\n",
      "Abdellilah: Are your bringing him over tonight:\r\n",
      "Sam: No in the morning:\r\n",
      "Abdellilah: ok, what time?\r\n",
      "Sam: About 9. Is that ok?\r\n",
      "Abdellilah: ok - see you then\n",
      "\n",
      "What was going on?\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Debbie: Help, I don't know which dress to buy! <file_photo> or <file_photo>?\r\n",
      "Kelly: The red one! It's beautiful.\r\n",
      "Denise: It is, but the green one will suit you better.\r\n",
      "Kelly: Why? Debbie looks good in red.\r\n",
      "Denise: She does, but in my opinion that dress would look better on someone taller. Deb needs a shorter one.\r\n",
      "Kelly: Right, I haven't thought about it.\r\n",
      "Debbie: So the green one?\r\n",
      "Denise: Definitely!\r\n",
      "Kelly: Yeah. But can you send me the link to the store? I'm considering buying the red one for myself :D\r\n",
      "Debbie: LOL, okay. Here's the link: <file_other>\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [70, 100, 200]\n",
    "example_index_to_summarize = 260\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f634843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Debbie can't decide between buying a red dress and a green one. On Kelly and Denise's advice she will buy the green one. Kelly is considering buying the red one for herself.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Debbie is looking for a red dress. Kelly recommends the green dress. Kelly is considering buying the red one for herself.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74535f4",
   "metadata": {},
   "source": [
    "# In this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either.\n",
    "\n",
    "However, we can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall.\n",
    "\n",
    "Exercise:\n",
    "Experiment with the few shot inferencing.\n",
    "\n",
    "Choose different dialogues - change the indices in the example_indices_full list and example_index_to_summarize value.\n",
    "Change the number of shots. Be sure to stay within the model's 512 context length, however.\n",
    "How well does few shot inferencing work with other examples?\n",
    "\n",
    "Choosing various other dialogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0f1a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "Deirdre: Hi Beth, how are you love?\r\n",
      "Beth: Hi Auntie Deirdre, I'm been meaning to message you, had a favour to ask.\r\n",
      "Deirdre: Wondered if you had any thought about your Mum's 40th, we've got to do something special!\r\n",
      "Beth: How about a girls weekend, just mum, me, you and the girls, Kira will have to come back from Uni, of course.\r\n",
      "Deirdre: Sounds fab! Get your thinking cap on, it's only in 6 weeks! Bet she's dreading it, I remember doing that!\r\n",
      "Beth: Oh yeah, we had a surprise party for you, you nearly had a heart attack! \r\n",
      "Deirdre: Well, it was a lovely surprise! Gosh, thats nearly 4 years ago now, time flies! What was the favour, darling?\r\n",
      "Beth: Oh, it was just that I fancied trying a bit of work experience in the salon, auntie.\r\n",
      "Deirdre: Well, I am looking for Saturday girls, are you sure about it? you could do well in the exams and go on to college or 6th form.\r\n",
      "Beth: I know, but it's not for me, auntie, I am doing all foundation papers and I'm struggling with those.\r\n",
      "Deirdre: What about a tutor? Kira could help you in the hols.\r\n",
      "Beth: Maybe, but I'd like to try working. I'm 16 soon, I'm old enough.\r\n",
      "Deirdre: I know. Look, pop in tomorrow after school and we'll have a cuppa and a chat.\r\n",
      "Beth: Yes, thanks auntie. I'd really like to try the beauty therapy side.\r\n",
      "Deirdre: Its not for the squeamish, mind. Massage, pedicures, not to mention waxing!\r\n",
      "Beth: Oh yes, I was chatting to a friend about it yesterday!\r\n",
      "Deirdre: Maxine manages the beauty side, you can meet her tomorrow and we'll see how it goes.\r\n",
      "Beth: Yes, I'd really like that. \r\n",
      "Deirdre: We can try a few hours on a Saturday for a couple of weeks as work experience. I'll give you a tenner or so per session to start off for your lunch, coffee and bus fare etc. If you like, we'll take it from there.\r\n",
      "Beth: OK, I like the sound of it! See you tomorrow Auntie! Love you!\r\n",
      "Deirdre: Bye, lovely girl! Xx\n",
      "\n",
      "What was going on?\n",
      "Beth wants to organize a girls weekend to celebrate her mother's 40th birthday. She also wants to work at Deidre's beauty salon. Deidre offers her a few hours on Saturdays as work experience. They set up for a meeting tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Nick: You look absolutely gorgeous and have a lovely smile. \r\n",
      "Nick: Would love to get to know you a bit more. How about we meet up for a drink sometime?\r\n",
      "Jane: Hmmm... You're shooting a bit above your range aren't you?\r\n",
      "Nick: Why would you think that hon?\r\n",
      "Jane: Because I'm not that desperate.\r\n",
      "Nick: That was a bit below the belt.\r\n",
      "Nick: You're nice but you're not THAT hot.\r\n",
      "Jane: Oh is your poor little dick shriveling at the thought?\r\n",
      "Nick: Actually I'll take it back. Forget about the drink.\r\n",
      "Nick: Forget I ever wrote to you.\r\n",
      "Jane: Bye loser!\r\n",
      "Nick: Fucking bitch!\r\n",
      "Jane: You're welcome!\n",
      "\n",
      "What was going on?\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Ali: I think I left my wallet at your place yesterday. Could you check? \r\n",
      "Mohammad: Give me a sec, I'll have a look around my room.\r\n",
      "Ali: OK.\r\n",
      "Mohammad: Found it!\r\n",
      "Ali: Phew, I don't know what I'd do if it wasn't there. Can you bring it to uni tomorrow?\r\n",
      "Mohammad: Sure thing.\n",
      "\n",
      "What was going on?\n",
      "Ali left his wallet at Mohammad's place. Mohammad'll bring it to uni tomorrow.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Teacher: Rashi, why are you so low? \r\n",
      "Rashi: Ma’am I’m a bit confused about my career. \r\n",
      "Teacher: What is your confusion?\r\n",
      "Rashi: I was discussing with my friends about the career options. \r\n",
      "Teacher: Hmm.\r\n",
      "Rashi: There are too many to choose from.\r\n",
      "Teacher: Choose a career based on what truly interests you. \r\n",
      "Rashi: I have many that interests me. How does it determine the career?\r\n",
      "Teacher: The passion you have for what you do drives you to success. \r\n",
      "Rashi: But what about earnings?\r\n",
      "Teacher: Remember at some point of time one should learn to balance  between duties and success.\r\n",
      "Rashi: How do I do that?\r\n",
      "Teacher: Choose a career which interests you, get experienced and try to progress and widen the scope after a while.\r\n",
      "Rashi: Hmm, ok.\r\n",
      "Teacher: Something like earn and learn sort of..\r\n",
      "Rashi: You are so right. I will remember this.\r\n",
      "Teacher: So hope I managed to answer your questions.\r\n",
      "Rashi: Yes mam! Thank you very much! \r\n",
      "Teacher : You are most welcome, Rashi.\n",
      "\n",
      "What was going on?\n",
      "Rashi is confused by too many career choices. Teacher advises him to choose something he has passion for and what interests him.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Alexander: Personal request to send me message when you will be in taxi\r\n",
      "Alexander: If any problem, call me\r\n",
      "Tom: ;)\r\n",
      "Tom: Thank You, I appreciate it\r\n",
      "Alexander: Taxi confirmation below\r\n",
      "Alexander: <file_photo>\r\n",
      "Tom: Thank you for the transport, we arrived safely, although without luggages :/\r\n",
      "Alexander: Good but bad\r\n",
      "Tom: Yeeees\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [20, 50, 70, 110]\n",
    "example_index_to_summarize = 160\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553f1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom a message when he will be in taxi. Tom arrived safely without luggages.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56002f6f",
   "metadata": {},
   "source": [
    "# 5 - Configuring Generation Parameters for Inference\n",
    "You can alter the configuration parameters of the generate() method to vary the output of the large language model. Previously, you've primarily adjusted the max_new_tokens=50 parameter, which limits the number of tokens the model can generate.\n",
    "\n",
    "The GenerationConfig class is a useful way to organize these parameters.\n",
    "\n",
    "Exercise:\n",
    "Modify the configuration parameters to explore their impact on the output.\n",
    "\n",
    "By setting do_sample = True, you enable different decoding strategies that affect the selection of the next token from the probability distribution across the vocabulary. You can then fine-tune the output by adjusting parameters like temperature, top_k, and top_p.\n",
    "\n",
    "Uncomment the lines in the cell below and run the code again. Try to analyze the results. Below are some comments for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5b867d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom a message when he will be in taxi. Tom arrived safely without luggages.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef232ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will send Tom \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e7e36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Alexander will be in a taxi, has received the taxi confirmation below. He arrived safely not even without luggages.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Tom arrived safely, but without his luggage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f62fd",
   "metadata": {},
   "source": [
    "Comments related to the choice of the parameters in the code cell above:\n",
    "\n",
    "Choosing max_new_tokens=10 will make the output text too short, so the dialogue summary will be cut.\n",
    "Putting do_sample = True and changing the temperature value you get more flexibility in the output.\n",
    "As you can see, prompt engineering can take you a long way for this use case, but there are some limitations. Next, you will start to explore how you can use fine-tuning to help your LLM to understand a particular use case in better depth!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b899b",
   "metadata": {},
   "source": [
    "# Lab 2: Fine-Tune a Generative AI Model for Dialogue Summarization\n",
    "1 - Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "558dfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488355c3",
   "metadata": {},
   "source": [
    "1.1 - Load the model\n",
    "Changes: using smaller version of flan-t5 google/flan-t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eae1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name='google/flan-t5-small'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bc0a8",
   "metadata": {},
   "source": [
    "You can extract the number of model parameters and determine how many are trainable using the function below. At this stage, detailed understanding of the function is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6350679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16549468",
   "metadata": {},
   "source": [
    "# 1.2 - Evaluating the Model with Zero-Shot Inference\n",
    "Evaluate the model using zero-shot inference. You'll notice that the model has difficulty summarizing the dialogue as effectively as the baseline summary, but it still extracts some crucial information from the text. This suggests that the model has potential for further fine-tuning specific to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4cb5bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Linda: Hi Dad, I want to buy flowers for mum! But I don't remember which one she likes :(\r\n",
      "Michael: Well, she likes all the flowers I believe\r\n",
      "Linda: That doesn't help! I'm on a flower market right now!\r\n",
      "Michael: Send me some pics then\r\n",
      "Linda: <file_photo> \r\n",
      "Michael: Tulips are nice, roses too\r\n",
      "Linda:  What about carnations?\r\n",
      "Michael: No, carnations are boring :D\r\n",
      "Linda: Thanks Dad, srsly…\r\n",
      "Michael:  What about freesias? She likes them a lot, are there any there?\r\n",
      "Linda: <file_photo> \r\n",
      "Michael: Take those!\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Linda wants to buy flowers for her mother and asks Michael which flowers does she like. Michael suggests Linda to buy freesias.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Linda wants to buy flowers for mum. She's on a flower market. Michael sends her some pictures.\n"
     ]
    }
   ],
   "source": [
    "index = 800\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add4eb0",
   "metadata": {},
   "source": [
    "# 2 - Perform Full Fine-Tuning\n",
    "2.1 - Preprocess the Dialog-Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d3a760a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90431b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a242576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (148, 2)\n",
      "Validation: (9, 2)\n",
      "Test: (9, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 148\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b8889",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de3c04",
   "metadata": {},
   "source": [
    "## 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
    "Now utilize the built-in Hugging Face Trainer class (see the documentation here). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.\n",
    "\n",
    "Changes: Training a fully fine-tuned version of the model would take a few hours on a GPU. Instead we download a pre-fine-tuned model mrm8488/flan-t5-small-finetuned-samsum to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the instruct model in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4cf79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_name=\"mrm8488/flan-t5-small-finetuned-samsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3203c88",
   "metadata": {},
   "source": [
    "Create an instance of the AutoModelForSeq2SeqLM class for the instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e03070dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d2f1f260374d94bd3bccb1add0df6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f649ceb1e4794cd7b0e0192657634fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f9691",
   "metadata": {},
   "source": [
    "## 2.3 - Qualitative Evaluation of the Model (Human Assessment)\n",
    "In many generative AI applications, beginning with a qualitative evaluation by asking, \"Is my model performing as expected?\" is often an effective approach. In the example below (the same one we opened this notebook with), observe how the fine-tuned model now generates a reasonable summary of the dialogue, showing a marked improvement over its initial failure to comprehend the task required of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c440d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Nick finds Jane pretty and invites her for a drink to get to know her better. Jane rejects Nick and is unpleasant to him. Nick suggests Jane to forget about their conversation.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Nick and Jane are going to meet for a drink.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Nick and Jane are going to meet up for a drink.\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049083f",
   "metadata": {},
   "source": [
    "## 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The ROUGE metric is used to assess the accuracy of summaries generated by models by comparing them to a \"baseline\" summary typically crafted by a human. Although it isn't flawless, this metric serves as an indicator of the improvement in summarization effectiveness achieved through fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d82364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906869546abf4083b5f9e9224bbf570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a04572",
   "metadata": {},
   "source": [
    "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c08b539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "      <td>Betty called Larry last time they were at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "      <td>Eric and Rob are watching a show on YouTube.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "      <td>Bob will send Lenny photos of the trousers. Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "      <td>Emma will pick Will up at the moment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "      <td>Jane is in Warsaw. Ollie will bring some sun w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hilary has the keys to the apartment. Benjamin...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "      <td>Benjamin, Hilary and Daniel are meeting for dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Payton provides Max with websites selling clot...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "      <td>Payton is looking for clothes to buy. Max will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rita and Tina are bored at work and have still...</td>\n",
       "      <td>Rita is tired and is not able to concentrate a...</td>\n",
       "      <td>Rita is tired and is tired. Tina is tired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beatrice wants to buy Leo a scarf, but he does...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "      <td>Beatrice is in town. She doesn't have a scarf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric doesn't know if his parents let him go to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "      <td>Eric is coming to the wedding. He has a lot to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Hannah needs Betty's number but Amanda doesn't...   \n",
       "1  Eric and Rob are going to watch a stand-up on ...   \n",
       "2  Lenny can't decide which trousers to buy. Bob ...   \n",
       "3  Emma will be home soon and she will let Will k...   \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....   \n",
       "5  Hilary has the keys to the apartment. Benjamin...   \n",
       "6  Payton provides Max with websites selling clot...   \n",
       "7  Rita and Tina are bored at work and have still...   \n",
       "8  Beatrice wants to buy Leo a scarf, but he does...   \n",
       "9  Eric doesn't know if his parents let him go to...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Amanda can't find Betty's number. Amanda will ...   \n",
       "1  Eric and Rob are watching a stand-up. Eric and...   \n",
       "2  Lenny wants to buy two pairs of purple trouser...   \n",
       "3     Emma will be home soon. Will will pick her up.   \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...   \n",
       "5  Hilary and Elliot are meeting at the conferenc...   \n",
       "6  Payton likes shopping but he doesn't always bu...   \n",
       "7  Rita is tired and is not able to concentrate a...   \n",
       "8  Beatrice is in town, shopping. She has a scarf...   \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  Betty called Larry last time they were at the ...  \n",
       "1       Eric and Rob are watching a show on YouTube.  \n",
       "2  Bob will send Lenny photos of the trousers. Le...  \n",
       "3              Emma will pick Will up at the moment.  \n",
       "4  Jane is in Warsaw. Ollie will bring some sun w...  \n",
       "5  Benjamin, Hilary and Daniel are meeting for dr...  \n",
       "6  Payton is looking for clothes to buy. Max will...  \n",
       "7         Rita is tired and is tired. Tina is tired.  \n",
       "8  Beatrice is in town. She doesn't have a scarf....  \n",
       "9  Eric is coming to the wedding. He has a lot to...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa782e",
   "metadata": {},
   "source": [
    "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45f8a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.47028472286610057, 'rouge2': 0.22995235132837163, 'rougeL': 0.37927486414974926, 'rougeLsum': 0.3802753543727778}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.36376015671847006, 'rouge2': 0.1298066050954753, 'rougeL': 0.2917625018054684, 'rougeLsum': 0.29182423684675857}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0bdc9",
   "metadata": {},
   "source": [
    "Rouge scores of this model are bad, even worse than our regular model. Let's move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad65672",
   "metadata": {},
   "source": [
    "The file data/dialogue-summary-training-results.csv contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aa3940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334267762606164, 'rouge2': 0.07583872163969117, 'rougeL': 0.20145533544294464, 'rougeLsum': 0.2013454634200133}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42157999366320953, 'rouge2': 0.18024457353812656, 'rougeL': 0.3383623425777854, 'rougeLsum': 0.3382783013380308}\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"/Users/shivanisahu/Desktop/ADV_DS_Assignment/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a27b8",
   "metadata": {},
   "source": [
    "The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3bafdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 18.82%\n",
      "rouge2: 10.44%\n",
      "rougeL: 13.69%\n",
      "rougeLsum: 13.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec34f5b",
   "metadata": {},
   "source": [
    "# 3 - Implementing Parameter Efficient Fine-Tuning (PEFT)\n",
    "Next, we will explore Parameter Efficient Fine-Tuning (PEFT), an alternative to the \"full fine-tuning\" method previously used. PEFT is a more resource-efficient approach to fine-tuning that typically yields results comparable to full fine-tuning.\n",
    "\n",
    "PEFT encompasses techniques such as Low-Rank Adaptation (LoRA) and prompt tuning (distinct from prompt engineering). Often, PEFT specifically refers to LoRA. LoRA allows for fine-tuning a model using significantly fewer computational resources, sometimes only requiring a single GPU. After fine-tuning for a particular task, use case, or client, LoRA modifies only a small component of the model, known as the \"LoRA adapter,\" which is significantly smaller in size than the full model—often just a fraction of the original size in megabytes as opposed to gigabytes.\n",
    "\n",
    "During inference, this LoRA adapter must be integrated back with the original large language model (LLM) to process requests. This integration allows the original LLM to be reused with multiple LoRA adapters, thus optimizing memory usage when managing various tasks and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36fa6e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cda1a8f7a244836b42662985dbb160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c074053ee0541c0b50a8a9a91bf33ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       \"RohitKeswani/flan-t5-base-peft-samsum\",\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147bed6",
   "metadata": {},
   "source": [
    "The number of trainable parameters will be 0 due to is_trainable=False setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "deabb554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617f9bd",
   "metadata": {},
   "source": [
    "## 3.2 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "Make inferences with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae2b9b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Sam is working at 9. Sam will bring him over tonight.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "# baseline_human_summary = dataset['test'][index]['summary']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328fdd6",
   "metadata": {},
   "source": [
    "## 3.3 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab280f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "      <td>Betty called Larry last time they were at the ...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "      <td>Eric and Rob are watching a show on YouTube.</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "      <td>Bob will send Lenny photos of the trousers. Le...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "      <td>Emma will pick Will up at the moment.</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "      <td>Jane is in Warsaw. Ollie will bring some sun w...</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hilary has the keys to the apartment. Benjamin...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "      <td>Benjamin, Hilary and Daniel are meeting for dr...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Payton provides Max with websites selling clot...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "      <td>Payton is looking for clothes to buy. Max will...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rita and Tina are bored at work and have still...</td>\n",
       "      <td>Rita is tired and is not able to concentrate a...</td>\n",
       "      <td>Rita is tired and is tired. Tina is tired.</td>\n",
       "      <td>Rita is tired and is not able to concentrate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beatrice wants to buy Leo a scarf, but he does...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "      <td>Beatrice is in town. She doesn't have a scarf....</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric doesn't know if his parents let him go to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "      <td>Eric is coming to the wedding. He has a lot to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Hannah needs Betty's number but Amanda doesn't...   \n",
       "1  Eric and Rob are going to watch a stand-up on ...   \n",
       "2  Lenny can't decide which trousers to buy. Bob ...   \n",
       "3  Emma will be home soon and she will let Will k...   \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....   \n",
       "5  Hilary has the keys to the apartment. Benjamin...   \n",
       "6  Payton provides Max with websites selling clot...   \n",
       "7  Rita and Tina are bored at work and have still...   \n",
       "8  Beatrice wants to buy Leo a scarf, but he does...   \n",
       "9  Eric doesn't know if his parents let him go to...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Amanda can't find Betty's number. Amanda will ...   \n",
       "1  Eric and Rob are watching a stand-up. Eric and...   \n",
       "2  Lenny wants to buy two pairs of purple trouser...   \n",
       "3     Emma will be home soon. Will will pick her up.   \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...   \n",
       "5  Hilary and Elliot are meeting at the conferenc...   \n",
       "6  Payton likes shopping but he doesn't always bu...   \n",
       "7  Rita is tired and is not able to concentrate a...   \n",
       "8  Beatrice is in town, shopping. She has a scarf...   \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  Betty called Larry last time they were at the ...   \n",
       "1       Eric and Rob are watching a show on YouTube.   \n",
       "2  Bob will send Lenny photos of the trousers. Le...   \n",
       "3              Emma will pick Will up at the moment.   \n",
       "4  Jane is in Warsaw. Ollie will bring some sun w...   \n",
       "5  Benjamin, Hilary and Daniel are meeting for dr...   \n",
       "6  Payton is looking for clothes to buy. Max will...   \n",
       "7         Rita is tired and is tired. Tina is tired.   \n",
       "8  Beatrice is in town. She doesn't have a scarf....   \n",
       "9  Eric is coming to the wedding. He has a lot to...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  Amanda can't find Betty's number. Amanda will ...  \n",
       "1  Eric and Rob are watching a stand-up. Eric and...  \n",
       "2  Lenny wants to buy two pairs of purple trouser...  \n",
       "3     Emma will be home soon. Will will pick her up.  \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...  \n",
       "5  Hilary and Elliot are meeting at the conferenc...  \n",
       "6  Payton likes shopping but he doesn't always bu...  \n",
       "7  Rita is tired and is not able to concentrate a...  \n",
       "8  Beatrice is in town, shopping. She has a scarf...  \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4e46a",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b981189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.47028472286610057, 'rouge2': 0.22995235132837163, 'rougeL': 0.37927486414974926, 'rougeLsum': 0.3802753543727778}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.36376015671847006, 'rouge2': 0.1298066050954753, 'rougeL': 0.2917625018054684, 'rougeLsum': 0.29182423684675857}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.47753687688771096, 'rouge2': 0.23028476049778054, 'rougeL': 0.3773752395014854, 'rougeLsum': 0.37961260085429527}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2fb83",
   "metadata": {},
   "source": [
    "Notice, that PEFT model performed a little bit better than flan-t5-base.\n",
    "\n",
    "We already computed ROUGE score on the full dataset, after loading the results from the data/dialogue-summary-training-results.csv file. Load the values for the PEFT model now and check its performance compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b28a969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334267762606164, 'rouge2': 0.07583872163969117, 'rougeL': 0.20145533544294464, 'rougeLsum': 0.2013454634200133}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42157999366320953, 'rouge2': 0.18024457353812656, 'rougeL': 0.3383623425777854, 'rougeLsum': 0.3382783013380308}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.4080553198406258, 'rouge2': 0.16332717404983593, 'rougeL': 0.3251568978594342, 'rougeLsum': 0.32488871719602286}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af4b01",
   "metadata": {},
   "source": [
    "Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9f35b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 17.46%\n",
      "rouge2: 8.75%\n",
      "rougeL: 12.37%\n",
      "rougeLsum: 12.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe9ad0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.35%\n",
      "rouge2: -1.69%\n",
      "rougeL: -1.32%\n",
      "rougeLsum: -1.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbc46c",
   "metadata": {},
   "source": [
    "Here we see a small percentage decrease in the ROUGE metrics vs. full fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2885771",
   "metadata": {},
   "source": [
    "# Lab 3 - Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cae56",
   "metadata": {},
   "source": [
    "1 - Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de717f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524856fd",
   "metadata": {},
   "source": [
    "2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator\n",
    "2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction\n",
    "You will keep working with the same Hugging Face dataset samsum and the pre-trained model FLAN-T5-BASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1464e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee94dbc9f4b41e9b7007c4b8e5f7a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"samsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e2cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0b8889b284d688f26f37e4234b263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "\n",
    "dataset_original = load_dataset(\"samsum\")\n",
    "\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da7f3c",
   "metadata": {},
   "source": [
    "The next step will be to preprocess the dataset. We will take only a part of it, then filter the dialogues of a particular length (just to make those examples long enough and, at the same time, easy to read). Then wrap each dialogue with the instruction and tokenize the prompts. Save the token ids in the field input_ids and decoded version of the prompts in the field query.\n",
    "\n",
    "We could do that all step by step in the cell below, but it is a good habit to organize that all in a function build_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9df0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n",
      "Loading cached processed dataset at /Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a2c1ba800a8fbe71.arrow\n",
      "Loading cached processed dataset at /Users/shivanisahu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-c0c08df2e22f6f24.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'query'],\n",
      "        num_rows: 7851\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'query'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length,\n",
    "                  input_max_text_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Tokenizer model name.\n",
    "    - dataset_name (str): Name of the dataset to load.\n",
    "    - input_min_text_length (int): Minimum length of the dialogues.\n",
    "    - input_max_text_length (int): Maximum length of the dialogues.\n",
    "\n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # load dataset (only \"train\" part will be enough for this lab).\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    def tokenize(sample):\n",
    "\n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "\n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "\n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200,\n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd51594",
   "metadata": {},
   "source": [
    "Prepare a function to pull out the number of model parameters (it is the same as in the previous lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc8e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4451988",
   "metadata": {},
   "source": [
    "Add the adapter to the original FLAN-T5 model. In the previous lab you were adding the fully trained adapter only for inferences, so there was no need to pass LoRA configurations doing that. Now you need to pass them to the constructed PEFT model, also putting is_trainable=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e64bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       'RohitKeswani/flan-t5-base-peft-samsum',\n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       device_map=\"auto\",\n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c43af6",
   "metadata": {},
   "source": [
    "In this lab, you are preparing to fine-tune the LLM using Reinforcement Learning (RL). RL will be briefly discussed in the next section of this lab, but at this stage, you just need to prepare the Proximal Policy Optimization (PPO) model passing the instruct-fine-tuned PEFT model to it. PPO will be used to optimize the RL policy against the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d50fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0fd72",
   "metadata": {},
   "source": [
    "During PPO, only a few parameters will be updated. Specifically, the parameters of the ValueHead. More information about this class of models can be found in the documentation. The number of trainable parameters can be computed as \n",
    ", where \n",
    " is the number of input units (here \n",
    ") and \n",
    " is the number of output units (\n",
    "). The \n",
    " term in the equation takes into account the bias term.\n",
    "\n",
    "Now create a frozen copy of the PPO which will not be fine-tuned - a reference model. The reference model will represent the LLM before detoxification. None of the parameters of the reference model will be updated during PPO training. This is on purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54a79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4707243",
   "metadata": {},
   "source": [
    "## 2.2 - Setting Up the Reward Model\n",
    "Reinforcement Learning (RL) is a branch of machine learning where agents perform actions in an environment to maximize cumulative rewards. The behavior of these agents is directed by a policy, and the primary aim of RL is for the agent to develop an optimal or near-optimal policy that maximizes the reward function.\n",
    "\n",
    "Previously, the original policy was based on the instruct PEFT model, which is the LLM before undergoing detoxification. Although human labelers could provide feedback on the toxicity levels of the outputs, relying on them throughout the fine-tuning process can be costly. An effective alternative is to employ a reward model that encourages the agent to produce less toxic dialogue summaries. A straightforward strategy would be to utilize sentiment analysis, categorizing outputs into two classes—nothate and hate—and assigning higher rewards for outputs more likely to be classified as nothate.\n",
    "\n",
    "For this purpose, you will utilize Meta AI’s RoBERTa-based model for detecting hate speech. This model will produce logits, which are then used to calculate probabilities for the two classes: nothate and hate. The logits corresponding to nothate will be considered as positive rewards. The model will subsequently undergo fine-tuning using these reward values with Proximal Policy Optimization (PPO).\n",
    "\n",
    "To begin, we need to instantiate the necessary RoBERTa model class and load a tokenizer to evaluate the model. In this setup, label 0 represents the class nothate, and label 1 corresponds to the class hate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a445b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8a7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU if GPU is not available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Move the model to the device\n",
    "toxicity_model = toxicity_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c37e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU if GPU is not available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Move the model to the device\n",
    "toxicity_model = toxicity_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cddd2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.1140999794006348, -2.489616870880127]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.003670621896162629]\n",
      "reward (high): [3.1140999794006348]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0932dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities [not hate, hate]: [0.2564719319343567, 0.7435280084609985]\n",
      "reward (low): [-0.6921163201332092]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "# print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (low): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c440f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model,\n",
    "                      toxicity_evaluator,\n",
    "                      tokenizer,\n",
    "                      dataset,\n",
    "                      num_samples):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model (trl model): Model to be evaluated.\n",
    "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
    "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
    "    - dataset (dataset): Input dataset for the evaluation.\n",
    "    - num_samples (int): Maximum number of samples for the evaluation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two numpy.float64 values:\n",
    "    - mean (numpy.float64): Mean of the samples toxicity.\n",
    "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "\n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8192d79",
   "metadata": {},
   "source": [
    "# 3 - Perform Fine-Tuning to Detoxify the Summaries\n",
    "Optimize a RL policy against the reward model using Proximal Policy Optimization (PPO).\n",
    "\n",
    "3.1 - Initialize PPOTrainer\n",
    "For the PPOTrainer initialization, we will need a collator. Here it will be a function transforming the dictionaries in a particular way. We can define and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53e522a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3bf6e",
   "metadata": {},
   "source": [
    "Set up the configuration parameters. Load the ppo_model and the tokenizer. We will also load a frozen version of the model ref_model. The first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This works as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde33a8",
   "metadata": {},
   "source": [
    "## 3.2 - Fine-Tune the Model\n",
    "The fine-tuning loop consists of the following main steps:\n",
    "\n",
    "Get the query responses from the policy LLM (PEFT model).\n",
    "Get sentiments for query/responses from hate speech RoBERTa model.\n",
    "Optimize policy with PPO using the (query, response, reward) triplet.\n",
    "The operation is running if you see the following metrics appearing:\n",
    "\n",
    "objective/kl: minimize kl divergence,\n",
    "ppo/returns/mean: maximize mean returns,\n",
    "ppo/policy/advantages_mean: maximize advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "\n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c4c4b",
   "metadata": {},
   "source": [
    "## 3.3 - Evaluate the Model Qualitatively\n",
    "Let's inspect some examples from the test dataset. We can compare the original ref_model to the fine-tuned/detoxified ppo_model using the toxicity evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "\n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e7851",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Throughout the labs, we explored the implementation of zero-shot, one-shot, and few-shot inference, observing how these methods impact model output. We also delved into prompt engineering, using various Google models as foundational models to enhance our understanding of in-context learning. Additionally, we worked with instruct models, also known as instruction fine-tuned models, and assessed their performance both manually and quantitatively using ROUGE scores. We further explored Parameter Efficient Fine-Tuning (PEFT) models, noting the variations in summarization outputs. Our experiments with reinforcement learning for detoxifying summaries by utilizing feedback and rewards allowed us to qualitatively measure the improvement in outputs.\n",
    "\n",
    "In summary, the labs provided an extensive overview of advanced techniques for enhancing the capabilities of language models. From prompt engineering and instruction fine-tuning to PEFT and reinforcement learning, these sessions equipped us with a diverse set of tools to customize and optimize language models for specific applications, highlighting the broad applicability and adaptability of contemporary natural language processing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff68cd",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **Generative AI with Large Language Models**: [https://www.coursera.org/learn/generative-ai-with-llms](https://www.coursera.org/learn/generative-ai-with-llms)\n",
    "- **SAMSum Dataset**: [https://huggingface.co/datasets/samsum](https://huggingface.co/datasets/samsum)\n",
    "- **RohitKeswani/flan-t5-base-peft-samsum**: [https://huggingface.co/RohitKeswani/flan-t5-base-peft-samsum](https://huggingface.co/RohitKeswani/flan-t5-base-peft-samsum)\n",
    "- **google/flan-t5-small**: [https://huggingface.co/google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "- **google/flan-t5-base**: [https://huggingface.co/google/flan-t5-base](https://huggingface.co/google/flan-t5-base)\n",
    "- **mrm8488/flan-t5-small-finetuned-samsum**: [https://huggingface.co/mrm8488/flan-t5-small-finetuned-samsum](https://huggingface.co/mrm8488/flan-t5-small-finetuned-samsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01927e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
