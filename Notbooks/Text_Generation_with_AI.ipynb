{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh5xmAakV8E74Tu8vu8mn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikbearbrown/INFO_7390_Art_and_Science_of_Data/blob/main/TA_Contest_Fall_23/ShardulChavan/Week8-Generating-Text/Text_Generation_with_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Libraries"
      ],
      "metadata": {
        "id": "YwbwYonMmClo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_07SYtb-gQpa",
        "outputId": "0d40e604-5945-456d-dd0a-afef1459cd2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mSpfQ9V_mJ8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries\n",
        "The library **transformers** has pre-trained AI models such as GPT2LMHeadModel, GPT2Tokenizer for various language tasks  \n",
        "### Loading Pre-trained Model and Tokenizer:\n",
        "The code loads a pre-trained AI model called \"gpt2\" and a tokenizer. The model is like a smart writer, and the tokenizer helps the model understand and process text.\n",
        "\n",
        "###Preparing the Model:\n",
        "The model is set to \"generate text\" mode using model.eval(). It's like telling the model to start writing.\n",
        "\n",
        "###Setting Up the Prompt:\n",
        "The code provides a starting sentence called a \"prompt.\" The model will use this to continue generating text.\n",
        "\n",
        "###Encoding the Prompt:\n",
        "The starting sentence is turned into numbers that the model can understand. This is done using the tokenizer, which converts words into codes.\n",
        "\n",
        "### Generating Text:\n",
        "The model starts generating text based on the prompt. The generate function is used, which takes the encoded prompt and generates new text. It follows a strategy called \"temperature\" (0.5 in this case) to control how random the text should be.\n",
        "\n",
        "### Decoding results:\n",
        "The generated text is in numbers, so it's decoded back into words using the tokenizer. The resulting text is printed, which is the AI's creative writing based on the prompt.\n",
        "\n",
        "### Removing Repeated Phrases:\n",
        "To make the generated text more interesting, a post-processing step removes repeated phrases. This helps the text sound more natural and varied.\n",
        "\n",
        "### Final Output:\n",
        "The code prints the final generated text that the AI created based on the given prompt."
      ],
      "metadata": {
        "id": "V3ogYomWnWJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the model for generating text\n",
        "model.eval()\n",
        "\n",
        "# Prompt for text generation\n",
        "prompt = \"In a world where technology advanced beyond imagination\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "temperature = 0.5  # temperature for randomness of the text\n",
        "max_length = 100\n",
        "attention_mask = input_ids.new_ones(input_ids.shape)\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=pad_token_id, max_length=max_length, num_return_sequences=1, temperature=temperature)\n",
        "\n",
        "# Decode and print generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply post-processing to remove repeated phrases\n",
        "def remove_repeated_phrases(text):\n",
        "    phrases = text.split(\". \")\n",
        "    unique_phrases = []\n",
        "    for phrase in phrases:\n",
        "        if phrase not in unique_phrases:\n",
        "            unique_phrases.append(phrase)\n",
        "    return \". \".join(unique_phrases)\n",
        "\n",
        "generated_text = remove_repeated_phrases(generated_text)\n",
        "\n",
        "# Final Output\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LWUuSgeMx70",
        "outputId": "5cdfffbc-eb64-4e1f-e6ed-ea4bfba75cf0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "In a world where technology advanced beyond imagination, the world of the future is a world of possibilities.\n",
            "\n",
            "The future is not a future where we can make a living, but a future where we can make a living.\n",
            "\n",
            "The future is a world where we can make a living.\n",
            "\n",
            "The future is a world where we can make a living.\n",
            "\n",
            "The future is a world where we can make a living.\n",
            "\n",
            "The future is a world where we can make a living\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the model seems to get stuck in a loop where it repeats the same phrases over and over again, resulting in a lack of meaningful variation in the text.\n",
        "\n",
        "## What's Wrong:\n",
        "\n",
        "**Repetitive Phrasing:** The phrases \"The future is a world where we can make a living\" and similar variations are repeated multiple times, creating a repetitive and uninteresting pattern.\n",
        "\n",
        "**Lack of Meaningful Variation:** The text lacks meaningful variation and progression. The repetition doesn't contribute to the development of ideas or narrative.\n",
        "\n",
        "**Contextual Disconnection:** The text appears to lose its context as it progresses, causing the repetition to become more noticeable and disconnected from the initial theme.\n",
        "\n",
        "## What could be the possible causes:\n",
        "\n",
        "**Seed Phrase Influence:** The initial prompt might be influencing the repetitive output, especially if the training data contained similar patterns.\n",
        "\n",
        "**Temperature Setting:** The temperature parameter might be set low, making the model overly deterministic and leading to repeated choices.\n",
        "\n",
        "**Limited Training Data Variation:** If the training data lacks diverse and creative examples, the model might struggle to generate unique content. (But this couldn't be the reason in our case)\n",
        "\n",
        "## How to Improve:\n",
        "\n",
        "**Adjust Temperature**: We could try to experiment with a higher temperature setting to introduce more randomness into the text generation process.\n",
        "\n",
        "**Vary Prompts**: Also trying different prompts that encourage the model to explore various directions and concepts could be an option.\n",
        "\n",
        "**Larger Beam Size:** Moreover we can consider using a diverse beam search method to reduce repetition and set larger beam size during generation to encourage more varied outputs.\n",
        "\n",
        "Few additional approaches we can consider to further reduce repetition in generated text are:\n",
        "\n",
        "1. N-gram Tracking: Keep track of generated n-grams (subsequences of length n) and use them to guide the generation process. Avoid generating n-grams that have appeared recently to promote diversity.\n",
        "\n",
        "2. Denoising Autoencoders: Train a denoising autoencoder to recognize and penalize repeated phrases during text generation.\n",
        "\n",
        "3. Sampling Strategy: Experiment with different sampling strategies (e.g., top-k, top-p sampling) to influence the distribution of generated tokens.\n",
        "\n",
        "4. Custom Decoding Algorithm: Develop a custom decoding algorithm that incorporates uniqueness and diversity as objectives.\n"
      ],
      "metadata": {
        "id": "Djey0IhJrkgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this below snippet code compared to the previous one we have utilized the Diverse Beam Search. In the previous code, we used a simpler approach of temperature-controlled generation to produce a single text. While the temperature-controlled generation can introduce randomness, it doesn't explicitly tackle the issue of repetitiveness.\n",
        "\n",
        "By incorporating Diverse Beam Search, we've addressed the problem of repetition more effectively. The no_repeat_ngram_size parameter helps avoid generating sequences with repeated n-grams, which leads to a richer and more diverse set of generated texts. This results in a better overall output quality by reducing monotony and enhancing creativity in the generated text."
      ],
      "metadata": {
        "id": "_C23aT7CyrYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model_GPT2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_GPT2.eval()\n",
        "\n",
        "# Prompt for text generation\n",
        "prompt = \"In a quaint village, a curious child discovered an ancient map\"\n",
        "\n",
        "# Encode the prompt\n",
        "prompt_input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Diverse Beam Search parameters\n",
        "beam_size = 5\n",
        "max_length = 100\n",
        "attention_mask = prompt_input_ids.new_ones(prompt_input_ids.shape)\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "# Generate diverse text using beam search\n",
        "output_sequences = model_GPT2.generate(\n",
        "    prompt_input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    pad_token_id=pad_token_id,\n",
        "    max_length=max_length,\n",
        "    num_return_sequences=beam_size,\n",
        "    num_beams=beam_size,\n",
        "    no_repeat_ngram_size=3  # Avoid repeated n-grams\n",
        ")\n",
        "\n",
        "# Decode and print generated text\n",
        "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in output_sequences]\n",
        "\n",
        "print(\"Generated Texts:\")\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"{i+1}. {text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha969gmvZJXT",
        "outputId": "1c3a2f57-121f-4baa-e2d4-36f67d5f3b62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Texts:\n",
            "1. In a quaint village, a curious child discovered an ancient map of the world.\n",
            "\n",
            "It was a map of a world that had been lost for thousands of years. It was the only one of its kind in the world, and the only map that had ever been found in the history of the universe. The map had been discovered by a group of explorers who had traveled to the distant past to find the lost map. The group had found the map, but it had never been found.\n",
            "\n",
            "2. In a quaint village, a curious child discovered an ancient map of the world.\n",
            "\n",
            "It was a map of a world that had been lost for thousands of years. It was the only one of its kind in the world, and the only map that had ever been found in the history of the universe. The map had been discovered by a group of explorers who had traveled to the distant past to find the lost map. The group had found the map, but it had never been found. The\n",
            "3. In a quaint village, a curious child discovered an ancient map of the world.\n",
            "\n",
            "It was a map of a world that had been lost for thousands of years. It was the only one of its kind in the world, and the only map that had ever been found in the history of the universe. The map had been discovered by a group of explorers who had traveled to the distant past to find the lost map. The group had found the map, but it had never been seen by anyone\n",
            "4. In a quaint village, a curious child discovered an ancient map of the world.\n",
            "\n",
            "It was a map of a world that had been lost for thousands of years. It was the only one of its kind in the world, and the only map that had ever been found in the history of the universe. The map had been discovered by a group of explorers who had traveled to the distant past to find the lost map. The group had found the map, but it had not been found by the\n",
            "5. In a quaint village, a curious child discovered an ancient map of the world.\n",
            "\n",
            "It was a map of a world that had been lost for thousands of years. It was the only one of its kind in the world, and the only map that had ever been found in the history of the universe. The map had been discovered by a group of explorers who had traveled to the distant past to find the lost map. The group had found the map, but it had never been seen by the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After experimenting with Temperature Setting + Prompt Variation, Temperature Setting + Post-Processing and repetition reduction technique, let's try Text Priming + Model Size in which we start the generation with a complete sentence to establish context and use a larger model variant for improved coherence and depth.\n",
        "\n",
        "Moreover by using a complete sentence as the priming text, the model is more likely to produce coherent and relevant text that extends from that initial context. Additionally, the use of a larger model (gpt2-large) and longer max_length helps in generating more detailed and diverse text."
      ],
      "metadata": {
        "id": "DBaqRQnKz0qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load a larger pre-trained model and tokenizer\n",
        "model_name = \"gpt2-large\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to generate text\n",
        "model.eval()\n",
        "\n",
        "# Text priming with a complete sentence\n",
        "priming_text = \"In a world where technology advanced beyond imagination\"\n",
        "\n",
        "# Encode the priming text\n",
        "priming_input_ids = tokenizer.encode(priming_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with text priming\n",
        "temperature=0.2\n",
        "max_length = 150  # Longer length to provide depth to the generated content\n",
        "attention_mask = priming_input_ids.new_ones(priming_input_ids.shape)\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "output = model.generate(priming_input_ids, attention_mask=attention_mask, pad_token_id=pad_token_id, max_length=max_length, num_return_sequences=1, temperature=temperature)\n",
        "\n",
        "\n",
        "# Decode and print generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2oNPvR9M3A5",
        "outputId": "1835d8c9-b59b-44a1-a5ff-5f02b6a8edf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "In a world where technology advanced beyond imagination, the idea of a \"living\" computer was a fantasy.\n",
            "\n",
            "But in the early 1990s, the idea of a computer that could be \"living\" was a reality.\n",
            "\n",
            "The first computer to be \"living\" was the IBM PC.\n",
            "\n",
            "The PC was a computer that could be \"living\" by using a \"living\" operating system.\n",
            "\n",
            "The operating system was called Windows.\n",
            "\n",
            "Windows was a computer operating system that was based on the UNIX operating system.\n",
            "\n",
            "The UNIX operating system was a Unix-like operating system that was based on the UNIX philosophy of \"do one thing, do it well\".\n",
            "\n",
            "The UNIX philosophy was based on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, the combination of a larger model, text priming, longer length, controlled randomness, avoidance of repeated n-grams, and advanced language patterns collectively contributed to the generation of better results.\n",
        "\n",
        "**Using a Larger Pre-trained Model (gpt2-large):** Switching to a larger pre-trained model, such as gpt2-large, often leads to improved text generation. Larger models have more parameters and capacity to capture intricate patterns in the input data, resulting in more coherent and contextually relevant outputs.\n",
        "\n",
        "**Text Priming: **By providing an initial context (\"In a world where technology advanced beyond imagination\"), you guide the model's generated text to follow a coherent direction from the given starting point. This helps the model stay on topic and produce text that logically extends from the priming text.\n",
        "\n",
        "**Longer Maximum Length:** Increasing the max_length parameter to 150 allows the model to generate more extended and detailed text. This additional length enables the model to provide more context and depth to the generated content.\n",
        "\n",
        "**Low Temperature (temperature=0.2):** A lower temperature value (0.2 in this case) reduces randomness in the generated text. This often results in more focused and deterministic output, which can contribute to more coherent and structured text.\n",
        "\n",
        "**No Repeated N-grams**: The no_repeat_ngram_size parameter set to 3 prevents the generation of repeated n-grams. This helps avoid monotony and redundancy in the generated text, leading to more diverse and engaging content.\n",
        "\n",
        "**Advanced Language Patterns:** The increased model size and capacity of gpt2-large enable the model to capture and replicate more complex language patterns, resulting in text that appears more sophisticated and well-structured.\n",
        "\n",
        "Thus each of these factors plays a role in producing coherent, contextually relevant, and engaging generated text."
      ],
      "metadata": {
        "id": "tnB-7O5O1hR_"
      }
    }
  ]
}