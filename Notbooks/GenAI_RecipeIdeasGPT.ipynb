{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1067cbca",
   "metadata": {},
   "source": [
    "# Recipe Ideas using Custom GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689aecc",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "In the bustling life of a student, managing time between studying, working, and preparing for interviews can be challenging. Often, amidst this chaos, maintaining a balanced diet and managing grocery shopping becomes a neglected task. This project aims to address this issue by providing students with simple and easy recipe ideas tailored to the ingredients they already have on hand.\n",
    "\n",
    "<img src = images/worried_student.png width = 400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35103186",
   "metadata": {},
   "source": [
    "## Generative AI Technique for Solution\n",
    "\n",
    "The solution leverages the power of custom Generative Pre-trained Transformers (GPT) to generate recipe ideas based on the ingredients provided by the user. By interfacing this custom GPT model with a user-friendly Streamlit application, students can effortlessly access a plethora of recipe suggestions without the hassle of extensive planning or grocery shopping.\n",
    "\n",
    "This model has been fine-tuned on a dataset of recipes and culinary knowledge, enabling it to generate coherent and contextually relevant recipe ideas based on the input ingredients provided by the user.\n",
    "\n",
    "Custom GPT: https://chat.openai.com/g/g-5p6SQgcCQ-indian-chef\n",
    "\n",
    "<img src = images/conversational_AI.png width = 400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46d469",
   "metadata": {},
   "source": [
    "## Interest and Relevance in Data Science\n",
    "\n",
    "This project exemplifies the intersection of artificial intelligence and everyday life. By harnessing the power of generative AI, it addresses a practical problem faced by many individuals, showcasing the tangible applications of advanced data science techniques in enhancing daily routines. Furthermore, the utilization of Streamlit for the user interface highlights the importance of user experience and interaction design in data science projects, underscoring the holistic approach required for successful implementation.\n",
    "\n",
    "- Real-World Application: This project showcases the practical application of data science techniques in addressing real-world problems. By leveraging generative AI and user-friendly interfaces like Streamlit, it bridges the gap between cutting-edge research and everyday challenges faced by individuals. This emphasis on practicality highlights the importance of data science in creating tangible solutions that enhance people's lives.\n",
    "\n",
    "- Personalization and Customization: The utilization of a custom GPT model enables personalized recipe recommendations tailored to each user's specific ingredients and preferences. This aspect underscores the growing trend towards personalized experiences in various domains, fueled by advances in machine learning and AI. In the realm of data science, catering to individual preferences and needs through personalized recommendations represents a compelling area of research and application.\n",
    "\n",
    "- User Experience Design: The integration of Streamlit for the user interface exemplifies the significance of user experience (UX) design in data science projects. Streamlit offers a simple yet powerful platform for building interactive web applications with minimal code, making data science more accessible to a broader audience. By prioritizing intuitive design and seamless interaction, this project demonstrates the pivotal role of UX design in driving user engagement and satisfaction.\n",
    "\n",
    "- Data-driven Decision Making: The underlying principles of data science permeate every aspect of this project, from model training and fine-tuning to interface design and user interaction. By leveraging data-driven approaches, this project empowers users to make informed decisions about their dietary choices and meal planning. This emphasis on data-driven decision-making reflects a broader trend in data science towards leveraging data insights to drive actionable outcomes and informed decision-making across various domains.\n",
    "\n",
    "- Interdisciplinary Collaboration: Data science projects often require collaboration across diverse disciplines, including computer science, statistics, domain expertise, and design. This project exemplifies the interdisciplinary nature of data science by integrating expertise from AI research, culinary knowledge, user interface design, and human-computer interaction. Such interdisciplinary collaboration fosters innovation and creativity, leading to more robust and impactful solutions to complex problems.\n",
    "\n",
    "- Ethical Considerations: As with any data-driven project, ethical considerations play a crucial role in ensuring the responsible development and deployment of AI systems. This project raises important ethical questions related to privacy, data security, algorithmic bias, and the societal impact of AI-generated content. By addressing these ethical considerations proactively, data scientists can contribute to building trust and transparency in AI systems while mitigating potential harms and biases.\n",
    "\n",
    "Overall, the interest and relevance of this project in data science lie in its ability to demonstrate the practical application of advanced AI techniques, its emphasis on user-centric design and personalization, and its reflection of broader ethical and interdisciplinary considerations inherent in data-driven innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec0c4c",
   "metadata": {},
   "source": [
    "## Theoretical Foundations Behind Generative AI\n",
    "\n",
    "Generative AI, particularly models like GPT, is rooted in deep learning and natural language processing (NLP) techniques. At its core, GPT utilizes a transformer architecture, which enables it to capture long-range dependencies and generate coherent sequences of text. The training process involves feeding the model with vast amounts of text data, allowing it to learn the statistical patterns and structures inherent in the language. During inference, the model generates text by sampling from a learned probability distribution conditioned on the input provided. This probabilistic approach, coupled with the model's ability to generate contextually relevant responses, forms the theoretical foundation of generative AI.\n",
    "\n",
    "- Transformer Architecture: The backbone of GPT is the transformer architecture. This architecture revolutionized natural language processing by introducing mechanisms to capture long-range dependencies without relying on recurrent or convolutional layers. It consists of encoder and decoder blocks, each containing self-attention mechanisms and feed-forward neural networks. The self-attention mechanism allows the model to weigh the importance of different words in a sequence when generating output, enabling it to capture contextual information effectively.\n",
    "\n",
    "- Training Data and Pre-training: Generative AI models like GPT are trained on vast amounts of text data. This data typically includes diverse sources such as books, articles, websites, and social media posts. During pre-training, the model learns to predict the next word in a sequence given the preceding context. This process encourages the model to capture linguistic patterns, syntactic structures, and semantic relationships present in the training data.\n",
    "\n",
    "- Fine-tuning: After pre-training on a large corpus of text, the model can be fine-tuned on specific tasks or domains. Fine-tuning involves further training the model on a smaller dataset that is tailored to the target task. For instance, in the context of our project, the GPT model would be fine-tuned on a dataset of recipes and culinary knowledge. This fine-tuning process adapts the model's learned representations to the nuances of the target domain, enhancing its ability to generate relevant and coherent outputs.\n",
    "\n",
    "- Sampling and Inference: During inference, the generative AI model generates text by sampling from a learned probability distribution conditioned on the input provided. The model generates one token at a time, considering the previously generated tokens as context. This autoregressive sampling process continues until a predefined stopping criterion is met, such as reaching a maximum length or generating an end-of-sequence token.\n",
    "\n",
    "- Beam Search and Other Decoding Techniques: In addition to simple autoregressive sampling, advanced decoding techniques like beam search can be employed to improve the quality of generated sequences. Beam search maintains a set of candidate sequences during generation and explores multiple possible continuations in parallel. This helps mitigate issues such as repetitive or nonsensical outputs commonly observed in simple sampling approaches.\n",
    "\n",
    "- Evaluation and Optimization: Evaluating the performance of generative AI models involves assessing the quality, diversity, and coherence of the generated outputs. Metrics such as perplexity, BLEU score, and human evaluation can be used for quantitative and qualitative evaluation. Additionally, model optimization techniques such as gradient clipping, learning rate scheduling, and regularization help stabilize training and improve overall performance.\n",
    "\n",
    "By understanding these foundational concepts and techniques, we gain insight into the inner workings of generative AI models like GPT and their ability to generate coherent and contextually relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19939800",
   "metadata": {},
   "source": [
    "## Streamlit\n",
    "\n",
    "Streamlit is a popular open-source framework used for building interactive web applications with Python. It simplifies the process of creating data-driven web apps by allowing developers to focus on writing Python code while handling the underlying complexities of web development.\n",
    "\n",
    "Here's how Streamlit works:\n",
    "\n",
    "**Python-Based Development**: Streamlit enables developers to create web applications entirely using Python. This means developers can leverage their existing Python skills and libraries to build interactive applications without needing to learn additional languages or frameworks like HTML, CSS, or JavaScript.\n",
    "\n",
    "**Simple and Intuitive API**: Streamlit provides a straightforward and intuitive API for defining the structure and behavior of web applications. Developers can use familiar Python syntax to declare user interface elements such as buttons, sliders, text inputs, and plots, as well as define application logic and data processing pipelines.\n",
    "\n",
    "**Reactive Programming Model**: One of the key features of Streamlit is its reactive programming model. This means that changes to user interface elements automatically trigger updates to the corresponding components in the application. For example, modifying a slider value or selecting a different option from a dropdown menu will instantly update any dependent plots or data visualizations without the need for manual intervention.\n",
    "\n",
    "**Automatic Rendering**: Streamlit handles the rendering of user interface elements and data visualizations automatically based on the Python code provided by the developer. This includes generating HTML, CSS, and JavaScript code under the hood to display the application in a web browser. Developers can focus on writing Python code to define application logic and data visualization, while Streamlit takes care of the rendering process.\n",
    "\n",
    "**Deployment and Sharing**: Once an application is developed using Streamlit, it can be easily deployed and shared with others. Streamlit provides built-in support for deploying applications to various platforms, including Streamlit Sharing, Heroku, and Docker containers. This allows developers to quickly share their applications with colleagues, clients, or the broader community without the need for complex deployment setups.\n",
    "\n",
    "**Integration with Data Science Libraries**: Streamlit seamlessly integrates with popular data science libraries and frameworks such as Pandas, Matplotlib, Plotly, and TensorFlow. This makes it easy for data scientists and machine learning engineers to incorporate data analysis, visualization, and machine learning models into their web applications with minimal effort.\n",
    "\n",
    "Overall, Streamlit simplifies the process of building interactive web applications by providing a Python-based development environment, a simple and intuitive API, automatic rendering of user interface elements, support for reactive programming, easy deployment and sharing options, and seamless integration with data science libraries. These features make Streamlit a powerful tool for data scientists, developers, and researchers looking to create and share interactive data-driven applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41448328",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e9134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 22:23:51.535 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/vinaykumargudooru/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import streamlit as st\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialization and Configuration:\n",
    "#Here, some variables are initialized, such as response, prompt_tokens, etc. \n",
    "#The OpenAI API key is set as API_KEY, and an instance of the OpenAI client is created with this key.\n",
    "response = False\n",
    "prompt_tokens = 0\n",
    "completion_tokes = 0\n",
    "total_tokens_used = 0\n",
    "cost_of_response = 0\n",
    "\n",
    "API_KEY = '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'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "#Function Definition - make_request:\n",
    "#This function make_request takes the user-provided ingredients as input, constructs a prompt using the provided ingredients, sends this prompt to the OpenAI GPT model, and retrieves the response.\n",
    "def make_request(ingredients):\n",
    "    PROMPT = \"\"\"\n",
    "    This GPT is designed to suggest famous Indian recipes based on the ingredients available in the user's pantry.\n",
    "    It assumes default items such as common Indian spices and basic cooking staples.\n",
    "    It generates recipe suggestions strictly based on the user-provided list of ingredients, ensuring the recommendations are customized, relevant, and reflect well-known dishes.\n",
    "    The GPT communicates in a friendly and casual tone, making the interaction enjoyable and engaging.\n",
    "    Try to give me only one recipe with the best match. Make sure the output in beautiful markdown.\n",
    "    Add emojis but do not over do. \n",
    "    These are my ingredients: {ingredients}\n",
    "    \"\"\".format(ingredients=ingredients)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": PROMPT}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output = chat_completion.choices[0].message.content\n",
    "    return output\n",
    "\n",
    "#Streamlit UI Components:\n",
    "#These lines define the user interface components using Streamlit. It creates a header, text input field for entering ingredients, a button to trigger the processing of ingredients, and a horizontal rule.\n",
    "st.header(\"👨‍🍳🍳 Indian Chef\")\n",
    "st.markdown(\"\"\"---\"\"\")\n",
    "ingredients = st.text_input(\"Enter your ingredients, comma seperated\")\n",
    "rerun_button = st.button(\"Run\")\n",
    "st.markdown(\"\"\"---\"\"\")\n",
    "\n",
    "\n",
    "#Processing User Input and Displaying Output:\n",
    "#This section checks if the user has provided ingredients. If so, it calls the make_request function to process the input and retrieve recipe suggestions. If there is a response from the make_request function, it displays the header \"Recipes:\" and the generated response using the st.markdown function.\n",
    "if ingredients:\n",
    "    response = make_request(ingredients)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if response:\n",
    "    st.header(\"Recipes:\")\n",
    "    st.markdown(response)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Sidebar Information:\n",
    "#This part adds a sidebar in the Streamlit app, providing information about the project and how to use the application.\n",
    "st.sidebar.header('About the Project')\n",
    "st.sidebar.markdown(\"\"\"\n",
    "    Add ingredients in comma seperated\n",
    "    e.g. `tomato, potato, eggs`\n",
    "    Press enter or `Run`\n",
    "\n",
    "\n",
    "    Made by Vinay Kumar\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
